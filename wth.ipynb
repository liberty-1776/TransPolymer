{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:211: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream_Dataset class is called\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/practice.csv')\n",
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*C*</td>\n",
       "      <td>0.671870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*CC(*)C</td>\n",
       "      <td>0.440891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*CC(*)CC</td>\n",
       "      <td>0.439301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*CC(*)CCC</td>\n",
       "      <td>0.571796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*CC(*)CC(C)C</td>\n",
       "      <td>0.575343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>*CC1CCC(*)C1</td>\n",
       "      <td>0.711080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>*CC(*)CCCC1CCCCC1</td>\n",
       "      <td>0.349564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>*C=CCCC*</td>\n",
       "      <td>-0.710518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>*C=CCC*</td>\n",
       "      <td>-0.358177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>*C=C*</td>\n",
       "      <td>-2.691151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              smiles     value\n",
       "0                *C*  0.671870\n",
       "1            *CC(*)C  0.440891\n",
       "2           *CC(*)CC  0.439301\n",
       "3          *CC(*)CCC  0.571796\n",
       "4       *CC(*)CC(C)C  0.575343\n",
       "5       *CC1CCC(*)C1  0.711080\n",
       "6  *CC(*)CCCC1CCCCC1  0.349564\n",
       "7           *C=CCCC* -0.710518\n",
       "8            *C=CCC* -0.358177\n",
       "9              *C=C* -2.691151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*CC(*)CCCC1CCCCC1\n",
      "['*', 'C', 'C', '(', '*', ')', 'C', 'C', 'C', 'C', '1', 'C', 'C', 'C', 'C', 'C', '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   0, 3226,  347,  347, 1640, 3226,   43,  347,  347,  347,  347,  134,\n",
       "         347,  347,  347,  347,  347,  134,    2,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "text = train_data['smiles'][6]\n",
    "print(text)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "train_dataset[6]['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0010,  0.0193,  0.0198,  ...,  0.0112, -0.0060, -0.0348],\n",
      "        [ 0.0221,  0.0339, -0.0025,  ...,  0.0096, -0.0119,  0.0099],\n",
      "        [ 0.0002, -0.0194,  0.0157,  ..., -0.0070, -0.0220,  0.0310],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([411, 768])\n",
      "tensor([-9.7080e-04,  1.9268e-02,  1.9810e-02, -5.1340e-03, -6.2634e-03,\n",
      "        -2.7622e-04, -8.4029e-04,  1.1478e-02, -2.7793e-02, -3.1914e-02,\n",
      "         1.0053e-02, -4.8070e-03, -2.0411e-02,  1.0643e-02,  5.7324e-05,\n",
      "         2.6326e-02, -1.1406e-02,  1.8332e-02, -1.2637e-02,  2.0174e-02,\n",
      "        -2.6650e-02, -1.3453e-02, -1.0993e-02, -1.1852e-02, -1.3301e-02,\n",
      "         8.3330e-04,  4.9652e-02, -1.0200e-02, -3.1332e-03,  1.3913e-02,\n",
      "         2.7729e-02, -5.9845e-03,  1.6797e-02, -8.2463e-03, -1.3446e-02,\n",
      "        -9.6205e-03,  1.9222e-02,  3.0443e-02, -7.2826e-03,  6.6225e-03,\n",
      "        -9.2761e-03,  7.2548e-03,  3.9831e-03,  3.8538e-02, -2.8205e-02,\n",
      "         2.5482e-02, -1.8995e-02, -5.0444e-03, -9.0358e-03, -8.2121e-03,\n",
      "        -7.1582e-03, -9.9676e-03,  3.1666e-02, -1.9015e-02, -1.4391e-03,\n",
      "        -6.5854e-03,  1.3722e-02, -8.3274e-03,  3.2982e-02,  1.5253e-02,\n",
      "        -1.3284e-04,  1.3172e-02,  1.1058e-02,  3.1563e-02, -1.8743e-02,\n",
      "         8.0009e-03,  5.1374e-02,  6.0924e-04, -1.2928e-02, -3.0607e-02,\n",
      "         7.5440e-03, -6.4948e-03, -1.2567e-02, -6.5779e-03,  1.9873e-02,\n",
      "        -9.2933e-03, -2.3529e-02, -2.7951e-04,  1.9849e-02, -1.1990e-02,\n",
      "        -1.4484e-02,  3.3086e-02,  2.2619e-02,  4.5045e-02, -4.2100e-02,\n",
      "         2.3431e-02,  1.5772e-03, -1.9672e-02,  4.4331e-03, -6.8245e-03,\n",
      "        -2.6766e-02, -2.7356e-02, -2.4315e-03, -1.4326e-02,  8.6389e-03,\n",
      "        -1.2387e-02, -1.3720e-02, -1.5696e-02,  7.5142e-03, -5.0002e-02,\n",
      "        -4.2824e-03,  2.3496e-03, -8.1913e-03, -2.2646e-02,  9.4551e-03,\n",
      "        -2.2355e-03, -2.4526e-02, -7.6654e-03,  5.2607e-03,  3.1798e-02,\n",
      "         7.2818e-03,  1.2510e-02,  3.1988e-02,  8.2712e-03, -2.8640e-02,\n",
      "         1.9182e-02,  6.6008e-03,  2.3771e-03,  1.4205e-02, -1.4334e-02,\n",
      "         1.5946e-02, -5.3247e-03, -2.6317e-03,  1.4680e-02, -2.0579e-02,\n",
      "        -1.2961e-02, -3.1422e-02,  6.4414e-02, -7.6117e-03,  2.1123e-02,\n",
      "         2.3125e-02,  5.4485e-03, -2.8689e-02, -4.3256e-03, -1.6863e-02,\n",
      "         1.1874e-03, -3.7278e-02, -2.1083e-02,  7.7370e-03,  1.0256e-02,\n",
      "        -1.9428e-02,  3.4103e-02,  6.5976e-03,  1.6205e-02, -2.7647e-02,\n",
      "        -4.8302e-03, -9.8664e-03,  8.1609e-03, -2.7056e-02,  2.2350e-02,\n",
      "        -3.9862e-03,  1.4685e-02, -7.3599e-03,  1.3191e-02,  1.4669e-02,\n",
      "         4.3676e-03, -3.4424e-02, -3.5194e-02, -1.0115e-02,  1.2742e-02,\n",
      "         3.5670e-02, -4.0845e-03, -1.1794e-02, -9.7620e-03,  1.7124e-02,\n",
      "        -1.6083e-02,  6.0199e-02, -2.0228e-02, -1.1197e-02,  1.1057e-03,\n",
      "        -1.0429e-02,  1.0202e-02,  2.2354e-02,  1.2391e-02,  1.3160e-02,\n",
      "         2.7447e-02,  1.5023e-02, -3.4576e-03, -1.2577e-02, -2.3569e-03,\n",
      "        -2.7942e-02, -3.0848e-02, -1.6580e-03, -1.1336e-02, -2.3414e-02,\n",
      "         5.7557e-03,  7.5739e-03, -1.1400e-02, -1.3470e-02, -4.3336e-03,\n",
      "         1.2197e-03, -3.0842e-02,  1.1234e-02, -3.6423e-02, -3.2447e-03,\n",
      "        -1.1389e-03,  1.7538e-02,  1.7213e-02,  4.8718e-04,  7.3274e-03,\n",
      "         2.2437e-02, -2.8442e-03,  1.3518e-02, -3.5466e-04,  3.4810e-03,\n",
      "         6.6526e-03, -3.6468e-03,  2.0215e-02,  4.4842e-02, -1.5023e-02,\n",
      "        -1.0197e-02, -4.2961e-02,  2.0021e-02,  1.1943e-02, -2.8423e-03,\n",
      "         1.9269e-02, -3.0744e-02, -1.5453e-02, -2.4831e-02, -1.6208e-03,\n",
      "        -1.6355e-03, -7.7111e-03, -5.5146e-03, -1.6106e-02,  2.6384e-02,\n",
      "        -1.9643e-02,  8.4013e-03, -4.3219e-03,  5.8270e-03, -4.5544e-02,\n",
      "        -5.5337e-03, -1.9224e-03,  3.0573e-02,  3.4921e-02, -3.5296e-02,\n",
      "         2.9927e-02, -4.2254e-02, -5.4369e-02,  1.2163e-02,  2.5193e-02,\n",
      "         6.5846e-02, -2.6331e-02,  1.7083e-02, -4.9164e-02,  4.6929e-03,\n",
      "        -6.2254e-03,  5.0607e-03, -1.4350e-02, -1.6379e-02, -2.3205e-02,\n",
      "         4.6438e-03, -1.7672e-02, -2.9347e-03,  8.1140e-03,  9.4980e-03,\n",
      "         3.5186e-02, -1.6223e-02,  1.5755e-03, -1.1483e-02,  9.4974e-03,\n",
      "        -1.7634e-02,  1.6886e-02,  1.8798e-02, -2.9763e-03, -1.9088e-02,\n",
      "         2.8335e-02,  1.5883e-03, -8.2596e-03,  1.7640e-02, -2.2413e-03,\n",
      "         6.0319e-04,  3.2672e-02,  1.7890e-02,  2.2837e-03,  1.4338e-02,\n",
      "        -7.5561e-03, -4.8655e-03,  7.2369e-03, -1.8925e-02, -3.5587e-02,\n",
      "         3.3071e-02, -4.1978e-02,  3.2941e-03,  8.0502e-02, -1.9762e-02,\n",
      "        -1.3756e-03,  9.5853e-04,  4.4694e-03,  1.7540e-02, -8.7839e-03,\n",
      "        -1.7712e-02, -5.1649e-03, -2.1428e-02, -1.2274e-02, -1.1533e-03,\n",
      "        -8.8961e-03,  1.5596e-02, -1.4115e-02,  7.7942e-03, -1.0059e-02,\n",
      "         2.9971e-02, -1.3086e-02, -7.5103e-03, -2.1546e-02,  4.8223e-03,\n",
      "        -2.7026e-02, -1.6250e-02, -1.3021e-02,  3.9122e-02,  1.0547e-02,\n",
      "         4.6077e-03,  1.5729e-02,  1.6011e-02, -1.9783e-02,  1.3132e-02,\n",
      "         2.6421e-02, -4.2622e-03,  1.3263e-02,  1.9437e-02, -2.8745e-02,\n",
      "         6.8920e-03, -1.0153e-02,  6.2852e-03,  7.0039e-05,  1.4305e-02,\n",
      "         3.9896e-03,  2.0589e-02, -4.5858e-02,  3.5829e-02, -2.0606e-03,\n",
      "         3.2261e-02, -1.4716e-02,  1.8933e-02,  1.9186e-02,  4.3564e-03,\n",
      "        -8.3635e-03,  2.9390e-02, -3.4508e-03,  9.6105e-03, -3.5070e-04,\n",
      "         1.8883e-02,  3.5442e-03, -7.4919e-03,  1.3132e-02, -1.1589e-02,\n",
      "         5.4481e-04,  5.0739e-02, -1.4195e-02,  9.9236e-03,  9.6917e-03,\n",
      "        -7.0184e-03, -2.8041e-04, -1.2691e-02,  1.2971e-03,  4.5345e-04,\n",
      "        -2.2559e-02,  2.3464e-03, -3.4562e-02,  1.7280e-02,  1.4203e-02,\n",
      "         5.6493e-03, -7.4426e-04,  3.9294e-02,  2.7332e-02, -3.0310e-03,\n",
      "        -3.4011e-02, -9.9111e-03,  2.0561e-03,  1.3923e-02,  1.8130e-02,\n",
      "         8.3207e-03, -1.5092e-02, -8.7495e-03,  3.1419e-02, -4.7219e-03,\n",
      "         1.4655e-02, -4.2601e-02,  3.1972e-03, -7.3132e-03,  8.1544e-03,\n",
      "         1.1064e-03, -1.0112e-02, -4.2300e-02,  1.6679e-02, -3.9032e-03,\n",
      "         3.0216e-02,  8.6716e-03,  1.2277e-02,  2.1515e-02, -4.2909e-02,\n",
      "         1.5421e-02, -2.7794e-02, -6.0094e-04, -2.0704e-02, -1.7134e-02,\n",
      "         1.2134e-02,  5.7453e-03,  3.4751e-02, -2.6246e-03, -8.3743e-03,\n",
      "        -2.3447e-02, -2.8697e-02,  6.5643e-03, -2.6949e-03,  6.7728e-03,\n",
      "        -1.4918e-02,  1.6463e-02, -2.5531e-02,  8.6510e-03, -1.2389e-02,\n",
      "        -2.0726e-02, -2.5109e-02, -1.2730e-02,  1.6235e-02,  2.8409e-02,\n",
      "        -2.0987e-03,  2.5349e-02,  9.5171e-03,  1.1212e-02, -8.1208e-03,\n",
      "        -7.3708e-03, -1.8282e-02,  4.4146e-02,  2.0596e-02, -5.3953e-03,\n",
      "        -2.6227e-03, -2.4277e-02, -5.9506e-03,  1.6795e-02,  3.1439e-02,\n",
      "         1.8278e-03, -3.0040e-02,  5.9803e-03,  5.3520e-03,  8.5159e-03,\n",
      "        -1.7201e-03, -3.2902e-03,  1.5564e-02, -3.2951e-03, -5.5988e-03,\n",
      "         9.2336e-03, -6.9965e-03, -1.8920e-02, -2.2917e-03,  3.3197e-02,\n",
      "        -1.1686e-02, -1.8657e-02, -3.0258e-02,  4.7334e-02,  1.9269e-02,\n",
      "        -1.4643e-03, -1.4407e-02,  1.5996e-02, -2.5428e-02, -1.8353e-02,\n",
      "        -1.9986e-02, -1.6893e-02,  6.6542e-03, -2.1326e-03, -1.1348e-02,\n",
      "        -1.8220e-02, -1.6245e-02,  4.0180e-03,  1.9592e-02,  1.3807e-02,\n",
      "         3.7945e-02, -1.4811e-02, -2.7273e-02, -1.5293e-02, -3.1747e-02,\n",
      "        -7.5102e-04,  2.9399e-02, -2.0803e-02,  1.4006e-02,  3.5646e-02,\n",
      "        -2.3283e-02, -2.2538e-02, -1.0322e-02, -3.5189e-02,  7.2747e-03,\n",
      "         3.9289e-03, -1.2592e-02, -1.0576e-02, -2.0498e-02, -1.7310e-02,\n",
      "         4.3150e-03, -6.7039e-03,  1.3956e-02,  1.6457e-02, -5.6970e-04,\n",
      "        -1.7999e-02,  2.3955e-02,  6.1039e-03, -9.0094e-03, -2.0103e-02,\n",
      "        -4.9234e-03,  4.6248e-03,  5.1103e-03,  2.8919e-02, -1.7861e-02,\n",
      "        -1.3310e-02, -2.4390e-02, -3.1552e-02,  1.6259e-02,  1.1529e-02,\n",
      "        -8.8826e-03,  2.2213e-02,  2.8329e-03,  3.7585e-03,  7.5238e-03,\n",
      "        -1.1931e-02,  7.6128e-03,  3.5731e-02, -2.1909e-02,  3.8990e-02,\n",
      "         8.3475e-03, -1.3567e-02, -1.3325e-02,  7.3223e-03, -6.9853e-03,\n",
      "         2.7283e-02,  5.9445e-03, -2.3572e-02, -8.7035e-03,  4.5037e-02,\n",
      "        -1.9051e-02,  1.4843e-02, -2.7825e-02,  3.2109e-03,  1.0702e-02,\n",
      "        -1.3248e-02, -2.5191e-03,  1.6846e-02,  2.4821e-02, -9.5146e-03,\n",
      "         1.3533e-02,  8.9942e-03,  2.8624e-02,  3.0297e-02, -3.1525e-02,\n",
      "        -2.6760e-02,  4.6906e-02, -2.4574e-02, -1.9062e-02,  7.5797e-03,\n",
      "         6.1165e-03, -1.4971e-02, -1.4756e-02, -3.7235e-02, -1.1967e-03,\n",
      "         2.5997e-02,  1.0859e-02, -2.4623e-02, -6.5103e-03, -2.1568e-02,\n",
      "        -2.7200e-02,  1.9805e-02, -7.3885e-03, -4.5651e-03,  3.1423e-02,\n",
      "         3.0117e-02, -8.7295e-03,  2.5489e-02, -6.2877e-03,  8.9528e-03,\n",
      "        -3.3323e-02,  1.6959e-02,  2.2833e-02, -3.7745e-03, -2.1652e-02,\n",
      "        -1.2436e-02,  2.1797e-02,  3.2049e-03, -3.2847e-02, -3.6137e-02,\n",
      "         1.2951e-02, -7.3754e-03,  9.1437e-03,  4.0390e-02,  9.0603e-03,\n",
      "        -1.6617e-02, -2.3715e-02, -2.4411e-03, -1.6534e-02,  1.8772e-02,\n",
      "        -3.6360e-03, -1.5161e-02, -6.2219e-03, -2.2209e-02, -2.2281e-02,\n",
      "        -4.8516e-03, -7.2110e-03,  1.5142e-02,  1.4578e-02, -1.3112e-02,\n",
      "         2.4697e-02, -1.2421e-02,  1.1756e-02,  2.6086e-03, -2.9607e-02,\n",
      "         2.1331e-03,  8.5645e-03, -2.1141e-02, -2.7472e-02, -1.1046e-02,\n",
      "        -8.3891e-03, -2.8834e-02, -3.3260e-02, -7.8687e-03,  6.5412e-03,\n",
      "         5.9682e-02,  3.9081e-02,  2.5553e-02, -4.5452e-03, -2.7907e-02,\n",
      "        -4.1610e-03,  5.5637e-03,  1.9535e-02,  2.9943e-02, -3.2486e-02,\n",
      "         5.4807e-02,  1.3670e-02, -2.2523e-02, -1.5301e-02,  1.3453e-02,\n",
      "        -3.1551e-02, -1.8220e-02, -6.5686e-03, -1.3655e-02, -1.1134e-02,\n",
      "         2.7910e-02, -1.5528e-02, -2.4315e-02, -1.2756e-02,  1.1933e-02,\n",
      "        -2.4501e-02, -1.6404e-02,  2.3284e-02,  1.4434e-02,  2.8343e-02,\n",
      "         4.9468e-03,  1.6424e-02,  2.2306e-02, -5.5750e-03, -1.9754e-02,\n",
      "         1.8017e-03,  3.6098e-02, -2.2395e-02,  8.7278e-03, -8.7499e-03,\n",
      "        -2.1395e-02,  2.3489e-02,  5.1163e-02, -5.8164e-02, -1.8223e-02,\n",
      "         8.5819e-03, -1.8140e-02,  1.2178e-02,  1.1379e-02, -4.3365e-03,\n",
      "         1.0837e-02,  1.2440e-02,  1.6821e-02,  1.3019e-02,  2.6668e-02,\n",
      "        -9.7859e-03,  2.9934e-02, -1.3779e-02, -9.2834e-03, -3.1292e-02,\n",
      "        -1.1342e-02, -5.0350e-02,  1.6318e-02,  4.2143e-03,  2.7716e-02,\n",
      "         5.2415e-03, -1.3083e-02,  3.0448e-02, -2.5628e-02, -7.8131e-03,\n",
      "         2.0069e-04,  1.0241e-02,  5.6129e-03,  5.1282e-03,  1.2241e-02,\n",
      "        -3.7259e-02,  1.7582e-02, -1.4109e-02, -5.3219e-04,  2.9908e-02,\n",
      "        -2.8109e-02, -2.1778e-03,  7.9610e-03, -8.4872e-04, -9.6325e-03,\n",
      "         8.6042e-03, -2.4297e-02,  6.9123e-04, -1.1092e-02, -1.6612e-02,\n",
      "        -1.3143e-02,  1.2682e-03, -3.1778e-02,  1.3651e-02,  3.0082e-03,\n",
      "        -1.0119e-03,  4.7501e-02,  1.6420e-03,  1.5932e-02,  8.0260e-03,\n",
      "        -3.2852e-02,  4.9428e-03,  2.1497e-02, -5.3214e-03, -1.2773e-02,\n",
      "         8.2798e-03, -1.1234e-02, -2.6751e-02,  9.1959e-03,  1.9694e-02,\n",
      "        -1.3178e-02, -2.8403e-03,  1.1673e-02,  2.8064e-02,  2.5439e-02,\n",
      "        -1.9546e-02, -3.6558e-02, -1.1457e-02, -2.3319e-02,  8.6564e-03,\n",
      "        -4.0290e-02,  4.1986e-02, -2.1191e-02,  8.2110e-03,  1.4299e-02,\n",
      "         1.8046e-02, -1.5038e-03, -1.2703e-02,  1.8168e-03, -1.0228e-02,\n",
      "         5.3088e-03, -1.4838e-02,  3.4314e-03, -2.1531e-03, -1.3546e-02,\n",
      "         7.2493e-03,  1.0345e-02,  2.9247e-02,  4.1259e-03,  4.4799e-02,\n",
      "         2.2025e-02, -1.5686e-02, -3.9950e-03, -2.3360e-02,  1.8833e-03,\n",
      "         7.8659e-04, -5.9415e-02, -1.5111e-02,  1.3121e-02,  2.2364e-02,\n",
      "        -2.8255e-03, -4.0052e-03, -2.2248e-02, -1.6802e-02, -2.0361e-02,\n",
      "         1.1248e-02, -6.0278e-03, -3.4835e-02], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = RobertaConfig(\n",
    "            vocab_size=50265,\n",
    "            max_position_embeddings=514,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1\n",
    "        )\n",
    "PretrainedModel = RobertaModel(config=config)\n",
    "embeddings = PretrainedModel.embeddings.word_embeddings(train_dataset[6]['input_ids'])\n",
    "print(embeddings)\n",
    "print(embeddings.size())\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class DownstreamRegression is called\n"
     ]
    }
   ],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    print('Class DownstreamRegression is called')\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        print(f'Finger print is:')\n",
    "        print(logits)\n",
    "        print(logits.size())\n",
    "        output = self.Regressor(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, loss_fn, train_dataloader, device):\n",
    "    print('Train func is called')\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        print(f'Batch and Step: {step}')\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        print('Batch')\n",
    "        print(input_ids)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        prop = batch[\"prop\"].to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask).float()\n",
    "        print('Output for step',step)\n",
    "        print(outputs)\n",
    "        loss = loss_fn(outputs.squeeze(), prop.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print('End of one step')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters for scheduler\"\"\"\n",
    "steps_per_epoch = train_data.shape[0] // 1\n",
    "training_steps = steps_per_epoch * 1\n",
    "warmup_steps = int(training_steps * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.PretrainedModel.parameters(), \"lr\":  0.00005,\n",
    "                         \"weight_decay\": 0.0},\n",
    "                        {\"params\": model.Regressor.parameters(), \"lr\": 0.0001,\n",
    "                         \"weight_decay\": 0.01},\n",
    "                    ],\n",
    "    \t\t\t\tno_deprecation_warning=True\n",
    "                )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                        num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/1\n",
      "Train func is called\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd of one Epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler, loss_fn, train_dataloader, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain func is called\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch and Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    print(\"Training epoch: %s/%s\" % (epoch+1, 1))\n",
    "    train(model, optimizer, scheduler, loss_fn, train_dataloader, device)\n",
    "    print('End of one Epoch')\n",
    "    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(file_path):\n",
    "    dataset = pd.read_csv(file_path, header=None).values\n",
    "    train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1)\n",
    "    return train_data, valid_data\n",
    "\n",
    "config = RobertaConfig(\n",
    "        vocab_size=50265,\n",
    "        max_position_embeddings=514,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=6,\n",
    "        type_vocab_size=1,\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "    )\n",
    "\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=175)\n",
    "model = RobertaForMaskedLM(config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split('data/P1_practice.csv')\n",
    "data_train = LoadPretrainData(tokenizer=tokenizer, dataset=train_data, blocksize=175)\n",
    "data_valid = LoadPretrainData(tokenizer=tokenizer, dataset=valid_data, blocksize=175)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15 \n",
    "    )\n",
    "text = train_data[3][0]\n",
    "print(text)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print(data_train[3]['input_ids'])\n",
    "print(data_train[3]['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.roberta.embeddings.word_embeddings(data_train[3]['input_ids'])\n",
    "print(embeddings)\n",
    "print(embeddings.size())\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
