{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/practice.csv')\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*CC(*)C</td>\n",
       "      <td>0.673820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*CC(*)CC</td>\n",
       "      <td>0.787752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*CC(*)CCC</td>\n",
       "      <td>0.431143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*CC1CCC(*)C1</td>\n",
       "      <td>0.826822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*CC(*)CCCC1CCCCC1</td>\n",
       "      <td>0.601899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>*C(*)C</td>\n",
       "      <td>0.507022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>*C=C*</td>\n",
       "      <td>-3.138791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>*C=CCCCC*</td>\n",
       "      <td>-0.402452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>*C1CCC1*</td>\n",
       "      <td>0.019180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>*CC1(*)CCCCC1</td>\n",
       "      <td>0.469704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>*CC(*)c1ccc(C(=O)OCC(C)C)cc1</td>\n",
       "      <td>-0.963912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>*CC(*)O</td>\n",
       "      <td>0.588499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>*CC(*)C#N</td>\n",
       "      <td>0.528960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>*CC(*)(C)C#N</td>\n",
       "      <td>0.383535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>*CC(*)OC(C)=O</td>\n",
       "      <td>-0.322672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>*CC(*)CCc1ccc(C)cc1</td>\n",
       "      <td>-1.091696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>*C(C)C(*)O</td>\n",
       "      <td>0.795894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>*C(F)C(*)(F)OC(F)(F)F</td>\n",
       "      <td>0.731662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>*CC(*)C(=O)c1ccc(CC)cc1</td>\n",
       "      <td>-1.426367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          smiles     value\n",
       "0                        *CC(*)C  0.673820\n",
       "1                       *CC(*)CC  0.787752\n",
       "2                      *CC(*)CCC  0.431143\n",
       "3                   *CC1CCC(*)C1  0.826822\n",
       "4              *CC(*)CCCC1CCCCC1  0.601899\n",
       "5                         *C(*)C  0.507022\n",
       "6                          *C=C* -3.138791\n",
       "7                      *C=CCCCC* -0.402452\n",
       "8                       *C1CCC1*  0.019180\n",
       "9                  *CC1(*)CCCCC1  0.469704\n",
       "10  *CC(*)c1ccc(C(=O)OCC(C)C)cc1 -0.963912\n",
       "11                       *CC(*)O  0.588499\n",
       "12                     *CC(*)C#N  0.528960\n",
       "13                  *CC(*)(C)C#N  0.383535\n",
       "14                 *CC(*)OC(C)=O -0.322672\n",
       "15           *CC(*)CCc1ccc(C)cc1 -1.091696\n",
       "16                    *C(C)C(*)O  0.795894\n",
       "17         *C(F)C(*)(F)OC(F)(F)F  0.731662\n",
       "18       *CC(*)C(=O)c1ccc(CC)cc1 -1.426367"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpt/pretrain.pt were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.13MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.14MB/s]\n",
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 1.62MB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   0, 3226,  347,  347, 1640, 3226,   43,  347,  347,    2,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]),\n",
       " 'prop': 0.787751776470369}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PretrainedModel = RobertaModel.from_pretrained('ckpt/pretrain.pt')\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        print(type(logits))\n",
    "        output = self.Regressor(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, loss_fn, train_dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        print(f'Smile: {step+1}')\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        prop = batch[\"prop\"].to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask).float()\n",
    "        print(outputs)\n",
    "        \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters for scheduler\"\"\"\n",
    "steps_per_epoch = train_data.shape[0] // 1\n",
    "training_steps = steps_per_epoch * 1\n",
    "warmup_steps = int(training_steps * 0.05)\n",
    "\n",
    "optimizer = AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.PretrainedModel.parameters(), \"lr\":  0.00005,\n",
    "                         \"weight_decay\": 0.0},\n",
    "                        {\"params\": model.Regressor.parameters(), \"lr\": 0.0001,\n",
    "                         \"weight_decay\": 0.01},\n",
    "                    ],\n",
    "    \t\t\t\tno_deprecation_warning=True\n",
    "                )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                        num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/1\n",
      "Smile: 1\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.2513]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 2\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3112]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 3\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3186]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 4\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3677]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 5\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.4324]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 6\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.1009]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 7\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.2996]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 8\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.4147]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 9\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3492]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 10\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3859]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 11\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3317]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 12\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.2549]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 13\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.2840]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 14\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3467]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 15\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3354]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 16\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3205]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 17\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.2216]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 18\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3467]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Smile: 19\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.3274]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    print(\"Training epoch: %s/%s\" % (epoch+1, 1))\n",
    "    train(model, optimizer, scheduler, loss_fn, train_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
