{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ce2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mm22d016/miniconda3/envs/TransPolymer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = torch.empty(370, 768)\n",
    "pred_output = torch.empty(370,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,step):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        fingerprint[step] = logits\n",
    "        output = self.Regressor(logits)\n",
    "        return output\n",
    "\n",
    "def test(model, loss_fn, train_dataloader,device):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print(f'Smiles: {step+1}')\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            prop = batch[\"prop\"].to(device).float()\n",
    "            outputs = model(input_ids, attention_mask,step).float()\n",
    "            pred_output[step] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e535cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/Ei.csv')\n",
    "original_output = train_data['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461d67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpt/pretrain.pt were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "PretrainedModel = RobertaModel.from_pretrained('ckpt/pretrain.pt')\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "\n",
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=False, num_workers=8)\n",
    "\n",
    "steps_per_epoch = train_data.shape[0] // 1\n",
    "training_steps = steps_per_epoch * 1\n",
    "warmup_steps = int(training_steps * 0.05)\n",
    "\n",
    "optimizer = AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.PretrainedModel.parameters(), \"lr\":  0.00005,\n",
    "                         \"weight_decay\": 0.0},\n",
    "                        {\"params\": model.Regressor.parameters(), \"lr\": 0.0001,\n",
    "                         \"weight_decay\": 0.01},\n",
    "                    ],\n",
    "    \t\t\t\tno_deprecation_warning=True\n",
    "                )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                        num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89f493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles: 1\n",
      "Smiles: 2\n",
      "Smiles: 3\n",
      "Smiles: 4\n",
      "Smiles: 5\n",
      "Smiles: 6\n",
      "Smiles: 7\n",
      "Smiles: 8\n",
      "Smiles: 9\n",
      "Smiles: 10\n",
      "Smiles: 11\n",
      "Smiles: 12\n",
      "Smiles: 13\n",
      "Smiles: 14\n",
      "Smiles: 15\n",
      "Smiles: 16\n",
      "Smiles: 17\n",
      "Smiles: 18\n",
      "Smiles: 19\n",
      "Smiles: 20\n",
      "Smiles: 21\n",
      "Smiles: 22\n",
      "Smiles: 23\n",
      "Smiles: 24\n",
      "Smiles: 25\n",
      "Smiles: 26\n",
      "Smiles: 27\n",
      "Smiles: 28\n",
      "Smiles: 29\n",
      "Smiles: 30\n",
      "Smiles: 31\n",
      "Smiles: 32\n",
      "Smiles: 33\n",
      "Smiles: 34\n",
      "Smiles: 35\n",
      "Smiles: 36\n",
      "Smiles: 37\n",
      "Smiles: 38\n",
      "Smiles: 39\n",
      "Smiles: 40\n",
      "Smiles: 41\n",
      "Smiles: 42\n",
      "Smiles: 43\n",
      "Smiles: 44\n",
      "Smiles: 45\n",
      "Smiles: 46\n",
      "Smiles: 47\n",
      "Smiles: 48\n",
      "Smiles: 49\n",
      "Smiles: 50\n",
      "Smiles: 51\n",
      "Smiles: 52\n",
      "Smiles: 53\n",
      "Smiles: 54\n",
      "Smiles: 55\n",
      "Smiles: 56\n",
      "Smiles: 57\n",
      "Smiles: 58\n",
      "Smiles: 59\n",
      "Smiles: 60\n",
      "Smiles: 61\n",
      "Smiles: 62\n",
      "Smiles: 63\n",
      "Smiles: 64\n",
      "Smiles: 65\n",
      "Smiles: 66\n",
      "Smiles: 67\n",
      "Smiles: 68\n",
      "Smiles: 69\n",
      "Smiles: 70\n",
      "Smiles: 71\n",
      "Smiles: 72\n",
      "Smiles: 73\n",
      "Smiles: 74\n",
      "Smiles: 75\n",
      "Smiles: 76\n",
      "Smiles: 77\n",
      "Smiles: 78\n",
      "Smiles: 79\n",
      "Smiles: 80\n",
      "Smiles: 81\n",
      "Smiles: 82\n",
      "Smiles: 83\n",
      "Smiles: 84\n",
      "Smiles: 85\n",
      "Smiles: 86\n",
      "Smiles: 87\n",
      "Smiles: 88\n",
      "Smiles: 89\n",
      "Smiles: 90\n",
      "Smiles: 91\n",
      "Smiles: 92\n",
      "Smiles: 93\n",
      "Smiles: 94\n",
      "Smiles: 95\n",
      "Smiles: 96\n",
      "Smiles: 97\n",
      "Smiles: 98\n",
      "Smiles: 99\n",
      "Smiles: 100\n",
      "Smiles: 101\n",
      "Smiles: 102\n",
      "Smiles: 103\n",
      "Smiles: 104\n",
      "Smiles: 105\n",
      "Smiles: 106\n",
      "Smiles: 107\n",
      "Smiles: 108\n",
      "Smiles: 109\n",
      "Smiles: 110\n",
      "Smiles: 111\n",
      "Smiles: 112\n",
      "Smiles: 113\n",
      "Smiles: 114\n",
      "Smiles: 115\n",
      "Smiles: 116\n",
      "Smiles: 117\n",
      "Smiles: 118\n",
      "Smiles: 119\n",
      "Smiles: 120\n",
      "Smiles: 121\n",
      "Smiles: 122\n",
      "Smiles: 123\n",
      "Smiles: 124\n",
      "Smiles: 125\n",
      "Smiles: 126\n",
      "Smiles: 127\n",
      "Smiles: 128\n",
      "Smiles: 129\n",
      "Smiles: 130\n",
      "Smiles: 131\n",
      "Smiles: 132\n",
      "Smiles: 133\n",
      "Smiles: 134\n",
      "Smiles: 135\n",
      "Smiles: 136\n",
      "Smiles: 137\n",
      "Smiles: 138\n",
      "Smiles: 139\n",
      "Smiles: 140\n",
      "Smiles: 141\n",
      "Smiles: 142\n",
      "Smiles: 143\n",
      "Smiles: 144\n",
      "Smiles: 145\n",
      "Smiles: 146\n",
      "Smiles: 147\n",
      "Smiles: 148\n",
      "Smiles: 149\n",
      "Smiles: 150\n",
      "Smiles: 151\n",
      "Smiles: 152\n",
      "Smiles: 153\n",
      "Smiles: 154\n",
      "Smiles: 155\n",
      "Smiles: 156\n",
      "Smiles: 157\n",
      "Smiles: 158\n",
      "Smiles: 159\n",
      "Smiles: 160\n",
      "Smiles: 161\n",
      "Smiles: 162\n",
      "Smiles: 163\n",
      "Smiles: 164\n",
      "Smiles: 165\n",
      "Smiles: 166\n",
      "Smiles: 167\n",
      "Smiles: 168\n",
      "Smiles: 169\n",
      "Smiles: 170\n",
      "Smiles: 171\n",
      "Smiles: 172\n",
      "Smiles: 173\n",
      "Smiles: 174\n",
      "Smiles: 175\n",
      "Smiles: 176\n",
      "Smiles: 177\n",
      "Smiles: 178\n",
      "Smiles: 179\n",
      "Smiles: 180\n",
      "Smiles: 181\n",
      "Smiles: 182\n",
      "Smiles: 183\n",
      "Smiles: 184\n",
      "Smiles: 185\n",
      "Smiles: 186\n",
      "Smiles: 187\n",
      "Smiles: 188\n",
      "Smiles: 189\n",
      "Smiles: 190\n",
      "Smiles: 191\n",
      "Smiles: 192\n",
      "Smiles: 193\n",
      "Smiles: 194\n",
      "Smiles: 195\n",
      "Smiles: 196\n",
      "Smiles: 197\n",
      "Smiles: 198\n",
      "Smiles: 199\n",
      "Smiles: 200\n",
      "Smiles: 201\n",
      "Smiles: 202\n",
      "Smiles: 203\n",
      "Smiles: 204\n",
      "Smiles: 205\n",
      "Smiles: 206\n",
      "Smiles: 207\n",
      "Smiles: 208\n",
      "Smiles: 209\n",
      "Smiles: 210\n",
      "Smiles: 211\n",
      "Smiles: 212\n",
      "Smiles: 213\n",
      "Smiles: 214\n",
      "Smiles: 215\n",
      "Smiles: 216\n",
      "Smiles: 217\n",
      "Smiles: 218\n",
      "Smiles: 219\n",
      "Smiles: 220\n",
      "Smiles: 221\n",
      "Smiles: 222\n",
      "Smiles: 223\n",
      "Smiles: 224\n",
      "Smiles: 225\n",
      "Smiles: 226\n",
      "Smiles: 227\n",
      "Smiles: 228\n",
      "Smiles: 229\n",
      "Smiles: 230\n",
      "Smiles: 231\n",
      "Smiles: 232\n",
      "Smiles: 233\n",
      "Smiles: 234\n",
      "Smiles: 235\n",
      "Smiles: 236\n",
      "Smiles: 237\n",
      "Smiles: 238\n",
      "Smiles: 239\n",
      "Smiles: 240\n",
      "Smiles: 241\n",
      "Smiles: 242\n",
      "Smiles: 243\n",
      "Smiles: 244\n",
      "Smiles: 245\n",
      "Smiles: 246\n",
      "Smiles: 247\n",
      "Smiles: 248\n",
      "Smiles: 249\n",
      "Smiles: 250\n",
      "Smiles: 251\n",
      "Smiles: 252\n",
      "Smiles: 253\n",
      "Smiles: 254\n",
      "Smiles: 255\n",
      "Smiles: 256\n",
      "Smiles: 257\n",
      "Smiles: 258\n",
      "Smiles: 259\n",
      "Smiles: 260\n",
      "Smiles: 261\n",
      "Smiles: 262\n",
      "Smiles: 263\n",
      "Smiles: 264\n",
      "Smiles: 265\n",
      "Smiles: 266\n",
      "Smiles: 267\n",
      "Smiles: 268\n",
      "Smiles: 269\n",
      "Smiles: 270\n",
      "Smiles: 271\n",
      "Smiles: 272\n",
      "Smiles: 273\n",
      "Smiles: 274\n",
      "Smiles: 275\n",
      "Smiles: 276\n",
      "Smiles: 277\n",
      "Smiles: 278\n",
      "Smiles: 279\n",
      "Smiles: 280\n",
      "Smiles: 281\n",
      "Smiles: 282\n",
      "Smiles: 283\n",
      "Smiles: 284\n",
      "Smiles: 285\n",
      "Smiles: 286\n",
      "Smiles: 287\n",
      "Smiles: 288\n",
      "Smiles: 289\n",
      "Smiles: 290\n",
      "Smiles: 291\n",
      "Smiles: 292\n",
      "Smiles: 293\n",
      "Smiles: 294\n",
      "Smiles: 295\n",
      "Smiles: 296\n",
      "Smiles: 297\n",
      "Smiles: 298\n",
      "Smiles: 299\n",
      "Smiles: 300\n",
      "Smiles: 301\n",
      "Smiles: 302\n",
      "Smiles: 303\n",
      "Smiles: 304\n",
      "Smiles: 305\n",
      "Smiles: 306\n",
      "Smiles: 307\n",
      "Smiles: 308\n",
      "Smiles: 309\n",
      "Smiles: 310\n",
      "Smiles: 311\n",
      "Smiles: 312\n",
      "Smiles: 313\n",
      "Smiles: 314\n",
      "Smiles: 315\n",
      "Smiles: 316\n",
      "Smiles: 317\n",
      "Smiles: 318\n",
      "Smiles: 319\n",
      "Smiles: 320\n",
      "Smiles: 321\n",
      "Smiles: 322\n",
      "Smiles: 323\n",
      "Smiles: 324\n",
      "Smiles: 325\n",
      "Smiles: 326\n",
      "Smiles: 327\n",
      "Smiles: 328\n",
      "Smiles: 329\n",
      "Smiles: 330\n",
      "Smiles: 331\n",
      "Smiles: 332\n",
      "Smiles: 333\n",
      "Smiles: 334\n",
      "Smiles: 335\n",
      "Smiles: 336\n",
      "Smiles: 337\n",
      "Smiles: 338\n",
      "Smiles: 339\n",
      "Smiles: 340\n",
      "Smiles: 341\n",
      "Smiles: 342\n",
      "Smiles: 343\n",
      "Smiles: 344\n",
      "Smiles: 345\n",
      "Smiles: 346\n",
      "Smiles: 347\n",
      "Smiles: 348\n",
      "Smiles: 349\n",
      "Smiles: 350\n",
      "Smiles: 351\n",
      "Smiles: 352\n",
      "Smiles: 353\n",
      "Smiles: 354\n",
      "Smiles: 355\n",
      "Smiles: 356\n",
      "Smiles: 357\n",
      "Smiles: 358\n",
      "Smiles: 359\n",
      "Smiles: 360\n",
      "Smiles: 361\n",
      "Smiles: 362\n",
      "Smiles: 363\n",
      "Smiles: 364\n",
      "Smiles: 365\n",
      "Smiles: 366\n",
      "Smiles: 367\n",
      "Smiles: 368\n",
      "Smiles: 369\n",
      "Smiles: 370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.185,\n",
       " 7.6332,\n",
       " 8.1531,\n",
       " 8.5986,\n",
       " 9.0178,\n",
       " 7.5826,\n",
       " 7.0344,\n",
       " 7.9431,\n",
       " 6.904,\n",
       " 6.1428,\n",
       " 6.9158,\n",
       " 7.1012,\n",
       " 5.6689,\n",
       " 5.7733,\n",
       " 6.1739,\n",
       " 7.8277,\n",
       " 7.7636,\n",
       " 7.1539,\n",
       " 7.6583,\n",
       " 8.2018,\n",
       " 7.458,\n",
       " 7.7853,\n",
       " 6.3961,\n",
       " 7.179,\n",
       " 6.8452,\n",
       " 6.5849,\n",
       " 7.2525,\n",
       " 6.1179,\n",
       " 6.8713,\n",
       " 5.1674,\n",
       " 7.8184,\n",
       " 7.4318,\n",
       " 6.5915,\n",
       " 5.8246,\n",
       " 6.383,\n",
       " 5.0935,\n",
       " 5.6867,\n",
       " 6.8689,\n",
       " 6.5689,\n",
       " 7.5966,\n",
       " 8.3643,\n",
       " 4.5257,\n",
       " 6.2671,\n",
       " 4.9916,\n",
       " 6.0452,\n",
       " 3.5577,\n",
       " 4.5257,\n",
       " 5.6338,\n",
       " 6.2134,\n",
       " 7.4162,\n",
       " 6.2005,\n",
       " 5.4248,\n",
       " 4.7769,\n",
       " 5.7571,\n",
       " 6.0778,\n",
       " 6.3892,\n",
       " 5.0541,\n",
       " 6.1007,\n",
       " 4.7302,\n",
       " 5.6406,\n",
       " 5.1637,\n",
       " 5.6674,\n",
       " 5.1678,\n",
       " 5.7103,\n",
       " 4.9968,\n",
       " 5.9399,\n",
       " 5.1012,\n",
       " 5.1853,\n",
       " 5.6338,\n",
       " 5.5837,\n",
       " 5.9851,\n",
       " 6.2119,\n",
       " 5.8629,\n",
       " 5.6051,\n",
       " 5.7685,\n",
       " 6.5598,\n",
       " 5.9034,\n",
       " 5.9615,\n",
       " 6.6287,\n",
       " 5.1385,\n",
       " 5.7652,\n",
       " 5.2494,\n",
       " 5.2326,\n",
       " 5.6896,\n",
       " 5.4992,\n",
       " 5.797,\n",
       " 5.8858,\n",
       " 6.3555,\n",
       " 6.4325,\n",
       " 6.8862,\n",
       " 5.9541,\n",
       " 6.4155,\n",
       " 5.2507,\n",
       " 5.7769,\n",
       " 5.1816,\n",
       " 5.8606,\n",
       " 6.155,\n",
       " 5.1679,\n",
       " 5.3092,\n",
       " 5.8475,\n",
       " 5.3793,\n",
       " 5.528,\n",
       " 5.6651,\n",
       " 5.5737,\n",
       " 6.2368,\n",
       " 5.9697,\n",
       " 6.578,\n",
       " 6.6326,\n",
       " 6.2998,\n",
       " 5.8109,\n",
       " 5.6687,\n",
       " 6.2737,\n",
       " 5.4067,\n",
       " 5.3392,\n",
       " 5.8355,\n",
       " 5.6115,\n",
       " 5.631,\n",
       " 6.2998,\n",
       " 5.1709,\n",
       " 5.9475,\n",
       " 5.5608,\n",
       " 5.4079,\n",
       " 5.8549,\n",
       " 5.6727,\n",
       " 5.6976,\n",
       " 5.9933,\n",
       " 6.6298,\n",
       " 5.7881,\n",
       " 6.6564,\n",
       " 6.8635,\n",
       " 6.4199,\n",
       " 6.2041,\n",
       " 4.6632,\n",
       " 4.8208,\n",
       " 5.3628,\n",
       " 4.8207,\n",
       " 5.6786,\n",
       " 6.0852,\n",
       " 7.202,\n",
       " 6.3101,\n",
       " 6.261,\n",
       " 5.9294,\n",
       " 6.2808,\n",
       " 7.293,\n",
       " 6.4748,\n",
       " 6.5457,\n",
       " 6.0343,\n",
       " 6.2518,\n",
       " 5.6663,\n",
       " 5.8877,\n",
       " 6.97,\n",
       " 6.3305,\n",
       " 6.263,\n",
       " 6.8667,\n",
       " 6.719,\n",
       " 5.8021,\n",
       " 5.8783,\n",
       " 5.8547,\n",
       " 5.8386,\n",
       " 6.077,\n",
       " 8.5901,\n",
       " 6.7464,\n",
       " 4.932,\n",
       " 5.946,\n",
       " 4.2727,\n",
       " 5.6636,\n",
       " 5.926,\n",
       " 5.5936,\n",
       " 6.5803,\n",
       " 5.6761,\n",
       " 5.4046,\n",
       " 5.431,\n",
       " 5.3497,\n",
       " 4.8739,\n",
       " 5.3042,\n",
       " 5.0179,\n",
       " 4.6919,\n",
       " 5.1109,\n",
       " 5.8087,\n",
       " 6.259,\n",
       " 6.367,\n",
       " 6.0922,\n",
       " 6.0806,\n",
       " 6.4057,\n",
       " 6.3237,\n",
       " 7.0203,\n",
       " 5.8964,\n",
       " 5.75,\n",
       " 6.3386,\n",
       " 5.6855,\n",
       " 6.2178,\n",
       " 5.578,\n",
       " 5.7444,\n",
       " 6.7958,\n",
       " 6.3414,\n",
       " 5.3253,\n",
       " 5.7358,\n",
       " 5.8209,\n",
       " 8.8314,\n",
       " 6.8707,\n",
       " 7.3554,\n",
       " 7.0389,\n",
       " 5.3929,\n",
       " 6.113,\n",
       " 5.5977,\n",
       " 5.7571,\n",
       " 6.0589,\n",
       " 5.7839,\n",
       " 7.0438,\n",
       " 6.8471,\n",
       " 7.3658,\n",
       " 6.7886,\n",
       " 6.3539,\n",
       " 6.8199,\n",
       " 5.8483,\n",
       " 6.028,\n",
       " 5.9803,\n",
       " 6.1599,\n",
       " 7.268,\n",
       " 6.4709,\n",
       " 6.7765,\n",
       " 6.7625,\n",
       " 6.1761,\n",
       " 6.7719,\n",
       " 6.6347,\n",
       " 5.9838,\n",
       " 6.6755,\n",
       " 7.4502,\n",
       " 7.3548,\n",
       " 4.3058,\n",
       " 4.9311,\n",
       " 5.182,\n",
       " 5.3242,\n",
       " 4.3893,\n",
       " 5.6909,\n",
       " 5.2456,\n",
       " 6.311,\n",
       " 5.2424,\n",
       " 5.2303,\n",
       " 4.0261,\n",
       " 5.1241,\n",
       " 5.4925,\n",
       " 5.6596,\n",
       " 4.896,\n",
       " 4.4871,\n",
       " 5.6224,\n",
       " 4.5131,\n",
       " 4.7817,\n",
       " 5.1363,\n",
       " 5.3872,\n",
       " 5.4547,\n",
       " 6.3689,\n",
       " 6.8988,\n",
       " 5.3904,\n",
       " 4.9639,\n",
       " 5.3817,\n",
       " 4.8032,\n",
       " 5.9088,\n",
       " 5.5458,\n",
       " 7.093,\n",
       " 6.1268,\n",
       " 6.5697,\n",
       " 6.1434,\n",
       " 5.7787,\n",
       " 6.3279,\n",
       " 6.7472,\n",
       " 6.6147,\n",
       " 6.5245,\n",
       " 6.114,\n",
       " 7.7235,\n",
       " 4.8361,\n",
       " 5.9087,\n",
       " 7.159,\n",
       " 7.4857,\n",
       " 5.369,\n",
       " 5.9392,\n",
       " 7.661,\n",
       " 6.1012,\n",
       " 5.884,\n",
       " 6.5527,\n",
       " 5.5113,\n",
       " 7.0636,\n",
       " 5.7438,\n",
       " 6.6502,\n",
       " 7.9654,\n",
       " 7.5682,\n",
       " 5.9845,\n",
       " 5.378,\n",
       " 5.1123,\n",
       " 8.2121,\n",
       " 7.4364,\n",
       " 6.7464,\n",
       " 5.413,\n",
       " 6.3776,\n",
       " 6.2252,\n",
       " 7.215,\n",
       " 7.4526,\n",
       " 7.1508,\n",
       " 6.3844,\n",
       " 5.4902,\n",
       " 8.0693,\n",
       " 6.8266,\n",
       " 5.7225,\n",
       " 8.0251,\n",
       " 6.4266,\n",
       " 7.1656,\n",
       " 6.7826,\n",
       " 6.9673,\n",
       " 7.2935,\n",
       " 6.4074,\n",
       " 6.6407,\n",
       " 6.9546,\n",
       " 7.7355,\n",
       " 6.7085,\n",
       " 6.8728,\n",
       " 6.0965,\n",
       " 6.3228,\n",
       " 6.6143,\n",
       " 6.9837,\n",
       " 6.8904,\n",
       " 7.001,\n",
       " 7.9225,\n",
       " 6.438,\n",
       " 8.2676,\n",
       " 9.8385,\n",
       " 7.5565,\n",
       " 7.7091,\n",
       " 7.0732,\n",
       " 7.093,\n",
       " 5.871,\n",
       " 6.1304,\n",
       " 8.3456,\n",
       " 6.3593,\n",
       " 9.0368,\n",
       " 7.1393,\n",
       " 7.0413,\n",
       " 8.1829,\n",
       " 7.4545,\n",
       " 8.5515,\n",
       " 9.6085,\n",
       " 7.3587,\n",
       " 8.7917,\n",
       " 7.385,\n",
       " 7.3853,\n",
       " 7.2657,\n",
       " 8.5584,\n",
       " 7.8892,\n",
       " 8.5631,\n",
       " 9.067,\n",
       " 4.852,\n",
       " 5.7249,\n",
       " 5.643,\n",
       " 6.0202,\n",
       " 6.205,\n",
       " 5.5122,\n",
       " 6.162,\n",
       " 5.9085,\n",
       " 6.3469,\n",
       " 5.8604,\n",
       " 6.0364,\n",
       " 5.4552,\n",
       " 5.1795,\n",
       " 5.6233,\n",
       " 5.9175,\n",
       " 5.2813,\n",
       " 6.1951,\n",
       " 5.6948,\n",
       " 5.8838,\n",
       " 5.8176,\n",
       " 9.0985]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    test(model, loss_fn, train_dataloader, device)\n",
    "\n",
    "fingerprint = fingerprint.detach().cpu().numpy().tolist()\n",
    "pred_output = pred_output.detach().cpu().numpy().tolist()\n",
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f95a561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>pred_out</th>\n",
       "      <th>orig_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.2476686239242554, -0.3180617392063141, -0....</td>\n",
       "      <td>[-0.01442678365856409]</td>\n",
       "      <td>6.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1.0623645782470703, -0.5424985289573669, -0....</td>\n",
       "      <td>[-0.020341750234365463]</td>\n",
       "      <td>7.6332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.1539753675460815, -0.8528841137886047, -0....</td>\n",
       "      <td>[-0.06959985196590424]</td>\n",
       "      <td>8.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.881580650806427, -1.350131869316101, -0.28...</td>\n",
       "      <td>[0.00043667005957104266]</td>\n",
       "      <td>8.5986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.6920124292373657, -1.819503664970398, -0.4...</td>\n",
       "      <td>[-0.004440829623490572]</td>\n",
       "      <td>9.0178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[-0.2644903361797333, -1.8372126817703247, -0....</td>\n",
       "      <td>[0.027845410630106926]</td>\n",
       "      <td>6.1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>[1.2273632287979126, -1.4169275760650635, -0.2...</td>\n",
       "      <td>[0.1757153570652008]</td>\n",
       "      <td>5.6948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>[-0.33458125591278076, -1.3647533655166626, 0....</td>\n",
       "      <td>[0.01976815052330494]</td>\n",
       "      <td>5.8838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>[0.4122636914253235, -0.716169536113739, -0.09...</td>\n",
       "      <td>[0.02150258608162403]</td>\n",
       "      <td>5.8176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>[-0.6025153398513794, -1.9900256395339966, -0....</td>\n",
       "      <td>[0.02216639555990696]</td>\n",
       "      <td>9.0985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fingerprint  \\\n",
       "0    [-1.2476686239242554, -0.3180617392063141, -0....   \n",
       "1    [-1.0623645782470703, -0.5424985289573669, -0....   \n",
       "2    [-1.1539753675460815, -0.8528841137886047, -0....   \n",
       "3    [-0.881580650806427, -1.350131869316101, -0.28...   \n",
       "4    [-0.6920124292373657, -1.819503664970398, -0.4...   \n",
       "..                                                 ...   \n",
       "365  [-0.2644903361797333, -1.8372126817703247, -0....   \n",
       "366  [1.2273632287979126, -1.4169275760650635, -0.2...   \n",
       "367  [-0.33458125591278076, -1.3647533655166626, 0....   \n",
       "368  [0.4122636914253235, -0.716169536113739, -0.09...   \n",
       "369  [-0.6025153398513794, -1.9900256395339966, -0....   \n",
       "\n",
       "                     pred_out  orig_out  \n",
       "0      [-0.01442678365856409]    6.1850  \n",
       "1     [-0.020341750234365463]    7.6332  \n",
       "2      [-0.06959985196590424]    8.1531  \n",
       "3    [0.00043667005957104266]    8.5986  \n",
       "4     [-0.004440829623490572]    9.0178  \n",
       "..                        ...       ...  \n",
       "365    [0.027845410630106926]    6.1951  \n",
       "366      [0.1757153570652008]    5.6948  \n",
       "367     [0.01976815052330494]    5.8838  \n",
       "368     [0.02150258608162403]    5.8176  \n",
       "369     [0.02216639555990696]    9.0985  \n",
       "\n",
       "[370 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'fingerprint': fingerprint, 'pred_out': pred_output, 'orig_out': original_output }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('mywork/result_data/Ei/Ei_pretrain_fingerprint.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38602a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
