{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ce2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mm22d016/miniconda3/envs/TransPolymer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = torch.empty(368, 768)\n",
    "pred_output = torch.empty(368,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,step):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        fingerprint[step] = logits\n",
    "        output = self.Regressor(logits)\n",
    "        return output\n",
    "\n",
    "def test(model, loss_fn, train_dataloader,device):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print(f'Smiles: {step+1}')\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            prop = batch[\"prop\"].to(device).float()\n",
    "            outputs = model(input_ids, attention_mask,step).float()\n",
    "            pred_output[step] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e535cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/Eea.csv')\n",
    "original_output = train_data['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461d67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpt/pretrain.pt were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "PretrainedModel = RobertaModel.from_pretrained('ckpt/pretrain.pt')\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "\n",
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=False, num_workers=8)\n",
    "\n",
    "steps_per_epoch = train_data.shape[0] // 1\n",
    "training_steps = steps_per_epoch * 1\n",
    "warmup_steps = int(training_steps * 0.05)\n",
    "\n",
    "optimizer = AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.PretrainedModel.parameters(), \"lr\":  0.00005,\n",
    "                         \"weight_decay\": 0.0},\n",
    "                        {\"params\": model.Regressor.parameters(), \"lr\": 0.0001,\n",
    "                         \"weight_decay\": 0.01},\n",
    "                    ],\n",
    "    \t\t\t\tno_deprecation_warning=True\n",
    "                )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                        num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89f493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles: 1\n",
      "Smiles: 2\n",
      "Smiles: 3\n",
      "Smiles: 4\n",
      "Smiles: 5\n",
      "Smiles: 6\n",
      "Smiles: 7\n",
      "Smiles: 8\n",
      "Smiles: 9\n",
      "Smiles: 10\n",
      "Smiles: 11\n",
      "Smiles: 12\n",
      "Smiles: 13\n",
      "Smiles: 14\n",
      "Smiles: 15\n",
      "Smiles: 16\n",
      "Smiles: 17\n",
      "Smiles: 18\n",
      "Smiles: 19\n",
      "Smiles: 20\n",
      "Smiles: 21\n",
      "Smiles: 22\n",
      "Smiles: 23\n",
      "Smiles: 24\n",
      "Smiles: 25\n",
      "Smiles: 26\n",
      "Smiles: 27\n",
      "Smiles: 28\n",
      "Smiles: 29\n",
      "Smiles: 30\n",
      "Smiles: 31\n",
      "Smiles: 32\n",
      "Smiles: 33\n",
      "Smiles: 34\n",
      "Smiles: 35\n",
      "Smiles: 36\n",
      "Smiles: 37\n",
      "Smiles: 38\n",
      "Smiles: 39\n",
      "Smiles: 40\n",
      "Smiles: 41\n",
      "Smiles: 42\n",
      "Smiles: 43\n",
      "Smiles: 44\n",
      "Smiles: 45\n",
      "Smiles: 46\n",
      "Smiles: 47\n",
      "Smiles: 48\n",
      "Smiles: 49\n",
      "Smiles: 50\n",
      "Smiles: 51\n",
      "Smiles: 52\n",
      "Smiles: 53\n",
      "Smiles: 54\n",
      "Smiles: 55\n",
      "Smiles: 56\n",
      "Smiles: 57\n",
      "Smiles: 58\n",
      "Smiles: 59\n",
      "Smiles: 60\n",
      "Smiles: 61\n",
      "Smiles: 62\n",
      "Smiles: 63\n",
      "Smiles: 64\n",
      "Smiles: 65\n",
      "Smiles: 66\n",
      "Smiles: 67\n",
      "Smiles: 68\n",
      "Smiles: 69\n",
      "Smiles: 70\n",
      "Smiles: 71\n",
      "Smiles: 72\n",
      "Smiles: 73\n",
      "Smiles: 74\n",
      "Smiles: 75\n",
      "Smiles: 76\n",
      "Smiles: 77\n",
      "Smiles: 78\n",
      "Smiles: 79\n",
      "Smiles: 80\n",
      "Smiles: 81\n",
      "Smiles: 82\n",
      "Smiles: 83\n",
      "Smiles: 84\n",
      "Smiles: 85\n",
      "Smiles: 86\n",
      "Smiles: 87\n",
      "Smiles: 88\n",
      "Smiles: 89\n",
      "Smiles: 90\n",
      "Smiles: 91\n",
      "Smiles: 92\n",
      "Smiles: 93\n",
      "Smiles: 94\n",
      "Smiles: 95\n",
      "Smiles: 96\n",
      "Smiles: 97\n",
      "Smiles: 98\n",
      "Smiles: 99\n",
      "Smiles: 100\n",
      "Smiles: 101\n",
      "Smiles: 102\n",
      "Smiles: 103\n",
      "Smiles: 104\n",
      "Smiles: 105\n",
      "Smiles: 106\n",
      "Smiles: 107\n",
      "Smiles: 108\n",
      "Smiles: 109\n",
      "Smiles: 110\n",
      "Smiles: 111\n",
      "Smiles: 112\n",
      "Smiles: 113\n",
      "Smiles: 114\n",
      "Smiles: 115\n",
      "Smiles: 116\n",
      "Smiles: 117\n",
      "Smiles: 118\n",
      "Smiles: 119\n",
      "Smiles: 120\n",
      "Smiles: 121\n",
      "Smiles: 122\n",
      "Smiles: 123\n",
      "Smiles: 124\n",
      "Smiles: 125\n",
      "Smiles: 126\n",
      "Smiles: 127\n",
      "Smiles: 128\n",
      "Smiles: 129\n",
      "Smiles: 130\n",
      "Smiles: 131\n",
      "Smiles: 132\n",
      "Smiles: 133\n",
      "Smiles: 134\n",
      "Smiles: 135\n",
      "Smiles: 136\n",
      "Smiles: 137\n",
      "Smiles: 138\n",
      "Smiles: 139\n",
      "Smiles: 140\n",
      "Smiles: 141\n",
      "Smiles: 142\n",
      "Smiles: 143\n",
      "Smiles: 144\n",
      "Smiles: 145\n",
      "Smiles: 146\n",
      "Smiles: 147\n",
      "Smiles: 148\n",
      "Smiles: 149\n",
      "Smiles: 150\n",
      "Smiles: 151\n",
      "Smiles: 152\n",
      "Smiles: 153\n",
      "Smiles: 154\n",
      "Smiles: 155\n",
      "Smiles: 156\n",
      "Smiles: 157\n",
      "Smiles: 158\n",
      "Smiles: 159\n",
      "Smiles: 160\n",
      "Smiles: 161\n",
      "Smiles: 162\n",
      "Smiles: 163\n",
      "Smiles: 164\n",
      "Smiles: 165\n",
      "Smiles: 166\n",
      "Smiles: 167\n",
      "Smiles: 168\n",
      "Smiles: 169\n",
      "Smiles: 170\n",
      "Smiles: 171\n",
      "Smiles: 172\n",
      "Smiles: 173\n",
      "Smiles: 174\n",
      "Smiles: 175\n",
      "Smiles: 176\n",
      "Smiles: 177\n",
      "Smiles: 178\n",
      "Smiles: 179\n",
      "Smiles: 180\n",
      "Smiles: 181\n",
      "Smiles: 182\n",
      "Smiles: 183\n",
      "Smiles: 184\n",
      "Smiles: 185\n",
      "Smiles: 186\n",
      "Smiles: 187\n",
      "Smiles: 188\n",
      "Smiles: 189\n",
      "Smiles: 190\n",
      "Smiles: 191\n",
      "Smiles: 192\n",
      "Smiles: 193\n",
      "Smiles: 194\n",
      "Smiles: 195\n",
      "Smiles: 196\n",
      "Smiles: 197\n",
      "Smiles: 198\n",
      "Smiles: 199\n",
      "Smiles: 200\n",
      "Smiles: 201\n",
      "Smiles: 202\n",
      "Smiles: 203\n",
      "Smiles: 204\n",
      "Smiles: 205\n",
      "Smiles: 206\n",
      "Smiles: 207\n",
      "Smiles: 208\n",
      "Smiles: 209\n",
      "Smiles: 210\n",
      "Smiles: 211\n",
      "Smiles: 212\n",
      "Smiles: 213\n",
      "Smiles: 214\n",
      "Smiles: 215\n",
      "Smiles: 216\n",
      "Smiles: 217\n",
      "Smiles: 218\n",
      "Smiles: 219\n",
      "Smiles: 220\n",
      "Smiles: 221\n",
      "Smiles: 222\n",
      "Smiles: 223\n",
      "Smiles: 224\n",
      "Smiles: 225\n",
      "Smiles: 226\n",
      "Smiles: 227\n",
      "Smiles: 228\n",
      "Smiles: 229\n",
      "Smiles: 230\n",
      "Smiles: 231\n",
      "Smiles: 232\n",
      "Smiles: 233\n",
      "Smiles: 234\n",
      "Smiles: 235\n",
      "Smiles: 236\n",
      "Smiles: 237\n",
      "Smiles: 238\n",
      "Smiles: 239\n",
      "Smiles: 240\n",
      "Smiles: 241\n",
      "Smiles: 242\n",
      "Smiles: 243\n",
      "Smiles: 244\n",
      "Smiles: 245\n",
      "Smiles: 246\n",
      "Smiles: 247\n",
      "Smiles: 248\n",
      "Smiles: 249\n",
      "Smiles: 250\n",
      "Smiles: 251\n",
      "Smiles: 252\n",
      "Smiles: 253\n",
      "Smiles: 254\n",
      "Smiles: 255\n",
      "Smiles: 256\n",
      "Smiles: 257\n",
      "Smiles: 258\n",
      "Smiles: 259\n",
      "Smiles: 260\n",
      "Smiles: 261\n",
      "Smiles: 262\n",
      "Smiles: 263\n",
      "Smiles: 264\n",
      "Smiles: 265\n",
      "Smiles: 266\n",
      "Smiles: 267\n",
      "Smiles: 268\n",
      "Smiles: 269\n",
      "Smiles: 270\n",
      "Smiles: 271\n",
      "Smiles: 272\n",
      "Smiles: 273\n",
      "Smiles: 274\n",
      "Smiles: 275\n",
      "Smiles: 276\n",
      "Smiles: 277\n",
      "Smiles: 278\n",
      "Smiles: 279\n",
      "Smiles: 280\n",
      "Smiles: 281\n",
      "Smiles: 282\n",
      "Smiles: 283\n",
      "Smiles: 284\n",
      "Smiles: 285\n",
      "Smiles: 286\n",
      "Smiles: 287\n",
      "Smiles: 288\n",
      "Smiles: 289\n",
      "Smiles: 290\n",
      "Smiles: 291\n",
      "Smiles: 292\n",
      "Smiles: 293\n",
      "Smiles: 294\n",
      "Smiles: 295\n",
      "Smiles: 296\n",
      "Smiles: 297\n",
      "Smiles: 298\n",
      "Smiles: 299\n",
      "Smiles: 300\n",
      "Smiles: 301\n",
      "Smiles: 302\n",
      "Smiles: 303\n",
      "Smiles: 304\n",
      "Smiles: 305\n",
      "Smiles: 306\n",
      "Smiles: 307\n",
      "Smiles: 308\n",
      "Smiles: 309\n",
      "Smiles: 310\n",
      "Smiles: 311\n",
      "Smiles: 312\n",
      "Smiles: 313\n",
      "Smiles: 314\n",
      "Smiles: 315\n",
      "Smiles: 316\n",
      "Smiles: 317\n",
      "Smiles: 318\n",
      "Smiles: 319\n",
      "Smiles: 320\n",
      "Smiles: 321\n",
      "Smiles: 322\n",
      "Smiles: 323\n",
      "Smiles: 324\n",
      "Smiles: 325\n",
      "Smiles: 326\n",
      "Smiles: 327\n",
      "Smiles: 328\n",
      "Smiles: 329\n",
      "Smiles: 330\n",
      "Smiles: 331\n",
      "Smiles: 332\n",
      "Smiles: 333\n",
      "Smiles: 334\n",
      "Smiles: 335\n",
      "Smiles: 336\n",
      "Smiles: 337\n",
      "Smiles: 338\n",
      "Smiles: 339\n",
      "Smiles: 340\n",
      "Smiles: 341\n",
      "Smiles: 342\n",
      "Smiles: 343\n",
      "Smiles: 344\n",
      "Smiles: 345\n",
      "Smiles: 346\n",
      "Smiles: 347\n",
      "Smiles: 348\n",
      "Smiles: 349\n",
      "Smiles: 350\n",
      "Smiles: 351\n",
      "Smiles: 352\n",
      "Smiles: 353\n",
      "Smiles: 354\n",
      "Smiles: 355\n",
      "Smiles: 356\n",
      "Smiles: 357\n",
      "Smiles: 358\n",
      "Smiles: 359\n",
      "Smiles: 360\n",
      "Smiles: 361\n",
      "Smiles: 362\n",
      "Smiles: 363\n",
      "Smiles: 364\n",
      "Smiles: 365\n",
      "Smiles: 366\n",
      "Smiles: 367\n",
      "Smiles: 368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4343,\n",
       " 0.874,\n",
       " 1.1415,\n",
       " 1.524,\n",
       " 0.4489,\n",
       " 0.9897,\n",
       " 0.3936,\n",
       " 0.7546,\n",
       " 0.5672,\n",
       " 1.1795,\n",
       " 0.5413,\n",
       " 0.573,\n",
       " 3.0398,\n",
       " 0.5118,\n",
       " 2.0372,\n",
       " 1.0754,\n",
       " 0.8924,\n",
       " 2.2723,\n",
       " 2.4936,\n",
       " 1.2063,\n",
       " 0.5062,\n",
       " 0.7562,\n",
       " 1.5033,\n",
       " 2.2574,\n",
       " 1.1904,\n",
       " 0.6934,\n",
       " 0.7672,\n",
       " 2.356,\n",
       " 1.8315,\n",
       " 0.9199,\n",
       " 3.2451,\n",
       " 1.4499,\n",
       " 0.5296,\n",
       " 2.4594,\n",
       " 2.6013,\n",
       " 1.482,\n",
       " 0.8344,\n",
       " 1.6106,\n",
       " 0.4954,\n",
       " 0.4911,\n",
       " 2.9681,\n",
       " 2.4015,\n",
       " 0.9827,\n",
       " 2.2143,\n",
       " 1.5606,\n",
       " 2.9681,\n",
       " 3.155,\n",
       " 0.8499,\n",
       " 0.7602,\n",
       " 1.1922,\n",
       " 3.9365,\n",
       " 2.4093,\n",
       " 3.2944,\n",
       " 3.4574,\n",
       " 4.0075,\n",
       " 1.3443,\n",
       " 2.5249,\n",
       " 2.752,\n",
       " 1.7103,\n",
       " 2.3385,\n",
       " 3.3503,\n",
       " 1.8588,\n",
       " 3.6267,\n",
       " 2.1082,\n",
       " 2.8418,\n",
       " 1.897,\n",
       " 2.2963,\n",
       " 3.0264,\n",
       " 1.6576,\n",
       " 2.7321,\n",
       " 4.1807,\n",
       " 3.2177,\n",
       " 2.9311,\n",
       " 3.1315,\n",
       " 3.7508,\n",
       " 1.2264,\n",
       " 3.1094,\n",
       " 1.8722,\n",
       " 2.0787,\n",
       " 3.2587,\n",
       " 1.7705,\n",
       " 1.9244,\n",
       " 2.9754,\n",
       " 1.4385,\n",
       " 1.174,\n",
       " 2.588,\n",
       " 2.8498,\n",
       " 3.3428,\n",
       " 2.358,\n",
       " 3.3843,\n",
       " 2.9654,\n",
       " 1.1194,\n",
       " 2.0497,\n",
       " 1.3873,\n",
       " 1.2199,\n",
       " 2.3956,\n",
       " 1.9547,\n",
       " 1.8331,\n",
       " 3.0651,\n",
       " 1.812,\n",
       " 1.7487,\n",
       " 2.7075,\n",
       " 0.9966,\n",
       " 1.0649,\n",
       " 2.6743,\n",
       " 2.5375,\n",
       " 2.314,\n",
       " 3.3639,\n",
       " 3.0211,\n",
       " 2.874,\n",
       " 2.8319,\n",
       " 0.9705,\n",
       " 0.9546,\n",
       " 2.0656,\n",
       " 0.7333,\n",
       " 1.0627,\n",
       " 2.2773,\n",
       " 1.9411,\n",
       " 3.3272,\n",
       " 0.6627,\n",
       " 1.6982,\n",
       " 3.1161,\n",
       " 0.7536,\n",
       " 0.9489,\n",
       " 0.9937,\n",
       " 1.2346,\n",
       " 2.3997,\n",
       " 2.6933,\n",
       " 2.3583,\n",
       " 3.1911,\n",
       " 2.069,\n",
       " 0.6915,\n",
       " 0.6326,\n",
       " 1.6837,\n",
       " 0.6058,\n",
       " 2.8128,\n",
       " 2.6784,\n",
       " 3.1941,\n",
       " 3.6254,\n",
       " 2.4711,\n",
       " 2.4129,\n",
       " 2.336,\n",
       " 2.7979,\n",
       " 3.7473,\n",
       " 2.3793,\n",
       " 1.6069,\n",
       " 1.8209,\n",
       " 1.5258,\n",
       " 3.0632,\n",
       " 1.6393,\n",
       " 3.7805,\n",
       " 3.6382,\n",
       " 4.8698,\n",
       " 2.8847,\n",
       " 1.3272,\n",
       " 1.369,\n",
       " 1.5835,\n",
       " 2.9562,\n",
       " 1.4534,\n",
       " 2.2158,\n",
       " 3.1169,\n",
       " 2.8676,\n",
       " 3.131,\n",
       " 2.9742,\n",
       " 3.0309,\n",
       " 3.1915,\n",
       " 2.5664,\n",
       " 3.8755,\n",
       " 2.2076,\n",
       " 1.6671,\n",
       " 2.2797,\n",
       " 2.9244,\n",
       " 0.5629,\n",
       " 0.8203,\n",
       " 1.292,\n",
       " 0.54,\n",
       " 0.7695,\n",
       " 1.1612,\n",
       " 1.2974,\n",
       " 1.6269,\n",
       " 2.6857,\n",
       " 0.8689,\n",
       " 0.7624,\n",
       " 1.968,\n",
       " 4.0732,\n",
       " 2.3974,\n",
       " 2.3012,\n",
       " 3.7481,\n",
       " 1.4813,\n",
       " 1.5976,\n",
       " 1.0659,\n",
       " 0.891,\n",
       " 2.5,\n",
       " 0.9652,\n",
       " 1.6377,\n",
       " 0.8857,\n",
       " 0.9466,\n",
       " 0.7529,\n",
       " 2.0771,\n",
       " 1.2523,\n",
       " 0.421,\n",
       " 3.453,\n",
       " 4.1032,\n",
       " 2.952,\n",
       " 3.2278,\n",
       " 3.7335,\n",
       " 2.9075,\n",
       " 4.0693,\n",
       " 4.6396,\n",
       " 3.3779,\n",
       " 4.037,\n",
       " 2.882,\n",
       " 3.2522,\n",
       " 2.8757,\n",
       " 2.3086,\n",
       " 2.6442,\n",
       " 3.6994,\n",
       " 3.6543,\n",
       " 2.1866,\n",
       " 3.783,\n",
       " 3.4955,\n",
       " 2.5289,\n",
       " 3.2631,\n",
       " 3.8482,\n",
       " 1.7594,\n",
       " 3.6938,\n",
       " 2.3361,\n",
       " 3.1486,\n",
       " 2.2798,\n",
       " 1.6017,\n",
       " 2.9191,\n",
       " 3.3026,\n",
       " 2.0751,\n",
       " 2.3374,\n",
       " 3.0129,\n",
       " 4.1937,\n",
       " 3.5786,\n",
       " 2.9139,\n",
       " 1.4393,\n",
       " 2.2133,\n",
       " 1.1067,\n",
       " 2.3032,\n",
       " 1.6696,\n",
       " 1.9491,\n",
       " 3.1722,\n",
       " 1.8885,\n",
       " 1.5312,\n",
       " 1.1363,\n",
       " 2.5787,\n",
       " 2.4618,\n",
       " 4.0513,\n",
       " 2.5642,\n",
       " 3.0683,\n",
       " 1.0698,\n",
       " 1.1598,\n",
       " 1.1501,\n",
       " 2.1606,\n",
       " 2.6235,\n",
       " 3.408,\n",
       " 3.7504,\n",
       " 2.2174,\n",
       " 2.1302,\n",
       " 2.3815,\n",
       " 3.4516,\n",
       " 2.0709,\n",
       " 3.9034,\n",
       " 3.6154,\n",
       " 5.1781,\n",
       " 4.3681,\n",
       " 1.6045,\n",
       " 2.041,\n",
       " 3.7029,\n",
       " 1.9635,\n",
       " 1.551,\n",
       " 1.3447,\n",
       " 3.7701,\n",
       " 2.8874,\n",
       " 2.5935,\n",
       " 4.0646,\n",
       " 2.0356,\n",
       " 4.2115,\n",
       " 2.2894,\n",
       " 4.0637,\n",
       " 1.6985,\n",
       " 2.7178,\n",
       " 2.2298,\n",
       " 1.379,\n",
       " 0.755,\n",
       " 2.561,\n",
       " 5.1004,\n",
       " 2.0323,\n",
       " 2.6209,\n",
       " 0.6155,\n",
       " 0.7092,\n",
       " 0.7808,\n",
       " 1.4002,\n",
       " 0.6927,\n",
       " 0.5903,\n",
       " 2.7059,\n",
       " 2.0185,\n",
       " 0.6578,\n",
       " 1.5146,\n",
       " 1.6859,\n",
       " 3.5136,\n",
       " 2.5399,\n",
       " 4.1531,\n",
       " 2.3901,\n",
       " 2.9023,\n",
       " 2.0599,\n",
       " 4.0037,\n",
       " 2.1427,\n",
       " 4.1828,\n",
       " 1.9642,\n",
       " 2.4773,\n",
       " 2.3351,\n",
       " 3.9184,\n",
       " 1.7411,\n",
       " 1.8781,\n",
       " 1.7485,\n",
       " 2.8445,\n",
       " 3.5657,\n",
       " 0.6587,\n",
       " 0.6148,\n",
       " 3.7613,\n",
       " 3.4595,\n",
       " 5.1438,\n",
       " 3.6605,\n",
       " 1.8646,\n",
       " 1.524,\n",
       " 2.4953,\n",
       " 3.1022,\n",
       " 0.9509,\n",
       " 2.025,\n",
       " 1.9565,\n",
       " 3.352,\n",
       " 4.3183,\n",
       " 1.3115,\n",
       " 0.7292,\n",
       " 4.1382,\n",
       " 3.6501,\n",
       " 4.3264,\n",
       " 2.4083,\n",
       " 2.1641,\n",
       " 3.7985,\n",
       " 4.6097,\n",
       " 1.6576,\n",
       " 0.9184,\n",
       " 2.6224,\n",
       " 3.3959,\n",
       " 3.54,\n",
       " 1.4146,\n",
       " 4.1286,\n",
       " 2.775,\n",
       " 3.8058,\n",
       " 3.0595,\n",
       " 3.9759,\n",
       " 2.9183,\n",
       " 2.6486,\n",
       " 1.6869,\n",
       " 2.9241,\n",
       " 3.9089,\n",
       " 3.7067,\n",
       " 2.5831,\n",
       " 1.5387,\n",
       " 1.3385,\n",
       " 3.8901,\n",
       " 2.7568,\n",
       " 2.9195]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    test(model, loss_fn, train_dataloader, device)\n",
    "\n",
    "fingerprint = fingerprint.detach().cpu().numpy().tolist()\n",
    "pred_output = pred_output.detach().cpu().numpy().tolist()\n",
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f95a561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>pred_out</th>\n",
       "      <th>orig_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.2476686239242554, -0.3180617392063141, -0....</td>\n",
       "      <td>[-0.14158181846141815]</td>\n",
       "      <td>0.4343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1.0623645782470703, -0.5424985289573669, -0....</td>\n",
       "      <td>[-0.11254920065402985]</td>\n",
       "      <td>0.8740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.1539753675460815, -0.8528841137886047, -0....</td>\n",
       "      <td>[-0.15427304804325104]</td>\n",
       "      <td>1.1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.881580650806427, -1.350131869316101, -0.28...</td>\n",
       "      <td>[-0.1554633527994156]</td>\n",
       "      <td>1.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.6920124292373657, -1.819503664970398, -0.4...</td>\n",
       "      <td>[-0.02466488629579544]</td>\n",
       "      <td>0.4489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>[-0.2644903361797333, -1.8372126817703247, -0....</td>\n",
       "      <td>[-0.05417758226394653]</td>\n",
       "      <td>1.5387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>[1.2273632287979126, -1.4169275760650635, -0.2...</td>\n",
       "      <td>[-0.07640951126813889]</td>\n",
       "      <td>1.3385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[-0.33458125591278076, -1.3647533655166626, 0....</td>\n",
       "      <td>[0.05125347524881363]</td>\n",
       "      <td>3.8901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>[0.4122636914253235, -0.716169536113739, -0.09...</td>\n",
       "      <td>[-0.13112933933734894]</td>\n",
       "      <td>2.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>[-0.6025153398513794, -1.9900256395339966, -0....</td>\n",
       "      <td>[-0.007678577210754156]</td>\n",
       "      <td>2.9195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fingerprint  \\\n",
       "0    [-1.2476686239242554, -0.3180617392063141, -0....   \n",
       "1    [-1.0623645782470703, -0.5424985289573669, -0....   \n",
       "2    [-1.1539753675460815, -0.8528841137886047, -0....   \n",
       "3    [-0.881580650806427, -1.350131869316101, -0.28...   \n",
       "4    [-0.6920124292373657, -1.819503664970398, -0.4...   \n",
       "..                                                 ...   \n",
       "363  [-0.2644903361797333, -1.8372126817703247, -0....   \n",
       "364  [1.2273632287979126, -1.4169275760650635, -0.2...   \n",
       "365  [-0.33458125591278076, -1.3647533655166626, 0....   \n",
       "366  [0.4122636914253235, -0.716169536113739, -0.09...   \n",
       "367  [-0.6025153398513794, -1.9900256395339966, -0....   \n",
       "\n",
       "                    pred_out  orig_out  \n",
       "0     [-0.14158181846141815]    0.4343  \n",
       "1     [-0.11254920065402985]    0.8740  \n",
       "2     [-0.15427304804325104]    1.1415  \n",
       "3      [-0.1554633527994156]    1.5240  \n",
       "4     [-0.02466488629579544]    0.4489  \n",
       "..                       ...       ...  \n",
       "363   [-0.05417758226394653]    1.5387  \n",
       "364   [-0.07640951126813889]    1.3385  \n",
       "365    [0.05125347524881363]    3.8901  \n",
       "366   [-0.13112933933734894]    2.7568  \n",
       "367  [-0.007678577210754156]    2.9195  \n",
       "\n",
       "[368 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'fingerprint': fingerprint, 'pred_out': pred_output, 'orig_out': original_output }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38602a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
