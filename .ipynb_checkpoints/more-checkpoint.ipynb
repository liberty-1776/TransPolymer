{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ce2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-16 17:03:21.248938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-16 17:03:21.336419: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-16 17:03:21.354092: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-16 17:03:21.626306: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-16 17:03:21.626354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-16 17:03:21.626357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = torch.empty(2, 768)\n",
    "pred_output = torch.empty(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,step):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        print(logits)\n",
    "        fingerprint[step] = logits\n",
    "        output = self.Regressor(logits)\n",
    "        return output\n",
    "\n",
    "def test(model, loss_fn, train_dataloader,device):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print(f'Smiles: {step+1}')\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            prop = batch[\"prop\"].to(device).float()\n",
    "            outputs = model(input_ids, attention_mask,step).float()\n",
    "            print(outputs)\n",
    "            pred_output[step] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e535cf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.8972, 6.5196]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/Egc.csv')\n",
    "train_data = train_data[0:2]\n",
    "original_output = train_data['value'].tolist()\n",
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461d67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpt/pretrain.pt were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "\n",
    "PretrainedModel = RobertaModel.from_pretrained('ckpt/pretrain.pt')\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "\n",
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=False, num_workers=8)\n",
    "\n",
    "steps_per_epoch = train_data.shape[0] // 1\n",
    "training_steps = steps_per_epoch * 1\n",
    "warmup_steps = int(training_steps * 0.05)\n",
    "\n",
    "optimizer = AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.PretrainedModel.parameters(), \"lr\":  0.00005,\n",
    "                         \"weight_decay\": 0.0},\n",
    "                        {\"params\": model.Regressor.parameters(), \"lr\": 0.0001,\n",
    "                         \"weight_decay\": 0.01},\n",
    "                    ],\n",
    "    \t\t\t\tno_deprecation_warning=True\n",
    "                )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                        num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89f493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles: 1\n",
      "tensor([[-4.6697e-01, -8.1470e-01, -9.1897e-01, -8.7242e-02, -6.6621e-01,\n",
      "         -5.4890e-01, -7.7002e-01,  6.0730e-01, -1.3500e+00,  5.6777e-01,\n",
      "         -4.8231e-01,  4.2597e-01,  5.4432e-01,  4.8350e-01, -8.9016e-02,\n",
      "          1.4053e+00,  3.9309e-01,  9.9460e-01,  5.2886e-01, -5.9197e-01,\n",
      "          1.3898e+00, -4.5902e-02, -2.0183e+00,  6.6389e-01, -4.1917e-01,\n",
      "         -1.9579e-04,  5.6167e-01,  2.1255e-01,  5.9619e-01, -3.8329e-01,\n",
      "         -3.6094e-01, -3.5935e-01,  1.5565e+00,  9.9061e-01,  2.1953e-01,\n",
      "          7.8248e-01,  1.5975e+00, -3.7581e-02, -1.0295e+00, -1.0690e+00,\n",
      "          3.5503e-02,  4.5003e-01, -1.5753e+00, -3.8135e-01,  1.3160e+00,\n",
      "          9.6388e-01, -1.7682e+00,  2.7415e-01,  1.8462e+00,  1.8401e+00,\n",
      "         -9.9130e-01, -1.2174e+00, -1.0951e-01,  1.0074e+00, -1.4264e+00,\n",
      "          2.9040e-01,  1.5178e+00, -1.1104e+00, -7.6970e-01, -2.7635e-01,\n",
      "         -9.7517e-01, -2.2777e-01,  4.0092e-01, -5.5175e-01,  1.0737e-01,\n",
      "         -2.4473e-01,  2.0950e+00,  7.7065e-01, -1.9402e-02, -9.5197e-01,\n",
      "          2.1809e-01,  4.4904e-01,  1.1647e+00, -2.5848e-01,  1.4736e-01,\n",
      "         -1.1577e+00,  1.8559e-01,  1.3643e+00,  7.1234e-01,  1.1015e+00,\n",
      "         -2.2088e+00,  7.0407e-01,  6.2787e-01, -4.7395e-01, -2.4921e-01,\n",
      "          1.4475e+00, -4.1639e-01, -2.5937e-01,  2.1720e-01,  9.5841e-02,\n",
      "         -7.6731e-01,  6.7358e-02,  3.9132e-01, -1.5875e-01, -1.7426e+00,\n",
      "         -2.3856e-02, -5.3685e-01, -6.6092e-02, -1.4743e+00,  2.8445e-01,\n",
      "         -1.2962e+00, -1.3484e+00,  1.0621e+00, -9.1977e-01, -1.5127e-01,\n",
      "          1.3027e+00, -2.0073e+00,  2.3230e-02,  5.0991e-01, -1.7270e+00,\n",
      "         -1.9541e-01,  1.1626e-01,  8.3715e-01,  1.3217e+00,  2.5777e+00,\n",
      "         -1.8663e-01,  4.4058e-01,  2.1200e+00, -2.0767e-01, -1.6996e+00,\n",
      "         -1.7782e+00,  9.3883e-01, -4.2349e-01,  9.6765e-01, -2.4631e-01,\n",
      "          1.1867e+00,  5.3271e-01,  4.5220e-01,  1.0003e+00, -1.3313e-01,\n",
      "          4.8467e-01, -4.7560e-01, -1.3345e+00, -1.9721e-02,  2.9734e-01,\n",
      "          1.2269e+00, -1.2467e+00, -2.3498e-01, -1.1168e+00,  8.7987e-01,\n",
      "         -5.6042e-01,  1.6088e-01,  1.1583e+00, -5.9914e-01,  9.5904e-01,\n",
      "         -9.9717e-01,  2.1910e+00, -9.5567e-01, -1.0904e+00,  1.0301e+00,\n",
      "          1.2139e+00, -1.7889e-01, -6.7530e-01, -9.9567e-01, -4.9189e-01,\n",
      "          5.6217e-01, -1.3645e+00, -1.7517e+00, -5.4295e-01,  1.4050e+00,\n",
      "         -7.9450e-01,  2.7032e+00, -8.7099e-02,  1.5879e-01, -2.6289e-01,\n",
      "         -8.1752e-01,  5.4606e-01, -9.6593e-01, -5.7678e-01,  3.0077e-01,\n",
      "         -1.1667e-01, -1.3860e+00,  1.7862e-01,  3.4357e-01,  3.8843e-02,\n",
      "          5.5941e-01, -1.7102e+00,  1.3664e-01,  7.1779e-02,  4.0211e-01,\n",
      "          8.6310e-01,  9.5230e-01,  4.1776e-02,  7.7398e-01, -2.2264e-01,\n",
      "         -1.6691e+00, -1.4601e+00,  1.8641e+00,  4.4024e-01,  6.8126e-01,\n",
      "          8.3644e-01, -2.3726e-01,  1.8082e+00, -1.5387e+00,  1.5257e-01,\n",
      "         -9.1091e-01,  2.3305e-01, -1.4015e+00,  2.3614e-01, -7.4930e-01,\n",
      "         -1.0724e+00,  1.3440e+00,  3.8903e-01,  5.6764e-01, -2.4600e-01,\n",
      "         -7.8340e-01, -1.0604e+00, -9.8306e-02, -9.0207e-01,  1.1635e+00,\n",
      "         -8.5571e-02,  1.2650e+00, -9.3714e-01, -7.7779e-01,  1.9394e+00,\n",
      "          8.1413e-01, -4.9387e-01,  2.0759e+00,  1.6637e+00,  2.2201e-01,\n",
      "         -4.5412e-02, -4.9640e-01,  2.1462e+00,  2.2635e-02,  1.2288e+00,\n",
      "          1.3396e-01, -9.8571e-01,  1.5736e+00, -7.6808e-01, -8.6815e-01,\n",
      "         -2.4638e-01,  2.2327e-01, -1.6523e-01,  2.0125e+00,  7.5335e-01,\n",
      "          3.8462e-01, -4.5960e-01,  5.1695e-01, -2.9678e-01,  1.6479e+00,\n",
      "          1.7484e+00,  8.3395e-01,  4.5719e-01,  2.7745e-01, -1.2805e+00,\n",
      "         -7.1132e-01, -1.5905e+00, -1.4549e+00,  1.4116e+00, -3.0115e-01,\n",
      "         -1.6767e+00,  1.2974e+00,  1.3648e+00,  1.0925e-01,  4.4214e-01,\n",
      "          4.1835e-01,  8.6978e-01, -3.9751e-01, -6.7020e-01, -3.7413e-01,\n",
      "         -9.0657e-03,  2.8037e+00, -1.7009e+00, -8.9555e-01,  2.3287e+00,\n",
      "          7.6194e-02,  7.2198e-01, -2.4772e-01,  2.4994e-01,  9.8516e-01,\n",
      "          1.2466e+00, -1.1798e+00, -1.2530e+00, -3.2800e-01, -1.5562e+00,\n",
      "          2.0150e-01, -1.5459e+00,  4.5404e-01, -7.3757e-01,  1.0856e-01,\n",
      "         -4.9312e-01,  2.0857e+00, -8.4102e-01,  2.2815e-01, -7.2201e-01,\n",
      "         -7.8740e-01,  2.6513e-01, -1.1483e+00, -3.1779e-01,  2.5349e+00,\n",
      "         -3.0191e-01, -4.4043e-01,  6.2653e-01, -3.0054e-01, -1.0863e+00,\n",
      "          5.3002e-01, -1.0992e+00,  1.2282e+00, -6.0904e-02, -2.9485e-01,\n",
      "          8.6756e-01, -1.0454e+00, -1.0637e+00, -2.3862e-01, -8.9023e-02,\n",
      "          7.9887e-01,  1.5790e+00,  8.4508e-01, -7.5424e-01, -9.1668e-01,\n",
      "          4.2212e-01,  3.6900e-01, -4.6219e-01,  5.2482e-01, -5.3557e-01,\n",
      "         -9.2562e-01,  6.0648e-02,  1.4156e+00, -1.3421e+00,  4.0180e-01,\n",
      "         -1.1931e+00,  9.3821e-01,  1.3793e+00, -2.6368e+00, -4.8589e-02,\n",
      "          1.9571e+00, -2.0004e-02, -3.6166e-01,  1.7746e-01,  1.2740e+00,\n",
      "          1.4826e-01, -1.3270e+00,  4.3298e-01,  8.0637e-01, -4.1358e-01,\n",
      "         -5.4702e-01, -4.7044e-03,  1.2639e+00,  1.8262e-01,  7.3689e-01,\n",
      "          1.4096e+00, -6.6280e-01, -2.0760e+00, -2.5267e-01, -8.0638e-01,\n",
      "          9.9795e-01,  1.2907e+00,  4.3273e-01,  2.6198e-01, -1.2007e+00,\n",
      "         -7.1940e-02,  1.1974e-02,  1.4977e+00, -1.2466e+00, -1.0977e+00,\n",
      "          1.4285e+00, -1.5576e+00, -1.4207e+00, -2.2949e-01,  1.5079e-01,\n",
      "         -1.4827e+00,  3.6747e-01,  1.3564e+00, -1.5181e+00,  7.2646e-01,\n",
      "          1.4490e+00, -1.9524e+00,  3.0963e-01, -6.5579e-01, -5.1548e-01,\n",
      "          6.1458e-01,  5.3402e-01, -8.1278e-01, -1.6576e+00, -4.8211e-01,\n",
      "         -3.8033e-01, -1.8289e+00,  8.7004e-01, -1.0952e+00, -2.0676e-01,\n",
      "         -6.3136e-01, -1.3422e+00, -1.2382e+00,  1.8260e+00,  1.7476e+00,\n",
      "          1.0880e+00, -7.5871e-01,  4.9520e-01, -9.3434e-01,  3.7168e-01,\n",
      "         -8.2086e-01, -1.4684e-01, -2.8477e-01,  7.3909e-01,  1.3378e+00,\n",
      "          2.6033e-01, -5.5765e-01, -5.7681e-01,  6.5554e-02, -4.9025e-01,\n",
      "         -1.6277e+00, -1.2852e+00, -1.4686e-01, -1.8387e+00, -1.1192e+00,\n",
      "         -6.2948e-02,  1.5324e+00, -1.2621e+00,  2.5224e-01, -7.1394e-01,\n",
      "          1.2277e+00,  8.1790e-01,  1.1209e+00,  9.6841e-01, -1.8174e+00,\n",
      "         -5.8214e-01,  5.4310e-01,  9.3567e-01, -6.9703e-03, -5.6668e-01,\n",
      "          6.4480e-01,  7.3574e-01, -2.1489e-02, -1.6778e+00,  1.9054e+00,\n",
      "         -1.1200e+00, -1.0700e+00, -2.9004e-01, -2.8831e-01,  8.2343e-01,\n",
      "          2.3758e-01,  3.7452e-01,  2.7540e-01,  5.9016e-02,  8.5240e-01,\n",
      "          6.2947e-01, -8.6515e-01,  1.0702e+00,  2.3697e-01, -1.6489e+00,\n",
      "          1.3564e+00, -2.3595e-01,  1.6758e+00,  7.6487e-01, -4.3881e-01,\n",
      "          6.5557e-02,  7.1231e-01,  2.2093e+00, -2.1159e-01,  1.0225e+00,\n",
      "          2.5219e-01,  1.6417e+00, -1.6375e+00,  1.7423e+00, -1.7849e-01,\n",
      "         -1.8496e+00,  1.5109e+00,  6.9476e-01,  3.4295e-01,  5.4425e-01,\n",
      "         -1.5769e+00,  1.0008e+00, -4.1920e-01,  1.1761e+00, -9.7652e-01,\n",
      "          6.8798e-01, -2.0682e-01,  1.3780e-01,  1.0643e+00, -2.7776e-01,\n",
      "          1.3430e+00, -4.7606e-01,  1.3456e+00,  1.2247e+00, -2.1773e-01,\n",
      "         -4.5747e-01, -1.2789e+00, -2.3522e-01,  2.6109e-01,  6.5155e-01,\n",
      "         -1.0943e+00, -7.1652e-01, -1.4041e+00,  8.2193e-01, -6.9036e-01,\n",
      "         -1.3825e-01, -6.5596e-01, -2.1449e+00, -7.2127e-01,  6.5367e-02,\n",
      "         -4.6136e-01, -7.3197e-02, -3.0944e-01, -1.1438e+00,  5.5536e-01,\n",
      "          8.8823e-02, -1.4812e+00, -1.2712e-01,  4.9370e-01,  3.6138e-01,\n",
      "         -1.4806e+00,  1.5996e+00, -8.3433e-01, -9.1522e-01,  8.5593e-02,\n",
      "         -7.5093e-02, -6.1306e-01, -6.9045e-01, -2.2798e+00,  6.0486e-02,\n",
      "         -5.8199e-01, -4.1540e-02,  1.3379e+00,  1.6948e+00, -9.6638e-01,\n",
      "         -5.5029e-01, -6.4450e-01,  2.0233e-01, -7.7126e-01,  7.5621e-01,\n",
      "          1.4099e+00,  8.6751e-01, -2.9139e-01,  2.8003e-01, -9.3135e-01,\n",
      "         -3.8378e-01,  1.8378e+00,  5.7012e-01, -1.1498e+00, -5.2096e-01,\n",
      "          1.3757e+00, -1.4392e+00, -1.5472e+00,  1.1647e+00, -6.0664e-01,\n",
      "          1.4519e+00, -1.4219e-01,  1.5282e+00,  1.0616e+00,  8.2908e-01,\n",
      "          1.6590e+00, -5.1766e-01, -8.2916e-01, -6.4119e-01, -2.4326e+00,\n",
      "          1.4614e-02,  6.0549e-01, -9.2843e-01,  4.8809e-01, -8.9601e-01,\n",
      "          1.3742e+00, -2.0602e+00, -6.3057e-01, -8.0786e-01, -2.4676e-01,\n",
      "          1.6449e+00,  1.7355e+00, -2.1114e+00,  8.5201e-02,  2.4894e-01,\n",
      "         -1.0492e-01,  6.8494e-01,  4.5294e-01, -1.1130e-01, -8.3520e-02,\n",
      "         -3.3218e-01, -8.2423e-01,  1.4000e+00,  1.4854e-01, -5.1158e-01,\n",
      "          1.1724e+00,  5.0581e-01,  1.1540e+00,  1.1071e+00,  1.0701e+00,\n",
      "         -8.3626e-01,  2.4429e-01, -1.4779e+00, -1.5287e+00, -9.2802e-02,\n",
      "         -1.0932e+00,  5.6298e-01,  2.6729e-01, -9.2399e-01, -9.6515e-02,\n",
      "          1.5789e+00, -1.3949e+00,  1.0926e+00,  8.2135e-01, -2.5233e-01,\n",
      "          1.4618e+00, -6.6150e-01,  7.8956e-01, -1.1592e+00, -4.0186e-01,\n",
      "         -2.9516e-01,  1.8181e+00,  1.0274e+00,  1.1990e+00,  1.3110e+00,\n",
      "         -2.0603e+00,  3.5263e-01, -1.7636e+00, -6.7907e-01, -3.6688e-01,\n",
      "          9.7378e-01, -1.5930e-01,  3.2410e-01,  8.9983e-05, -1.5506e-01,\n",
      "          1.4217e+00, -8.6572e-01, -1.8427e+00, -5.9645e-01,  5.4725e-01,\n",
      "          2.7138e-01, -1.1351e+00, -1.0132e+00,  1.1205e+00,  6.3429e-01,\n",
      "         -3.6364e-01,  7.3911e-01, -5.2095e-01,  9.8439e-01, -1.8277e+00,\n",
      "         -5.4723e-01, -1.0738e-01, -2.6358e-01, -7.6011e-01,  5.5879e-01,\n",
      "         -1.0156e+00,  7.6053e-01, -5.2788e-01,  2.4385e+00, -1.3882e+00,\n",
      "          5.2250e-01,  5.0330e-01,  4.7161e-01, -2.9484e-01, -5.7440e-01,\n",
      "          3.1447e-02, -4.4317e-01,  2.0350e+00, -1.2502e+00, -6.1165e-01,\n",
      "         -6.5594e-01,  1.8364e-01, -1.2746e+00,  7.5360e-01,  7.3757e-01,\n",
      "         -1.1706e+00,  2.3352e+00,  1.8537e+00, -1.5599e+00, -7.0467e-01,\n",
      "         -3.0786e-01, -7.8853e-01,  1.6751e+00, -2.0426e-01,  7.6373e-01,\n",
      "         -8.6668e-01, -2.6635e-01,  1.2826e+00,  6.1778e-01,  2.2744e-01,\n",
      "          2.2173e-01,  5.8789e-01, -1.2763e+00,  9.7440e-01, -2.5403e-01,\n",
      "         -1.4957e+00,  1.6988e-01, -1.9437e-01, -8.4808e-01,  2.3111e-01,\n",
      "         -6.3407e-01,  1.2788e+00, -1.2653e+00,  6.9849e-01, -1.1969e+00,\n",
      "         -1.6240e+00,  4.3557e-01,  7.1833e-01,  1.5289e-01, -4.6843e-01,\n",
      "         -3.6421e-01, -2.8572e+00, -7.8650e-01,  2.8349e+00,  1.7700e+00,\n",
      "         -7.9340e-01,  7.2007e-01,  1.9438e-01,  1.3758e-02, -3.2723e-01,\n",
      "          8.8383e-02,  1.0685e+00,  1.7687e+00, -7.3782e-01,  4.1176e-01,\n",
      "          1.3920e+00,  4.5232e-01, -1.2031e+00,  9.8693e-01, -9.0854e-01,\n",
      "         -9.2522e-01,  3.1185e-02, -2.0717e+00,  6.7778e-01, -1.2123e-01,\n",
      "         -4.2763e-01,  4.3003e-01,  3.7326e-01,  8.3702e-01, -1.1140e+00,\n",
      "         -1.6845e+00,  5.7543e-01,  2.4989e-01, -1.2619e+00, -8.1492e-02,\n",
      "         -8.3608e-01,  1.2933e+00, -1.0548e+00,  5.2774e-01,  2.0550e-01,\n",
      "          2.0875e-01,  1.4282e+00, -6.3774e-01, -6.6736e-01, -1.2533e+00,\n",
      "          9.2555e-01, -1.2356e-01,  7.2569e-01, -1.4630e-02,  1.2273e-01,\n",
      "          1.6160e-01,  7.4184e-01, -2.2892e-01, -5.6910e-02, -6.2122e-01,\n",
      "         -1.3162e-01, -2.9092e-01,  1.2202e+00, -2.0656e-01,  2.6784e-02,\n",
      "         -2.3589e+00, -5.7035e-01,  3.5933e-01,  5.7009e-01,  1.4338e+00,\n",
      "         -1.6465e+00, -7.4031e-01, -8.7737e-01, -1.6452e+00, -9.8481e-01,\n",
      "          7.1860e-01,  1.0362e-02, -4.4063e-01, -1.1675e+00, -6.3958e-01,\n",
      "          1.6799e+00,  7.9378e-01, -1.7055e+00,  9.2058e-01,  8.2894e-01,\n",
      "          1.7192e-01,  2.2459e-02,  1.1973e+00]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([[0.1294]], device='cuda:0')\n",
      "Smiles: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.5491e-01,  7.5635e-02, -1.1678e+00,  1.9576e-02, -1.4808e-01,\n",
      "          1.1663e-01, -5.0793e-01,  1.9926e-01,  1.6595e-01, -2.2509e-01,\n",
      "         -3.2040e-03, -1.5773e-01,  3.1585e-01, -3.6816e-01,  5.0217e-01,\n",
      "          5.3881e-01, -7.0554e-01,  1.5189e+00,  1.0905e+00, -2.2474e-01,\n",
      "          1.8015e+00, -6.2293e-01, -1.4573e+00, -2.9402e-02,  6.6823e-02,\n",
      "          6.8088e-02,  1.2779e+00,  8.3722e-02,  4.1433e-01, -6.5794e-01,\n",
      "         -1.7629e+00,  3.2425e-01,  8.0662e-01,  6.8602e-01, -6.7008e-01,\n",
      "          6.8771e-01,  1.5388e+00, -9.7112e-01, -1.1376e+00, -8.2072e-01,\n",
      "         -3.1229e-02, -5.2293e-01, -7.4482e-01, -1.1387e-01,  1.5396e+00,\n",
      "         -2.6489e-02, -1.7372e+00,  1.0827e+00,  8.0250e-01,  1.4594e+00,\n",
      "         -1.2276e+00, -9.9465e-01,  6.1098e-01,  1.0827e+00,  3.2236e-01,\n",
      "          5.5270e-01,  1.1286e+00, -9.4201e-01, -8.3344e-01, -6.0996e-01,\n",
      "         -9.8769e-01,  1.3391e-01, -2.0504e-01, -6.1132e-01,  2.0911e-02,\n",
      "         -4.8847e-01,  1.1927e+00,  4.6453e-01,  1.2866e+00, -1.3027e+00,\n",
      "         -5.9137e-01,  8.9165e-01,  7.7199e-01, -2.3911e-01,  7.3994e-01,\n",
      "         -1.3139e+00,  1.7333e-01,  1.7080e-01,  5.7370e-01,  6.7157e-01,\n",
      "         -7.0491e-01,  1.7605e-02,  8.0110e-01,  5.5572e-01, -3.3771e-01,\n",
      "          1.6400e+00, -5.2624e-01, -2.8568e-01,  5.2219e-01,  6.7415e-01,\n",
      "         -6.1469e-01,  3.9392e-01,  9.5326e-01,  4.7545e-01, -1.7767e+00,\n",
      "         -8.3824e-01,  2.4575e-01,  5.9215e-01, -1.2892e+00,  3.1056e-01,\n",
      "         -1.9576e+00, -1.9096e+00,  7.3350e-01,  3.6556e-02, -5.0539e-02,\n",
      "         -3.1270e-01, -1.5985e+00,  1.0514e+00,  1.0858e+00, -1.0301e+00,\n",
      "         -1.0992e-01, -4.9959e-01,  1.1169e-01,  7.9769e-01,  2.6878e+00,\n",
      "         -1.9460e+00, -3.3869e-01,  2.4812e+00, -9.7139e-02, -1.1117e-01,\n",
      "         -5.7789e-01,  9.9611e-02,  3.2291e-01, -1.3546e-01, -1.0068e+00,\n",
      "          7.2426e-01, -2.4281e-01,  3.1971e-01,  9.2476e-01, -3.1345e-01,\n",
      "          4.7934e-01, -1.1926e-01, -8.5354e-01,  2.1612e-01,  6.1300e-01,\n",
      "          1.0589e+00, -1.1097e+00, -7.5361e-01, -1.1161e+00,  6.1755e-01,\n",
      "         -5.3470e-01, -2.9914e-01,  1.8478e-01, -8.1562e-01,  1.1776e+00,\n",
      "         -9.3693e-01,  1.5568e+00, -1.6684e+00, -7.8365e-01,  1.5729e+00,\n",
      "          1.2095e-01,  4.9399e-02, -1.6401e+00,  1.0178e-01, -4.4754e-01,\n",
      "          1.8502e+00, -1.9544e+00, -1.3075e+00,  1.1491e-01,  1.8764e+00,\n",
      "         -3.2319e-01,  2.9188e+00, -1.6449e-01, -3.9962e-01,  5.7632e-01,\n",
      "         -1.7221e+00,  2.4246e-02, -8.8275e-01, -8.0795e-01,  2.4772e-01,\n",
      "         -8.0455e-01, -1.9469e+00,  6.6395e-01,  1.4137e-01, -2.0162e-01,\n",
      "          7.6767e-01, -3.2556e+00, -9.0236e-01,  2.0048e-01,  1.0678e+00,\n",
      "          7.6659e-01,  5.0055e-01,  2.3963e-01,  7.1834e-01, -4.0797e-01,\n",
      "         -1.8491e+00, -9.0098e-01,  2.0669e+00, -6.4820e-01,  1.6015e-01,\n",
      "          2.0574e-01, -7.5927e-01,  1.4031e+00, -6.0030e-01,  4.9166e-01,\n",
      "         -1.6308e+00,  1.4401e+00, -1.2560e+00,  1.2781e+00, -1.0432e+00,\n",
      "         -2.7179e-01,  1.1399e+00, -2.2914e-02,  2.6195e-01, -5.1872e-01,\n",
      "         -1.7052e+00, -1.8089e+00, -8.9301e-01,  5.3859e-02,  6.4281e-01,\n",
      "          4.1378e-01,  7.1715e-01, -5.9361e-01, -2.5341e-01,  1.5802e+00,\n",
      "          1.2036e+00, -4.0886e-02,  1.0810e+00,  1.1472e+00,  7.9633e-01,\n",
      "         -3.7656e-01, -3.7094e-01,  8.7658e-01,  5.9132e-01,  1.4779e+00,\n",
      "          6.9585e-01, -9.6495e-01,  1.4133e+00, -4.0387e-01, -1.5226e+00,\n",
      "          2.0233e-02, -6.9140e-01, -7.1413e-01,  7.3244e-01,  1.4604e-01,\n",
      "          1.4054e-01, -6.8570e-01, -3.3226e-03,  2.2073e-01,  1.3915e+00,\n",
      "          1.8241e+00,  3.5713e-01, -5.9380e-02, -1.9493e-01, -1.1910e+00,\n",
      "          1.7877e-01, -1.3396e+00, -1.0692e+00,  1.5264e+00, -3.7866e-01,\n",
      "         -1.9487e+00,  1.6086e+00,  3.4600e-02, -7.4612e-02,  1.4161e+00,\n",
      "          7.5780e-01, -4.0853e-01,  1.5949e-01, -1.2144e+00, -4.9048e-01,\n",
      "         -1.2198e-01,  1.8253e+00, -1.6917e+00, -8.0439e-01,  1.4152e+00,\n",
      "          1.9634e-01,  8.3067e-01, -2.3322e-01,  5.7982e-01, -2.9840e-01,\n",
      "          2.6184e-01, -1.2890e+00, -1.4892e+00,  8.7138e-02, -6.5854e-01,\n",
      "         -1.0639e+00, -1.9105e+00,  8.2022e-01, -1.2467e-02, -7.3758e-02,\n",
      "         -8.9837e-01,  1.9514e+00, -1.3428e-01,  9.0550e-01,  8.1109e-02,\n",
      "         -1.5831e+00, -6.3712e-02, -1.2395e+00, -1.3206e+00,  1.1521e+00,\n",
      "         -9.8072e-01, -2.1901e-01,  9.5889e-01, -3.4988e-01, -3.6110e-01,\n",
      "          2.7605e-01, -1.5824e+00,  2.1123e-01, -1.8463e-01, -4.2249e-02,\n",
      "          6.8769e-01, -1.3785e+00, -8.2700e-01, -1.3113e-01,  9.5027e-01,\n",
      "          6.2546e-01,  1.4635e+00,  7.8589e-01,  2.2047e-01, -1.1706e+00,\n",
      "          1.2304e-01,  1.0123e+00, -1.6477e+00,  6.5258e-01, -2.5636e-02,\n",
      "         -1.4262e+00,  5.0811e-01,  1.3836e+00, -2.3954e+00,  9.9228e-01,\n",
      "         -1.2915e+00,  9.5039e-01,  1.4186e+00, -1.8292e+00, -8.3561e-01,\n",
      "          2.5565e+00, -1.2665e+00,  1.0140e+00,  1.7786e-01,  2.0765e+00,\n",
      "          6.3866e-01, -1.6076e+00, -3.8369e-01,  1.5012e+00,  2.3209e-01,\n",
      "         -2.0081e-01, -6.7412e-02,  1.9387e+00,  4.2246e-01, -4.7910e-01,\n",
      "          1.0998e+00, -9.9258e-01, -1.6096e+00, -9.4688e-01, -2.2606e-01,\n",
      "          8.3114e-01,  1.4836e+00,  9.2385e-02,  2.0081e-01, -1.9637e+00,\n",
      "         -1.0779e+00, -3.2093e-01,  8.4978e-01, -1.7596e-01, -6.0196e-01,\n",
      "          1.5116e+00, -1.3638e+00, -5.4536e-01,  6.1990e-01,  6.3802e-02,\n",
      "         -2.0658e+00,  1.0529e+00,  1.2911e+00, -2.2564e+00,  5.1298e-01,\n",
      "          1.6123e+00, -1.5327e+00,  1.2057e+00, -1.3992e+00, -2.1313e-01,\n",
      "          1.3328e-01,  1.6323e+00, -1.8474e+00, -5.5802e-01,  1.1967e-01,\n",
      "          5.6592e-03, -6.8987e-02,  1.5081e-01, -1.0090e+00, -1.1608e-01,\n",
      "          2.5596e-01, -2.0748e+00, -9.6870e-01,  1.8526e+00,  1.0005e+00,\n",
      "          2.5011e-01, -1.7059e+00,  9.7367e-01, -8.8170e-01, -2.9618e-01,\n",
      "          1.7458e-01, -1.7346e-02,  3.2137e-01,  1.2691e+00,  1.1334e+00,\n",
      "          8.0295e-01,  2.4010e-01, -5.3663e-01,  8.4620e-01, -1.6037e+00,\n",
      "         -7.6715e-01, -7.8281e-01,  4.5423e-01, -8.8178e-01,  1.5254e-01,\n",
      "         -2.6203e-01,  9.8835e-01, -5.7802e-01,  5.8132e-01, -1.3057e-01,\n",
      "          1.0804e+00, -7.4080e-02,  1.6503e+00,  9.8875e-01, -2.7645e+00,\n",
      "         -1.5705e-01, -2.3980e-01,  4.3857e-01,  6.8697e-01, -4.3272e-01,\n",
      "         -5.6481e-01, -2.5971e-01,  1.0401e-01, -1.6450e+00,  1.9462e+00,\n",
      "         -1.1860e+00, -1.0697e+00,  1.7390e-02, -6.3059e-01,  8.8402e-01,\n",
      "          7.5719e-01,  1.0793e+00, -1.7047e-01, -1.6150e-01,  9.6037e-01,\n",
      "          1.7191e+00,  1.3477e-02,  3.9884e-01, -6.2929e-03, -1.6362e+00,\n",
      "          1.6466e+00,  2.2656e-01,  1.1119e+00,  6.8122e-02, -1.0350e+00,\n",
      "          8.1601e-02,  9.5252e-01,  1.1249e+00, -2.8754e-01,  6.6626e-01,\n",
      "          9.4698e-01,  1.2221e+00, -9.0195e-01,  2.0496e+00, -6.8492e-01,\n",
      "         -1.4802e+00,  1.6067e+00,  1.7299e-01,  4.1103e-01,  7.7323e-01,\n",
      "         -1.2024e+00,  7.1927e-01,  3.9558e-01,  1.3130e+00, -3.8707e-01,\n",
      "          5.0702e-01, -4.5533e-01,  5.9310e-03,  1.3105e+00, -7.6881e-01,\n",
      "          1.6033e+00, -3.2664e-01, -2.7039e-01,  1.1442e+00,  5.4123e-01,\n",
      "         -3.8885e-01, -1.5709e+00,  5.6366e-01,  4.9826e-01, -4.8183e-01,\n",
      "         -1.9464e-01, -5.6671e-01, -1.4358e+00, -7.1106e-01, -1.1619e+00,\n",
      "         -5.5251e-01, -3.1817e-01, -2.7061e+00, -8.3588e-01,  2.7280e-01,\n",
      "         -7.3802e-01,  3.6856e-01, -2.4582e-01, -1.4967e+00,  1.7982e-01,\n",
      "         -8.4964e-02, -1.4573e+00, -1.3290e+00,  4.9751e-01,  9.2356e-01,\n",
      "         -5.4516e-01,  1.1047e+00,  5.2841e-01, -1.3935e+00,  4.2962e-01,\n",
      "         -1.3137e+00,  2.5035e-01, -3.3136e-01, -1.9322e+00, -1.9224e-01,\n",
      "          1.0273e-01, -2.4975e-01,  1.7103e+00,  1.8005e+00, -1.3303e+00,\n",
      "         -4.9924e-01, -4.9341e-01,  6.6417e-01, -8.7638e-01,  5.4638e-01,\n",
      "          7.9924e-01,  7.2239e-01,  4.5453e-01,  6.6320e-01,  3.4303e-01,\n",
      "         -5.7196e-01,  2.3310e+00,  1.2602e+00, -1.5436e+00,  8.8461e-02,\n",
      "          1.3110e+00, -9.0162e-01, -9.3560e-01,  4.6600e-01, -5.2589e-01,\n",
      "          1.3122e+00, -2.3324e-01,  1.9356e+00,  3.4521e-01,  6.8825e-01,\n",
      "          1.6372e+00, -7.1976e-01, -1.1039e+00, -5.2940e-01, -1.1755e+00,\n",
      "         -2.4844e-01,  8.3630e-01, -7.4955e-01,  7.5949e-01, -7.8014e-01,\n",
      "          1.5077e-01, -1.2637e+00, -1.9592e-01, -7.6863e-01, -1.5620e-01,\n",
      "          2.6455e-01,  1.3862e+00, -1.5489e+00,  5.0518e-01,  2.7659e-01,\n",
      "          8.5573e-01,  6.5092e-01,  1.0347e+00, -5.9236e-01, -7.1906e-01,\n",
      "         -4.8100e-01, -1.2696e+00,  5.6366e-01,  3.1745e-01,  5.9046e-02,\n",
      "          6.6157e-01,  1.2859e-01,  1.6933e+00,  1.3996e+00,  1.5301e+00,\n",
      "         -2.9449e-01,  2.4741e-01, -2.1337e+00, -1.4818e+00, -1.0014e+00,\n",
      "         -1.0895e+00,  9.0974e-01,  3.2398e-01, -1.9643e+00, -1.5434e-01,\n",
      "          2.1979e+00, -9.5928e-01,  1.5596e+00,  7.6022e-01,  8.1752e-01,\n",
      "          1.6983e+00, -1.7777e+00,  1.0021e-01, -1.9862e+00, -1.0058e+00,\n",
      "         -1.4725e-01,  1.6453e+00,  8.3825e-01,  2.2140e+00,  5.8076e-01,\n",
      "         -1.7083e+00,  1.1022e+00, -1.3933e+00, -6.0089e-01, -1.3701e-01,\n",
      "          3.6874e-02, -9.5277e-02, -4.6276e-04,  2.8468e-01,  1.3522e-01,\n",
      "          2.7219e+00, -8.3453e-01, -1.7441e+00, -9.5475e-01, -4.5987e-02,\n",
      "          1.4745e-01, -1.2771e+00, -1.5048e-01,  1.7285e+00,  2.7728e-01,\n",
      "         -7.0673e-01,  5.6504e-01, -3.3982e-01,  5.8577e-01, -2.3005e+00,\n",
      "         -2.1257e-02,  9.7907e-01, -1.2660e-01, -6.2051e-01,  8.8781e-01,\n",
      "          5.6332e-01,  3.7267e-01, -9.2381e-02,  1.9465e+00, -2.0793e+00,\n",
      "          3.2930e-01,  4.5813e-01, -6.6133e-01,  5.0259e-02, -9.8600e-01,\n",
      "         -8.2272e-01,  1.0365e-01,  2.6914e+00, -1.1920e+00, -2.6675e-01,\n",
      "         -1.0811e+00,  1.5370e-01,  1.0268e-01,  1.0576e+00,  4.0840e-01,\n",
      "         -2.7336e-01,  1.6314e+00,  1.6782e+00, -2.2652e-01, -4.1529e-01,\n",
      "         -8.6658e-02,  8.7499e-01,  7.1238e-01,  2.8592e-02,  6.0292e-01,\n",
      "         -4.4662e-01,  2.1014e-01,  7.4402e-01,  1.2750e+00, -1.0686e+00,\n",
      "          7.4978e-01,  1.0757e+00, -4.7559e-01,  2.0779e-01,  3.5338e-02,\n",
      "         -1.4051e+00, -9.2755e-01,  1.7827e-01, -7.3966e-01,  1.0430e+00,\n",
      "         -4.0764e-01,  1.1258e+00, -4.5687e-01,  4.2135e-02, -8.3572e-01,\n",
      "         -1.9234e+00, -1.6939e-02, -1.9053e-02,  5.5246e-01,  8.2858e-01,\n",
      "         -1.2746e+00, -1.7470e+00, -4.5813e-01,  1.9901e+00,  2.4632e+00,\n",
      "         -1.1151e+00,  1.6465e+00,  2.8124e-01,  6.7000e-01, -3.2700e-01,\n",
      "          4.8150e-01,  9.0958e-01,  1.1610e-01, -8.4028e-01,  3.4783e-01,\n",
      "          6.9229e-01, -1.6701e-01, -2.0487e+00,  7.8591e-01, -5.0242e-01,\n",
      "         -1.1852e-01, -5.6931e-01, -2.4190e+00,  5.7337e-01,  4.5080e-01,\n",
      "         -2.4746e-01,  1.9897e-01,  1.1352e+00,  4.2873e-01,  2.3159e-01,\n",
      "         -1.4907e+00, -1.3647e-01,  9.2369e-01, -1.4804e+00, -1.1191e-01,\n",
      "          9.5368e-01,  1.0341e+00, -8.0001e-01, -2.7720e-01,  7.1909e-01,\n",
      "         -1.3035e-01,  3.7611e-01, -9.2794e-01, -5.3033e-01, -1.6672e+00,\n",
      "          1.9496e-01,  1.0808e-01,  1.2452e+00, -3.5928e-01, -7.5829e-01,\n",
      "          8.1936e-01, -2.2484e-01,  6.9853e-01,  2.4053e-01, -7.8670e-01,\n",
      "         -5.1261e-01, -4.6154e-01,  7.9191e-01,  1.4276e-01,  1.3997e+00,\n",
      "         -2.2850e+00, -6.4207e-01,  6.3077e-01,  1.5166e+00,  6.3556e-01,\n",
      "         -1.0781e+00, -1.4728e+00, -1.9411e+00, -3.8797e-01, -1.5387e+00,\n",
      "          9.3456e-01, -4.1914e-01,  4.2719e-01, -3.7705e-01, -6.9425e-01,\n",
      "          1.6471e+00, -2.8428e-01, -1.4403e+00,  1.8318e+00,  7.7701e-02,\n",
      "          4.0839e-01,  5.4033e-01,  1.7583e+00]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([[0.0940]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     test(model, loss_fn, train_dataloader, device)\n\u001b[0;32m----> 4\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m \u001b[43mfingerprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m pred_output \u001b[38;5;241m=\u001b[39m pred_output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(fingerprint)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    test(model, loss_fn, train_dataloader, device)\n",
    "\n",
    "fingerprint = fingerprint.detach().cpu().numpy().tolist()\n",
    "pred_output = pred_output.detach().cpu().numpy().tolist()\n",
    "print(fingerprint)\n",
    "print(pred_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f95a561",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfingerprint\u001b[39m\u001b[38;5;124m'\u001b[39m: fingerprint, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_out\u001b[39m\u001b[38;5;124m'\u001b[39m: pred_output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morig_out\u001b[39m\u001b[38;5;124m'\u001b[39m: original_output }\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m df\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/internals/construction.py:125\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/internals/construction.py:625\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    622\u001b[0m             val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[1;32m    623\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m--> 625\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_cast_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[1;32m    630\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/construction.py:596\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    594\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy, raise_cast_failure)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    598\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    119\u001b[0m arr: ArrayLike\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mrange\u001b[39m)):\n\u001b[0;32m--> 122\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_object_array_from_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m#  or ExtensionArray here.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     arr \u001b[38;5;241m=\u001b[39m values\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1983\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[1;32m   1981\u001b[0m \u001b[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[1;32m   1982\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1983\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/torch/_tensor.py:759\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "data = {'fingerprint': fingerprint, 'pred_out': pred_output, 'orig_out': original_output }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38602a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
