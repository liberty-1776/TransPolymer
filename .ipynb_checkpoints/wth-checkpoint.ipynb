{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:211: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream_Dataset class is called\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/practice.csv')\n",
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*C*</td>\n",
       "      <td>0.671870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*CC(*)C</td>\n",
       "      <td>0.440891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*CC(*)CC</td>\n",
       "      <td>0.439301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*CC(*)CCC</td>\n",
       "      <td>0.571796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*CC(*)CC(C)C</td>\n",
       "      <td>0.575343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>*CC1CCC(*)C1</td>\n",
       "      <td>0.711080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>*CC(*)CCCC1CCCCC1</td>\n",
       "      <td>0.349564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>*C=CCCC*</td>\n",
       "      <td>-0.710518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>*C=CCC*</td>\n",
       "      <td>-0.358177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>*C=C*</td>\n",
       "      <td>-2.691151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              smiles     value\n",
       "0                *C*  0.671870\n",
       "1            *CC(*)C  0.440891\n",
       "2           *CC(*)CC  0.439301\n",
       "3          *CC(*)CCC  0.571796\n",
       "4       *CC(*)CC(C)C  0.575343\n",
       "5       *CC1CCC(*)C1  0.711080\n",
       "6  *CC(*)CCCC1CCCCC1  0.349564\n",
       "7           *C=CCCC* -0.710518\n",
       "8            *C=CCC* -0.358177\n",
       "9              *C=C* -2.691151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*CC(*)CCCC1CCCCC1\n",
      "['*', 'C', 'C', '(', '*', ')', 'C', 'C', 'C', 'C', '1', 'C', 'C', 'C', 'C', 'C', '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   0, 3226,  347,  347, 1640, 3226,   43,  347,  347,  347,  347,  134,\n",
       "         347,  347,  347,  347,  347,  134,    2,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "text = train_data['smiles'][6]\n",
    "print(text)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "train_dataset[6]['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0269, -0.0049, -0.0130,  ...,  0.0201,  0.0278, -0.0359],\n",
      "        [-0.0283,  0.0207,  0.0025,  ..., -0.0148, -0.0049, -0.0015],\n",
      "        [-0.0362,  0.0065,  0.0288,  ...,  0.0372,  0.0136,  0.0537],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([411, 768])\n",
      "tensor([ 2.6863e-02, -4.8934e-03, -1.2977e-02, -8.0318e-03, -2.5304e-02,\n",
      "         1.2681e-02,  7.1738e-03,  2.1254e-03, -1.1876e-02,  3.5735e-03,\n",
      "         1.4473e-02,  1.0845e-02, -2.6052e-03,  2.0021e-02, -3.6207e-02,\n",
      "         9.1943e-04,  1.7239e-02, -2.1348e-03,  5.8311e-03,  1.2511e-02,\n",
      "         1.7250e-02, -1.1004e-02,  1.5169e-02,  8.6988e-03,  2.3975e-03,\n",
      "        -3.3758e-02, -1.0915e-03,  2.4538e-03, -2.1620e-02, -1.9996e-02,\n",
      "        -1.7565e-02,  3.5023e-02, -1.0936e-03, -2.1540e-02,  1.6676e-03,\n",
      "         1.6263e-02,  1.3632e-02,  3.7709e-02,  8.7381e-03,  3.0593e-02,\n",
      "         2.1378e-02, -1.5397e-02,  5.8051e-03,  9.8002e-03,  1.3869e-02,\n",
      "         1.8278e-02,  6.9546e-03,  2.2924e-02,  1.9051e-02,  2.5402e-02,\n",
      "        -4.6047e-03, -1.4708e-02,  2.4458e-02, -1.0494e-02, -3.2108e-02,\n",
      "        -1.5563e-03, -9.9657e-04, -4.7853e-02, -1.9072e-02, -1.7078e-02,\n",
      "         3.7938e-02, -1.2645e-02,  1.0907e-02,  6.6624e-03,  3.7526e-02,\n",
      "         8.6412e-03, -1.5566e-02,  2.5444e-02, -6.3600e-03,  2.0193e-02,\n",
      "         7.4219e-03,  1.3870e-02,  2.0238e-03,  3.8250e-03,  4.4729e-02,\n",
      "        -1.9041e-03, -2.2850e-02, -3.6094e-02, -1.7334e-02, -5.1589e-03,\n",
      "        -2.2286e-03,  2.1420e-03, -2.4308e-02, -2.9794e-02,  4.1313e-03,\n",
      "        -9.5358e-03,  2.2950e-02,  2.5722e-03, -2.9662e-02, -1.2019e-02,\n",
      "         5.7925e-03,  3.9327e-02,  3.1178e-02, -7.9266e-03,  3.3146e-03,\n",
      "        -9.3369e-03, -1.1938e-02,  1.2855e-02, -1.3991e-03,  1.6849e-02,\n",
      "        -1.1807e-03, -2.5992e-02, -2.6074e-03, -1.9781e-02,  7.4293e-03,\n",
      "        -9.1608e-03, -1.5878e-02,  2.4065e-02,  4.8696e-03,  9.2780e-04,\n",
      "         4.4945e-03,  2.0806e-02, -1.9106e-02,  8.4475e-03, -3.0391e-02,\n",
      "        -1.5833e-02,  1.2302e-02, -1.8673e-02,  6.1094e-03, -2.3587e-03,\n",
      "        -6.8110e-03,  3.6890e-02,  6.2395e-03,  7.0914e-03, -8.4809e-03,\n",
      "        -8.6884e-04,  1.0204e-02,  3.6547e-02, -1.8380e-02, -1.5971e-02,\n",
      "         3.0449e-02,  2.2610e-02,  1.0446e-02,  1.3957e-02, -2.3653e-02,\n",
      "         3.2115e-02,  3.1177e-02, -2.0268e-02,  1.1106e-02, -2.4793e-02,\n",
      "        -3.6294e-02, -4.5100e-04, -3.0528e-02, -5.6822e-02, -7.8656e-03,\n",
      "        -5.2317e-03, -4.1930e-03, -1.2670e-02, -4.8199e-02,  1.2769e-02,\n",
      "        -3.9705e-02, -1.6817e-02, -1.9675e-02, -3.2157e-03,  6.4211e-03,\n",
      "        -3.4161e-03, -2.8403e-02, -2.9107e-02,  4.9888e-03,  2.2116e-02,\n",
      "         3.5659e-02, -1.8839e-02,  8.3496e-03,  7.5079e-03,  3.1471e-03,\n",
      "         8.5736e-04,  1.3202e-02, -1.8983e-03,  7.3197e-03,  4.2023e-02,\n",
      "         1.2966e-02, -7.4163e-04, -1.2303e-02,  6.0971e-03, -1.8537e-02,\n",
      "         2.6555e-03,  1.1145e-02,  8.9449e-03, -4.2551e-02,  2.0347e-02,\n",
      "        -2.1968e-02,  1.5755e-02,  3.2651e-02, -1.4397e-02, -1.5593e-02,\n",
      "        -1.0857e-02,  3.4239e-02,  1.1204e-02,  9.6133e-04, -3.6788e-03,\n",
      "        -1.4105e-02, -3.9551e-03, -1.8051e-03, -3.1724e-02, -4.2722e-02,\n",
      "        -1.1270e-02,  1.2681e-02,  8.5101e-03,  1.1072e-02,  4.2657e-03,\n",
      "        -2.7486e-02,  2.1520e-02,  1.3073e-02,  1.2607e-02,  7.2218e-03,\n",
      "        -9.3416e-03, -9.1791e-03, -2.4612e-02, -1.1098e-02, -1.4313e-02,\n",
      "         2.7089e-02,  2.0338e-02,  2.9065e-02, -3.5575e-03, -1.9400e-02,\n",
      "         3.5991e-02, -1.2482e-02, -3.2228e-02,  2.2869e-02, -3.1978e-02,\n",
      "         1.1459e-03,  7.0374e-03, -7.1646e-03, -1.9967e-02, -5.0451e-03,\n",
      "        -2.0944e-02, -1.6355e-02, -1.2205e-02, -1.8962e-02, -1.1135e-03,\n",
      "        -1.5893e-02,  7.3511e-03, -2.1275e-02,  1.1289e-02,  3.7923e-02,\n",
      "         3.2769e-02,  1.3151e-02, -2.7405e-02,  2.9294e-02,  5.4378e-02,\n",
      "        -4.9792e-02, -2.1765e-02, -1.3650e-02,  2.0421e-02, -1.7685e-03,\n",
      "        -2.2950e-03,  2.8541e-02, -1.4304e-02, -5.5749e-03, -1.6458e-02,\n",
      "         1.3992e-02, -3.2725e-02, -1.0487e-02,  2.4827e-02, -1.0016e-02,\n",
      "         2.1191e-02,  1.4332e-02,  1.5697e-02,  2.1610e-02,  2.3755e-03,\n",
      "        -1.9608e-02,  9.3479e-03,  1.2776e-03,  1.3264e-02,  2.4685e-02,\n",
      "         3.4479e-03,  3.4581e-02, -4.1667e-03,  4.5730e-03,  6.1565e-03,\n",
      "         1.4204e-02, -5.7171e-03, -9.9090e-03, -2.1901e-02, -1.0719e-02,\n",
      "        -1.6946e-02,  4.3622e-03, -6.4123e-03, -6.6170e-03,  1.2263e-02,\n",
      "        -6.3296e-04,  4.3887e-03, -6.7944e-03, -2.1518e-02,  6.2932e-03,\n",
      "        -1.7756e-02,  1.6604e-02, -1.4413e-02, -9.1322e-03, -5.1597e-02,\n",
      "        -5.5838e-02,  4.5818e-03,  4.7278e-03, -4.8152e-03,  2.7122e-02,\n",
      "        -1.0907e-02,  6.2564e-03,  2.1051e-03, -4.5361e-03, -8.6521e-03,\n",
      "        -1.9656e-03,  1.8562e-02, -9.5803e-03, -6.2706e-03,  1.4445e-03,\n",
      "        -9.7062e-03, -1.6955e-02,  5.1066e-03,  1.2707e-02,  1.2153e-02,\n",
      "         4.1739e-02,  4.4490e-03,  1.3064e-02, -3.1470e-02,  7.2348e-03,\n",
      "        -5.6469e-02,  2.3814e-02, -2.7664e-02,  2.4727e-02, -4.4481e-03,\n",
      "         2.2024e-02, -2.2392e-02,  1.3663e-02,  1.1762e-02,  6.6367e-03,\n",
      "        -2.5852e-02, -4.6860e-02, -1.5700e-02,  8.2252e-03, -1.0530e-02,\n",
      "        -1.1740e-02,  6.7627e-03, -1.3740e-03,  6.0441e-03, -1.9590e-02,\n",
      "         2.9881e-02, -2.8956e-02,  1.8921e-02, -2.3622e-03,  6.7723e-03,\n",
      "         2.0483e-02, -1.0082e-02, -1.4192e-02,  3.0110e-02, -8.6061e-03,\n",
      "        -3.3480e-03, -2.8679e-02,  7.8137e-03, -1.4634e-04, -1.7347e-02,\n",
      "        -6.4545e-03,  1.6699e-02,  8.3225e-03, -2.5203e-02,  1.7004e-02,\n",
      "         2.4822e-02,  2.1963e-02,  1.7084e-02,  1.4027e-02,  5.9627e-03,\n",
      "         3.4758e-02,  2.0575e-02, -3.6223e-02,  1.5711e-02,  1.0882e-02,\n",
      "         2.6237e-02,  1.0378e-03,  2.0628e-02, -4.3653e-02, -1.2470e-02,\n",
      "        -1.9715e-03,  2.3608e-02, -2.4613e-02, -7.7327e-03, -7.4366e-03,\n",
      "         1.7980e-02, -2.0204e-02,  4.9693e-03,  4.3906e-03, -6.6629e-04,\n",
      "        -2.0435e-03, -2.0618e-02,  2.6125e-02,  2.7373e-02, -6.9562e-03,\n",
      "         1.0647e-02, -4.2449e-03,  3.8769e-02, -2.5141e-02,  1.0105e-02,\n",
      "         4.5456e-03, -2.0793e-02,  2.3914e-02,  1.8945e-02, -6.2293e-03,\n",
      "        -2.5704e-02,  1.9131e-02, -7.2081e-03,  2.4087e-02, -1.4432e-02,\n",
      "         9.1793e-03,  3.9001e-03,  2.3789e-02,  1.8642e-02,  8.7374e-03,\n",
      "        -2.4655e-03,  4.0706e-03, -1.5795e-02, -2.5082e-02, -1.7662e-02,\n",
      "         3.9056e-02, -8.6194e-03,  1.2134e-02,  2.3894e-02, -2.0596e-02,\n",
      "         8.9698e-03,  2.2743e-03, -2.3870e-02, -4.5276e-03, -4.1324e-02,\n",
      "         4.8763e-03, -9.4382e-03,  3.1647e-02,  4.7080e-03, -9.5424e-03,\n",
      "         9.0131e-03,  3.2560e-02, -1.2826e-02,  1.0522e-02, -2.8975e-02,\n",
      "         2.8680e-03,  2.6781e-02, -2.3169e-02, -2.0567e-02,  3.9469e-02,\n",
      "         4.3787e-02, -1.2535e-02,  6.1744e-03,  5.6743e-03,  1.1203e-02,\n",
      "        -4.0210e-02, -6.8437e-03,  1.0852e-02,  1.1980e-03, -2.1349e-02,\n",
      "        -3.5117e-02, -2.8134e-02,  1.0048e-03,  1.6363e-02, -6.8578e-03,\n",
      "        -2.1572e-02, -3.3165e-03, -3.9350e-02, -2.5509e-03, -2.0549e-03,\n",
      "         1.2542e-02, -1.4749e-02, -1.2537e-02, -1.2650e-02,  5.5640e-03,\n",
      "        -8.2737e-03,  7.1049e-03,  1.4965e-02, -1.3155e-02, -1.1972e-02,\n",
      "         1.6412e-02, -1.4110e-02, -3.1754e-02,  2.5186e-03, -1.1346e-02,\n",
      "        -3.9147e-02, -1.3338e-02,  1.2003e-02,  1.7424e-02,  4.1109e-02,\n",
      "        -6.6222e-03,  3.3653e-03, -7.7769e-03,  1.4596e-02, -1.1917e-02,\n",
      "         1.4840e-02,  1.4590e-02, -2.2146e-02, -1.0082e-02, -2.4136e-02,\n",
      "         5.6353e-04,  2.4578e-03, -8.8644e-04, -2.8889e-02, -2.6615e-02,\n",
      "        -3.5692e-02, -4.3954e-02, -1.1617e-02,  1.7549e-02,  1.0480e-02,\n",
      "        -5.5475e-04,  1.2130e-02, -2.5258e-02,  2.0459e-02,  1.1580e-02,\n",
      "        -5.4927e-03, -3.1987e-03,  4.8172e-02,  3.6985e-03, -1.9733e-02,\n",
      "         8.3958e-03, -1.2547e-02, -1.9789e-02,  2.1114e-02,  2.1243e-02,\n",
      "        -2.1047e-02,  2.5549e-02, -1.5737e-02,  1.2766e-02,  1.2479e-02,\n",
      "        -2.2925e-02,  2.2800e-02, -2.9590e-03, -3.4083e-02,  3.6178e-02,\n",
      "        -2.9707e-03,  8.5603e-03,  3.6726e-02,  1.4908e-02, -4.3744e-03,\n",
      "         1.4859e-03,  9.3566e-03,  3.1971e-03, -1.2953e-02,  7.8685e-03,\n",
      "         2.2064e-02, -9.4247e-03, -2.4086e-02,  9.0309e-03, -1.5737e-02,\n",
      "         1.4105e-02,  3.7443e-02,  3.6302e-03, -7.1980e-03,  3.7118e-03,\n",
      "        -8.2493e-03,  1.4098e-02,  7.0921e-03,  4.6230e-02,  3.1994e-03,\n",
      "         1.3840e-02,  9.1660e-03,  6.9958e-04, -4.1017e-02, -1.0509e-02,\n",
      "        -4.0696e-04,  5.6418e-05,  5.6994e-03, -8.8750e-03,  7.1601e-03,\n",
      "         1.4024e-03,  2.3657e-02, -2.0159e-02,  3.6515e-03, -4.3228e-02,\n",
      "        -5.1450e-03, -3.3128e-02,  2.7589e-02,  5.7775e-03,  1.6056e-04,\n",
      "        -7.0858e-03, -3.6594e-03,  5.8105e-03,  3.8881e-02, -2.8135e-02,\n",
      "         8.9568e-03,  1.0334e-02,  1.3660e-02,  1.3718e-02, -4.3951e-03,\n",
      "        -1.3927e-02,  2.2005e-02, -1.4224e-02,  2.5396e-02, -8.2557e-05,\n",
      "         4.3469e-03, -9.5510e-03,  8.9576e-03,  6.5804e-03,  1.7939e-02,\n",
      "        -3.7446e-02,  1.5860e-02, -5.4142e-03, -5.7763e-03, -7.7142e-03,\n",
      "         2.5220e-02, -3.3864e-03, -1.6713e-02,  1.5125e-02, -8.3833e-04,\n",
      "         1.6523e-02, -5.9076e-03,  1.7054e-02,  3.4141e-02,  1.6174e-02,\n",
      "         1.9687e-02,  4.4884e-04,  2.0042e-02,  4.2505e-02,  2.8204e-02,\n",
      "         2.1606e-02,  2.6063e-02,  1.3907e-02,  1.2930e-02, -3.0866e-02,\n",
      "         5.0432e-03,  1.1302e-02,  5.3106e-02, -5.5303e-03, -3.4655e-02,\n",
      "        -3.0414e-02, -9.6414e-03, -3.6551e-03,  2.7217e-02, -1.7886e-02,\n",
      "         1.1847e-03, -3.7392e-03,  1.1025e-02,  2.5173e-02, -2.1654e-02,\n",
      "        -2.8427e-02, -7.1434e-03,  1.0564e-02,  9.3391e-03, -1.7183e-02,\n",
      "        -4.3075e-03,  1.0773e-02, -1.9394e-02, -2.8056e-03,  1.9166e-02,\n",
      "         3.2871e-02,  5.0479e-03, -2.9330e-04,  3.9535e-03, -2.7124e-02,\n",
      "        -9.5595e-03, -1.4580e-02, -2.8144e-02, -2.0285e-02,  3.0318e-03,\n",
      "        -1.0859e-02,  4.7247e-02,  5.1149e-02,  1.2092e-02, -1.0596e-02,\n",
      "         5.5654e-03, -1.8144e-02,  2.8387e-03,  7.2822e-03, -4.1461e-02,\n",
      "        -2.1106e-02,  5.9049e-03,  1.3728e-02,  6.4501e-03,  1.4034e-02,\n",
      "         5.5666e-03,  8.1453e-03,  1.0773e-02,  3.7500e-03, -5.1273e-03,\n",
      "         6.3735e-04,  6.1086e-03,  2.8533e-02, -2.5328e-02,  2.2880e-02,\n",
      "         1.5753e-02, -1.0802e-03, -3.2648e-02,  2.6671e-02,  2.7983e-02,\n",
      "        -1.5982e-02,  3.0112e-02, -1.3856e-02,  2.0904e-02, -3.8872e-02,\n",
      "         1.3377e-02,  2.8429e-02,  1.1742e-02,  1.8823e-02,  7.9092e-03,\n",
      "        -3.4709e-02,  2.8395e-02,  1.6894e-03, -8.5795e-03,  7.6706e-03,\n",
      "         3.6206e-04,  3.0456e-02, -1.4453e-02, -2.6068e-03,  1.5731e-02,\n",
      "        -2.0161e-02,  1.3869e-02, -1.8528e-02, -1.5240e-02, -1.7170e-03,\n",
      "         9.6028e-04, -1.0281e-02,  1.8175e-02, -3.9744e-03, -3.2604e-02,\n",
      "         7.6997e-03,  6.9114e-03, -3.9953e-02,  3.8693e-02, -8.0147e-03,\n",
      "        -1.8977e-02, -1.5057e-02, -2.2302e-02, -2.3139e-02,  2.6660e-02,\n",
      "         1.1702e-02, -1.8669e-02, -5.2346e-03,  3.3326e-02, -1.8824e-02,\n",
      "         1.7594e-03,  2.9061e-02,  9.1456e-03,  3.2663e-02,  1.9909e-03,\n",
      "        -1.3920e-02,  8.9997e-04,  2.4818e-02, -2.2576e-02, -3.9156e-04,\n",
      "        -3.1993e-02, -1.2388e-03,  2.5118e-02, -3.6050e-03,  2.7128e-02,\n",
      "         3.5082e-03, -1.1871e-02,  2.1010e-02,  3.6981e-03,  1.2098e-02,\n",
      "         1.7107e-03,  3.1803e-03, -2.2254e-02,  4.1379e-02,  4.4307e-02,\n",
      "         1.9778e-02, -3.4531e-03,  9.4171e-03, -5.3216e-03, -6.3030e-03,\n",
      "         1.2652e-02,  7.4879e-03,  2.1181e-03,  1.6526e-02,  1.3124e-02,\n",
      "         1.1814e-03,  1.6964e-02,  1.1123e-02,  3.4286e-02,  3.0051e-03,\n",
      "        -1.7963e-02, -1.9900e-02,  2.4322e-02,  2.8055e-02,  1.3928e-03,\n",
      "         2.0114e-02,  2.7763e-02, -3.5902e-02], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = RobertaConfig(\n",
    "            vocab_size=50265,\n",
    "            max_position_embeddings=514,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1\n",
    "        )\n",
    "PretrainedModel = RobertaModel(config=config)\n",
    "embeddings = PretrainedModel.embeddings.word_embeddings(train_dataset[6]['input_ids'])\n",
    "print(embeddings)\n",
    "print(embeddings.size())\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class DownstreamRegression is called\n"
     ]
    }
   ],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    print('Class DownstreamRegression is called')\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        print(f'Finger print is:')\n",
    "        print(logits)\n",
    "        print(logits.size())\n",
    "        output = self.Regressor(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, loss_fn, train_dataloader, device):\n",
    "    print('Train func is called')\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        print(f'Batch and Step: {step}')\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        print('Batch')\n",
    "        print(input_ids)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        prop = batch[\"prop\"].to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask).float()\n",
    "        print('Output for step',step)\n",
    "        print(outputs)\n",
    "        loss = loss_fn(outputs.squeeze(), prop.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print('End of one step')\n",
    "        print('--------------------------------------------------------------------------------')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters for scheduler\"\"\"\n",
    "steps_per_epoch = train_data.shape[0] // 1\n",
    "training_steps = steps_per_epoch * 1\n",
    "warmup_steps = int(training_steps * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.PretrainedModel.parameters(), \"lr\":  0.00005,\n",
    "                         \"weight_decay\": 0.0},\n",
    "                        {\"params\": model.Regressor.parameters(), \"lr\": 0.0001,\n",
    "                         \"weight_decay\": 0.01},\n",
    "                    ],\n",
    "    \t\t\t\tno_deprecation_warning=True\n",
    "                )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                        num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapture.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 9\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mcap\u001b[49m\u001b[38;5;241m.\u001b[39mstdout)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(\"Training epoch: %s/%s\" % (epoch+1, 1))\n",
    "    train(model, optimizer, scheduler, loss_fn, train_dataloader, device)\n",
    "    print('End of one Epoch')\n",
    "    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||')\n",
    "\n",
    "\n",
    "with open('capture.txt','w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "def split(file_path):\n",
    "    dataset = pd.read_csv(file_path, header=None).values\n",
    "    train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1)\n",
    "    return train_data, valid_data\n",
    "\n",
    "config = RobertaConfig(\n",
    "        vocab_size=50265,\n",
    "        max_position_embeddings=514,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=6,\n",
    "        type_vocab_size=1,\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "    )\n",
    "\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=175)\n",
    "model = RobertaForMaskedLM(config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*c1ccc(C(=O)c2ccc(*)cc2)cc1\n",
      "['*', 'c', '1', 'c', 'c', 'c', '(', 'C', '(', '=', 'O', ')', 'c', '2', 'c', 'c', 'c', '(', '*', ')', 'c', 'c', '2', ')', 'c', 'c', '1']\n",
      "tensor([   0, 3226,  438,  134,  438,  438,  438, 1640,  347, 1640, 5214,  673,\n",
      "          43,  438,  176,  438,  438,  438, 1640, 3226,   43,  438,  438,  176,\n",
      "          43,  438,  438,  134,    2,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1])\n",
      "torch.Size([175])\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = split('data/P1_practice.csv')\n",
    "data_train = LoadPretrainData(tokenizer=tokenizer, dataset=train_data, blocksize=175)\n",
    "data_valid = LoadPretrainData(tokenizer=tokenizer, dataset=valid_data, blocksize=175)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15 \n",
    "    )\n",
    "text = train_data[3][0]\n",
    "print(text)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print(data_train[3]['input_ids'])\n",
    "print(data_train[3]['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0053, -0.0112,  0.0057,  ..., -0.0167, -0.0335, -0.0034],\n",
      "        [-0.0216, -0.0323, -0.0052,  ...,  0.0201, -0.0015,  0.0232],\n",
      "        [ 0.0022, -0.0141, -0.0197,  ...,  0.0051,  0.0010,  0.0282],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([175, 768])\n",
      "tensor([ 5.2708e-03, -1.1235e-02,  5.6577e-03,  1.1266e-02, -2.9829e-03,\n",
      "        -5.5621e-03,  1.6243e-02,  8.6826e-03, -2.2348e-04,  5.7683e-03,\n",
      "         3.8777e-04,  2.8037e-02,  2.0732e-02, -1.4273e-02, -4.1919e-03,\n",
      "        -1.1770e-02, -1.0084e-03, -8.0002e-03,  2.1818e-02, -1.7368e-02,\n",
      "        -4.6848e-02, -4.8295e-02, -2.1069e-02,  2.7306e-02,  2.6023e-02,\n",
      "         8.9658e-03, -1.6958e-02, -4.7396e-02,  1.3298e-02,  3.6558e-02,\n",
      "        -1.5527e-02,  4.5800e-04, -2.4624e-03, -1.1655e-02,  1.1905e-02,\n",
      "         7.0082e-03, -2.9437e-02,  5.9958e-03, -2.5171e-02, -1.5966e-02,\n",
      "        -3.8125e-02,  1.6154e-02, -1.4723e-02, -2.7667e-02,  6.0385e-03,\n",
      "         1.7325e-02, -7.3535e-03,  2.0401e-02, -4.8671e-02, -7.6701e-03,\n",
      "        -5.3182e-02, -1.1439e-02, -1.2698e-03, -1.9473e-02,  1.9493e-02,\n",
      "         2.5950e-02,  1.3904e-02,  2.7939e-03,  8.2065e-03,  9.7295e-03,\n",
      "        -1.8104e-02, -3.3477e-02,  4.0423e-03, -1.1150e-02, -1.2358e-02,\n",
      "        -2.6212e-02,  2.7943e-02,  3.0751e-02,  1.1805e-02, -1.9308e-02,\n",
      "         9.0326e-03,  1.2002e-02, -4.4341e-03,  5.6531e-03, -1.3417e-02,\n",
      "        -1.3339e-02, -1.5806e-02,  3.2640e-02, -3.9522e-02,  2.5959e-02,\n",
      "        -2.7022e-03, -1.3933e-02, -1.3850e-02, -2.7713e-02, -1.9804e-02,\n",
      "         8.0474e-03, -8.9185e-03, -1.8050e-02,  1.1342e-02,  4.8881e-03,\n",
      "        -5.7296e-03, -7.7828e-03, -4.4245e-02, -5.7308e-03, -1.0432e-02,\n",
      "        -2.8128e-02,  2.0033e-03,  2.0870e-02,  2.0568e-02, -1.9381e-02,\n",
      "         4.8397e-03,  1.9617e-02, -5.2260e-02, -2.3487e-02,  2.0205e-02,\n",
      "         1.9561e-02, -5.1784e-03,  2.3675e-02, -2.2458e-02, -2.3540e-02,\n",
      "        -2.4380e-02,  2.8124e-02,  2.5951e-02, -1.9653e-02, -8.0673e-03,\n",
      "        -2.0276e-02, -1.7629e-03, -4.2714e-03, -2.2961e-02, -7.7580e-03,\n",
      "        -7.4547e-03,  3.9678e-02, -3.5373e-03,  2.1009e-02,  3.0318e-02,\n",
      "         5.9589e-03,  1.9076e-02,  8.3233e-03,  1.1477e-02,  1.8007e-03,\n",
      "        -1.2117e-03,  2.2997e-02,  1.7416e-02, -5.8983e-03, -1.7438e-02,\n",
      "         1.1948e-02, -9.7944e-03,  6.3896e-04,  2.5187e-02,  2.3113e-02,\n",
      "        -6.2957e-03,  1.8535e-02,  1.6432e-02, -1.3860e-02,  2.4848e-02,\n",
      "        -1.3005e-02,  2.4115e-03, -1.1201e-02, -9.4844e-03, -5.3365e-05,\n",
      "         1.3450e-02, -1.2544e-02, -1.8442e-02,  1.4434e-02, -3.0627e-03,\n",
      "         1.5209e-03,  4.9564e-03,  1.5230e-02, -1.1908e-02, -1.2541e-02,\n",
      "         8.0157e-05, -1.8860e-03, -5.1844e-04,  9.3328e-03,  2.5972e-02,\n",
      "        -2.8875e-02,  1.2556e-02,  4.8045e-03, -1.6032e-02, -1.4402e-02,\n",
      "         8.2009e-03,  2.4139e-02,  6.2186e-03, -5.8237e-03, -4.3488e-02,\n",
      "         1.2398e-02,  2.0974e-02,  2.7277e-03, -1.3682e-02,  3.0548e-02,\n",
      "         1.7845e-02, -2.0376e-02,  8.7288e-04,  6.0020e-04, -1.9897e-02,\n",
      "        -2.6182e-03, -1.1652e-02, -3.9616e-03, -2.2633e-03, -2.2094e-02,\n",
      "         1.3952e-02,  1.4947e-02,  1.2178e-02, -1.5487e-02, -1.3098e-02,\n",
      "         9.3048e-03,  2.3344e-02,  7.0364e-03, -6.6843e-03, -1.7467e-02,\n",
      "         3.7040e-02,  1.7086e-02, -1.8689e-02,  5.7264e-03,  1.5962e-02,\n",
      "         5.9166e-03,  2.0941e-02,  8.6760e-03, -4.5885e-03,  5.4164e-03,\n",
      "        -1.6451e-02,  7.2964e-03,  1.2373e-03,  1.9629e-02, -1.4547e-02,\n",
      "        -2.5364e-02,  4.6821e-02, -6.6946e-03, -1.2652e-02, -1.5317e-02,\n",
      "        -9.8165e-03, -1.2223e-03,  3.0123e-02,  6.7634e-03, -1.5423e-02,\n",
      "         5.9103e-03, -1.7822e-02, -2.1755e-02,  1.1527e-02, -1.4179e-02,\n",
      "         1.2197e-02,  1.2755e-02, -8.0254e-03, -1.0578e-03,  6.9242e-03,\n",
      "         9.8051e-03,  1.2760e-02,  3.6895e-02,  2.2671e-02,  2.1226e-03,\n",
      "         2.6838e-02,  5.2244e-02, -1.6012e-02,  1.5714e-03, -8.9777e-03,\n",
      "        -1.1362e-02,  3.1571e-02, -9.7496e-03, -1.6692e-02, -1.3146e-02,\n",
      "         2.3911e-02, -3.8294e-02,  5.1449e-02, -8.7647e-03,  5.4480e-03,\n",
      "        -5.6603e-04, -6.1895e-02,  2.4381e-02, -5.2903e-03,  2.8668e-02,\n",
      "        -1.7564e-02, -9.1206e-03,  2.0344e-02,  3.1199e-02, -1.2316e-02,\n",
      "        -2.7188e-02,  1.4872e-02, -5.3730e-03,  1.3645e-02,  2.9599e-02,\n",
      "         4.6326e-03, -3.5972e-02, -1.2664e-02, -8.6219e-03,  3.5940e-02,\n",
      "         1.0835e-02,  9.4334e-03, -7.1333e-03,  1.4771e-02,  9.1135e-05,\n",
      "         2.7944e-02,  8.7865e-03,  3.0293e-02, -3.1222e-02, -2.0307e-02,\n",
      "         1.5249e-02,  3.3144e-02,  9.6959e-03,  2.5926e-02,  2.4891e-02,\n",
      "         1.4372e-02,  3.0898e-02,  1.8487e-02, -2.1510e-02,  3.6125e-03,\n",
      "         2.8801e-02,  3.9493e-03, -2.1188e-02, -2.1089e-03,  9.5917e-03,\n",
      "        -6.8523e-03, -1.0695e-02, -1.9322e-02, -2.2625e-02,  1.5805e-03,\n",
      "        -2.1996e-02,  3.0394e-04, -2.3256e-02,  2.6073e-02,  2.9043e-02,\n",
      "         3.8731e-03,  3.0138e-02,  2.5796e-02,  1.3998e-02, -1.2611e-02,\n",
      "        -1.3969e-02,  1.0097e-02,  3.1613e-02, -2.1606e-02, -7.2648e-03,\n",
      "        -4.0364e-02, -2.3875e-03,  1.3022e-02, -1.1719e-02, -1.9831e-02,\n",
      "         2.8551e-02,  1.4267e-02, -1.4895e-02,  6.1696e-03,  4.7871e-02,\n",
      "        -2.1891e-02,  1.3176e-03,  1.2547e-02, -8.6434e-04,  1.2199e-02,\n",
      "         2.8250e-03,  3.3807e-02, -3.0809e-03, -1.6542e-02,  2.9478e-02,\n",
      "        -4.2177e-02,  3.8292e-02,  2.3053e-02,  8.3928e-03,  1.9049e-02,\n",
      "         4.1976e-03,  2.8763e-02,  1.7278e-02, -9.8706e-03,  9.8630e-03,\n",
      "        -3.6553e-02,  3.7226e-02,  9.3426e-03, -1.9923e-02, -1.1948e-02,\n",
      "         1.2273e-02,  3.4881e-02, -1.2746e-02, -3.5792e-02,  1.5235e-02,\n",
      "        -8.1827e-03,  6.9150e-04, -6.9373e-03, -3.6841e-03,  7.9332e-03,\n",
      "         1.0926e-03, -5.9856e-02,  3.7466e-02, -4.3834e-03, -6.5751e-03,\n",
      "        -2.3554e-02,  2.2842e-02,  1.7052e-02, -1.5464e-02,  1.6553e-02,\n",
      "        -2.5043e-03, -4.7762e-03, -1.7490e-03,  1.8550e-02, -1.0938e-02,\n",
      "        -5.6185e-02,  2.1524e-02,  2.4077e-02, -1.4014e-02, -1.1681e-02,\n",
      "        -2.4684e-03,  2.5400e-02,  3.8367e-03, -4.4290e-03,  4.6207e-03,\n",
      "         7.7439e-03,  1.5946e-03,  9.0622e-03, -2.7971e-02, -3.1321e-02,\n",
      "         1.7527e-02, -1.2732e-02, -9.0944e-04,  3.4699e-02, -7.8154e-03,\n",
      "        -6.2245e-03,  9.3965e-03, -2.0491e-02, -1.7156e-02, -1.8424e-03,\n",
      "         1.0202e-02,  5.8296e-03, -3.0070e-02, -1.9180e-02, -1.3134e-02,\n",
      "         5.7080e-05, -2.6518e-02,  1.7739e-02, -1.6966e-02,  3.5336e-03,\n",
      "        -2.2902e-02,  1.9152e-02, -2.8266e-02, -5.5507e-03,  1.6244e-03,\n",
      "        -1.2431e-02, -1.1277e-02,  8.3417e-03,  9.0144e-04,  9.2466e-03,\n",
      "         6.5190e-03,  3.5553e-03, -1.2856e-02,  2.8434e-02, -1.5262e-02,\n",
      "        -3.1754e-02, -1.6531e-04,  2.6785e-03,  1.0054e-02,  5.2712e-03,\n",
      "        -3.3605e-04,  7.8967e-03, -2.3059e-02,  3.2919e-04,  1.3707e-02,\n",
      "         2.3882e-02,  2.1535e-02, -1.5057e-02,  1.0043e-02,  9.3205e-04,\n",
      "         1.6663e-02,  3.8070e-02, -1.1739e-02,  2.1317e-02,  1.1239e-02,\n",
      "         1.5436e-02, -1.7164e-02,  3.1991e-03,  1.6135e-02, -9.7519e-03,\n",
      "        -1.4115e-02,  1.3349e-02, -1.7015e-03,  2.1929e-02,  6.2107e-03,\n",
      "         7.7437e-03,  1.9817e-03, -3.8820e-02,  4.3551e-03,  1.5703e-02,\n",
      "         8.2197e-03,  2.1460e-03,  6.4206e-02,  4.6954e-03,  8.0008e-03,\n",
      "        -1.4328e-02,  2.0307e-02,  6.9003e-03, -1.8515e-02,  3.0615e-02,\n",
      "        -2.6383e-02,  1.3201e-03,  3.4670e-02, -7.3521e-03,  1.1245e-02,\n",
      "        -2.7997e-02,  5.0895e-03, -5.0841e-03, -6.4043e-03,  1.6465e-02,\n",
      "        -2.2907e-02,  1.0019e-02,  2.1121e-02, -1.6175e-02, -8.4355e-03,\n",
      "        -8.0525e-03,  7.8649e-03,  4.3276e-03, -1.7562e-02,  2.0541e-02,\n",
      "         1.0995e-02, -2.1364e-02, -2.8831e-02,  1.3755e-02,  3.7096e-02,\n",
      "        -9.6673e-03,  2.7405e-02,  2.2964e-02, -3.4681e-03,  1.2929e-02,\n",
      "        -1.2280e-02,  4.7291e-02,  1.4982e-02,  1.2883e-02,  2.5075e-02,\n",
      "         2.9448e-02,  2.2341e-03,  1.4845e-02, -2.1338e-02, -1.5438e-02,\n",
      "         3.0009e-03, -8.3034e-03, -1.9852e-02,  3.5241e-02, -1.0482e-03,\n",
      "        -1.2432e-06,  5.1718e-02,  9.1332e-03,  1.0001e-02,  1.2446e-03,\n",
      "         2.1023e-02,  2.8226e-02, -1.0850e-02, -1.7821e-02, -2.1451e-02,\n",
      "         6.3786e-02, -2.9398e-02, -2.7208e-03, -9.5247e-03, -5.0697e-02,\n",
      "         8.3109e-03,  1.9681e-02, -4.2793e-02,  3.1706e-02,  1.2312e-02,\n",
      "         4.2878e-02, -2.0778e-02, -1.0463e-02,  2.8884e-02,  1.8784e-03,\n",
      "         7.9666e-03, -5.6760e-03,  1.8217e-02,  1.9622e-02, -1.9324e-02,\n",
      "         6.4811e-02,  4.6581e-02,  6.1068e-03, -3.9134e-02, -9.9301e-03,\n",
      "         7.4607e-03,  3.9257e-03, -8.5250e-03,  1.6001e-02, -6.1379e-03,\n",
      "        -1.6684e-02,  7.9172e-03, -2.1863e-02,  5.8274e-04, -8.1897e-03,\n",
      "         5.9365e-03,  1.5663e-04,  2.9971e-02, -1.1753e-02, -2.2487e-02,\n",
      "        -1.6925e-02, -4.2674e-02, -8.2268e-03,  2.8326e-03,  1.7045e-02,\n",
      "        -2.9045e-03,  2.1001e-02,  4.6689e-02, -1.3563e-02, -1.2685e-02,\n",
      "         3.8677e-03, -1.7062e-02, -1.9864e-02,  1.5170e-02, -2.6967e-02,\n",
      "         5.6602e-03,  2.2998e-02, -1.7259e-02,  5.8046e-02,  2.4971e-02,\n",
      "         2.8720e-02,  3.7011e-02,  1.4900e-02, -2.3608e-03,  2.3832e-03,\n",
      "         1.7278e-02,  1.1626e-02, -1.7270e-02, -2.2269e-03,  1.7215e-02,\n",
      "        -2.2633e-02, -2.2168e-02, -9.8064e-03, -2.5414e-02,  2.6788e-04,\n",
      "        -3.4358e-03,  3.8652e-02,  1.4286e-02, -2.4731e-02, -2.8461e-02,\n",
      "        -1.3991e-02,  2.7166e-02,  4.5671e-02,  7.4632e-03,  3.4500e-02,\n",
      "        -2.4171e-02,  1.7938e-02,  9.5279e-03, -3.2095e-02,  1.3659e-02,\n",
      "        -1.6621e-02,  2.0231e-02,  6.2073e-04,  3.1514e-02,  1.9065e-02,\n",
      "         8.9387e-03, -3.1050e-02, -3.6116e-02,  4.2235e-02,  3.0003e-02,\n",
      "        -1.3666e-02, -3.6351e-02,  5.0930e-03, -9.2235e-03,  1.2585e-02,\n",
      "        -2.4998e-02, -9.6011e-03,  1.6284e-02, -3.4421e-02,  1.0699e-02,\n",
      "         3.4793e-02,  2.5068e-02, -3.0461e-02, -1.8656e-02,  2.6950e-03,\n",
      "         2.5415e-03, -3.3264e-02, -5.3104e-02,  3.4529e-02,  2.7863e-02,\n",
      "         1.6410e-02, -1.0660e-02, -5.0803e-03,  1.9313e-02, -3.0421e-02,\n",
      "         2.1821e-03,  3.3170e-02, -3.9551e-03, -1.0412e-02, -1.0449e-02,\n",
      "        -2.7947e-02,  9.0988e-03, -4.5712e-02, -6.2853e-03, -3.1397e-03,\n",
      "        -6.8326e-03, -3.9230e-02,  3.2112e-02,  9.3532e-03,  1.6681e-02,\n",
      "         1.4998e-02, -1.3442e-02,  2.1690e-03,  3.3674e-02, -3.8530e-02,\n",
      "         3.2125e-03,  2.3369e-02,  4.0559e-02, -3.7626e-02, -5.0498e-03,\n",
      "         2.6259e-02,  2.7639e-02, -1.7116e-02, -7.3282e-03, -3.1398e-03,\n",
      "         1.2465e-02,  7.6900e-03, -5.4800e-03,  9.8052e-03,  1.2707e-02,\n",
      "        -8.3633e-04, -1.2883e-02,  6.1085e-03,  6.0799e-03, -6.6079e-03,\n",
      "         9.9036e-03, -3.4157e-03, -1.2863e-02,  1.1079e-03,  3.5486e-02,\n",
      "        -6.5416e-03, -1.8916e-04,  1.2817e-02, -1.6457e-02,  8.3011e-03,\n",
      "         8.6352e-03, -1.5163e-03, -3.2718e-02,  1.1449e-02,  3.3383e-03,\n",
      "         1.9173e-02, -4.4255e-03,  1.5905e-02,  6.4360e-03, -1.8645e-02,\n",
      "         1.2422e-03, -2.6693e-02,  2.7207e-02,  2.7319e-02, -1.9180e-02,\n",
      "         3.4634e-04,  2.0409e-02, -6.0049e-03,  1.5752e-02,  3.5899e-02,\n",
      "        -6.9398e-03,  6.0774e-03,  1.2916e-02,  1.0789e-02, -8.8027e-03,\n",
      "         3.3522e-02, -3.6228e-02, -2.9378e-03, -1.0054e-03, -1.4277e-02,\n",
      "         4.8017e-02, -7.8250e-03,  2.4320e-02, -5.1371e-03, -8.2693e-03,\n",
      "        -1.2246e-02,  1.4783e-02, -2.2995e-02, -1.1320e-02, -6.2526e-03,\n",
      "        -7.1885e-03, -2.4300e-02,  1.0096e-02, -2.5505e-02,  5.0303e-02,\n",
      "         6.3256e-03, -5.3435e-03, -3.3861e-02, -1.3796e-03, -1.4765e-02,\n",
      "         2.6233e-02,  3.0011e-02, -7.1011e-03, -4.2837e-03,  1.1278e-02,\n",
      "        -1.0341e-02,  2.4217e-03,  2.7430e-02,  1.8074e-02, -1.9548e-02,\n",
      "        -1.6696e-02, -3.3536e-02, -3.4175e-03], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.roberta.embeddings.word_embeddings(data_train[3]['input_ids'])\n",
    "print(embeddings)\n",
    "print(embeddings.size())\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
