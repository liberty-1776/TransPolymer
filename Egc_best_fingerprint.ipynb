{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253a74ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mm22d016/miniconda3/envs/TransPolymer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18acc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = torch.empty(370, 768)\n",
    "pred_output = torch.empty(370,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec056da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,step):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        fingerprint[step] = logits\n",
    "        output = self.Regressor(logits)\n",
    "        return output\n",
    "    \n",
    "def test(model, loss_fn, train_dataloader,device):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print(f'Smiles: {step+1}')\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            prop = batch[\"prop\"].to(device).float()\n",
    "            outputs = model(input_ids, attention_mask,step).float()\n",
    "            pred_output[step] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bb2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/Ei.csv')\n",
    "original_output = train_data['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02b3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpt/pretrain.pt were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "saved_state = torch.load('ckpt/Ei/Ei_best_model.pt')\n",
    "vocab_sup = pd.read_csv('data/vocab/vocab_sup_PE_I.csv', header=None).values.flatten().tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "PretrainedModel = RobertaModel.from_pretrained('ckpt/pretrain.pt')\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "tokenizer.add_tokens(vocab_sup)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "\n",
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "model.load_state_dict(saved_state['model'])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6bcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Smiles: 1\n",
      "Smiles: 2\n",
      "Smiles: 3\n",
      "Smiles: 4\n",
      "Smiles: 5\n",
      "Smiles: 6\n",
      "Smiles: 7\n",
      "Smiles: 8\n",
      "Smiles: 9\n",
      "Smiles: 10\n",
      "Smiles: 11\n",
      "Smiles: 12\n",
      "Smiles: 13\n",
      "Smiles: 14\n",
      "Smiles: 15\n",
      "Smiles: 16\n",
      "Smiles: 17\n",
      "Smiles: 18\n",
      "Smiles: 19\n",
      "Smiles: 20\n",
      "Smiles: 21\n",
      "Smiles: 22\n",
      "Smiles: 23\n",
      "Smiles: 24\n",
      "Smiles: 25\n",
      "Smiles: 26\n",
      "Smiles: 27\n",
      "Smiles: 28\n",
      "Smiles: 29\n",
      "Smiles: 30\n",
      "Smiles: 31\n",
      "Smiles: 32\n",
      "Smiles: 33\n",
      "Smiles: 34\n",
      "Smiles: 35\n",
      "Smiles: 36\n",
      "Smiles: 37\n",
      "Smiles: 38\n",
      "Smiles: 39\n",
      "Smiles: 40\n",
      "Smiles: 41\n",
      "Smiles: 42\n",
      "Smiles: 43\n",
      "Smiles: 44\n",
      "Smiles: 45\n",
      "Smiles: 46\n",
      "Smiles: 47\n",
      "Smiles: 48\n",
      "Smiles: 49\n",
      "Smiles: 50\n",
      "Smiles: 51\n",
      "Smiles: 52\n",
      "Smiles: 53\n",
      "Smiles: 54\n",
      "Smiles: 55\n",
      "Smiles: 56\n",
      "Smiles: 57\n",
      "Smiles: 58\n",
      "Smiles: 59\n",
      "Smiles: 60\n",
      "Smiles: 61\n",
      "Smiles: 62\n",
      "Smiles: 63\n",
      "Smiles: 64\n",
      "Smiles: 65\n",
      "Smiles: 66\n",
      "Smiles: 67\n",
      "Smiles: 68\n",
      "Smiles: 69\n",
      "Smiles: 70\n",
      "Smiles: 71\n",
      "Smiles: 72\n",
      "Smiles: 73\n",
      "Smiles: 74\n",
      "Smiles: 75\n",
      "Smiles: 76\n",
      "Smiles: 77\n",
      "Smiles: 78\n",
      "Smiles: 79\n",
      "Smiles: 80\n",
      "Smiles: 81\n",
      "Smiles: 82\n",
      "Smiles: 83\n",
      "Smiles: 84\n",
      "Smiles: 85\n",
      "Smiles: 86\n",
      "Smiles: 87\n",
      "Smiles: 88\n",
      "Smiles: 89\n",
      "Smiles: 90\n",
      "Smiles: 91\n",
      "Smiles: 92\n",
      "Smiles: 93\n",
      "Smiles: 94\n",
      "Smiles: 95\n",
      "Smiles: 96\n",
      "Smiles: 97\n",
      "Smiles: 98\n",
      "Smiles: 99\n",
      "Smiles: 100\n",
      "Smiles: 101\n",
      "Smiles: 102\n",
      "Smiles: 103\n",
      "Smiles: 104\n",
      "Smiles: 105\n",
      "Smiles: 106\n",
      "Smiles: 107\n",
      "Smiles: 108\n",
      "Smiles: 109\n",
      "Smiles: 110\n",
      "Smiles: 111\n",
      "Smiles: 112\n",
      "Smiles: 113\n",
      "Smiles: 114\n",
      "Smiles: 115\n",
      "Smiles: 116\n",
      "Smiles: 117\n",
      "Smiles: 118\n",
      "Smiles: 119\n",
      "Smiles: 120\n",
      "Smiles: 121\n",
      "Smiles: 122\n",
      "Smiles: 123\n",
      "Smiles: 124\n",
      "Smiles: 125\n",
      "Smiles: 126\n",
      "Smiles: 127\n",
      "Smiles: 128\n",
      "Smiles: 129\n",
      "Smiles: 130\n",
      "Smiles: 131\n",
      "Smiles: 132\n",
      "Smiles: 133\n",
      "Smiles: 134\n",
      "Smiles: 135\n",
      "Smiles: 136\n",
      "Smiles: 137\n",
      "Smiles: 138\n",
      "Smiles: 139\n",
      "Smiles: 140\n",
      "Smiles: 141\n",
      "Smiles: 142\n",
      "Smiles: 143\n",
      "Smiles: 144\n",
      "Smiles: 145\n",
      "Smiles: 146\n",
      "Smiles: 147\n",
      "Smiles: 148\n",
      "Smiles: 149\n",
      "Smiles: 150\n",
      "Smiles: 151\n",
      "Smiles: 152\n",
      "Smiles: 153\n",
      "Smiles: 154\n",
      "Smiles: 155\n",
      "Smiles: 156\n",
      "Smiles: 157\n",
      "Smiles: 158\n",
      "Smiles: 159\n",
      "Smiles: 160\n",
      "Smiles: 161\n",
      "Smiles: 162\n",
      "Smiles: 163\n",
      "Smiles: 164\n",
      "Smiles: 165\n",
      "Smiles: 166\n",
      "Smiles: 167\n",
      "Smiles: 168\n",
      "Smiles: 169\n",
      "Smiles: 170\n",
      "Smiles: 171\n",
      "Smiles: 172\n",
      "Smiles: 173\n",
      "Smiles: 174\n",
      "Smiles: 175\n",
      "Smiles: 176\n",
      "Smiles: 177\n",
      "Smiles: 178\n",
      "Smiles: 179\n",
      "Smiles: 180\n",
      "Smiles: 181\n",
      "Smiles: 182\n",
      "Smiles: 183\n",
      "Smiles: 184\n",
      "Smiles: 185\n",
      "Smiles: 186\n",
      "Smiles: 187\n",
      "Smiles: 188\n",
      "Smiles: 189\n",
      "Smiles: 190\n",
      "Smiles: 191\n",
      "Smiles: 192\n",
      "Smiles: 193\n",
      "Smiles: 194\n",
      "Smiles: 195\n",
      "Smiles: 196\n",
      "Smiles: 197\n",
      "Smiles: 198\n",
      "Smiles: 199\n",
      "Smiles: 200\n",
      "Smiles: 201\n",
      "Smiles: 202\n",
      "Smiles: 203\n",
      "Smiles: 204\n",
      "Smiles: 205\n",
      "Smiles: 206\n",
      "Smiles: 207\n",
      "Smiles: 208\n",
      "Smiles: 209\n",
      "Smiles: 210\n",
      "Smiles: 211\n",
      "Smiles: 212\n",
      "Smiles: 213\n",
      "Smiles: 214\n",
      "Smiles: 215\n",
      "Smiles: 216\n",
      "Smiles: 217\n",
      "Smiles: 218\n",
      "Smiles: 219\n",
      "Smiles: 220\n",
      "Smiles: 221\n",
      "Smiles: 222\n",
      "Smiles: 223\n",
      "Smiles: 224\n",
      "Smiles: 225\n",
      "Smiles: 226\n",
      "Smiles: 227\n",
      "Smiles: 228\n",
      "Smiles: 229\n",
      "Smiles: 230\n",
      "Smiles: 231\n",
      "Smiles: 232\n",
      "Smiles: 233\n",
      "Smiles: 234\n",
      "Smiles: 235\n",
      "Smiles: 236\n",
      "Smiles: 237\n",
      "Smiles: 238\n",
      "Smiles: 239\n",
      "Smiles: 240\n",
      "Smiles: 241\n",
      "Smiles: 242\n",
      "Smiles: 243\n",
      "Smiles: 244\n",
      "Smiles: 245\n",
      "Smiles: 246\n",
      "Smiles: 247\n",
      "Smiles: 248\n",
      "Smiles: 249\n",
      "Smiles: 250\n",
      "Smiles: 251\n",
      "Smiles: 252\n",
      "Smiles: 253\n",
      "Smiles: 254\n",
      "Smiles: 255\n",
      "Smiles: 256\n",
      "Smiles: 257\n",
      "Smiles: 258\n",
      "Smiles: 259\n",
      "Smiles: 260\n",
      "Smiles: 261\n",
      "Smiles: 262\n",
      "Smiles: 263\n",
      "Smiles: 264\n",
      "Smiles: 265\n",
      "Smiles: 266\n",
      "Smiles: 267\n",
      "Smiles: 268\n",
      "Smiles: 269\n",
      "Smiles: 270\n",
      "Smiles: 271\n",
      "Smiles: 272\n",
      "Smiles: 273\n",
      "Smiles: 274\n",
      "Smiles: 275\n",
      "Smiles: 276\n",
      "Smiles: 277\n",
      "Smiles: 278\n",
      "Smiles: 279\n",
      "Smiles: 280\n",
      "Smiles: 281\n",
      "Smiles: 282\n",
      "Smiles: 283\n",
      "Smiles: 284\n",
      "Smiles: 285\n",
      "Smiles: 286\n",
      "Smiles: 287\n",
      "Smiles: 288\n",
      "Smiles: 289\n",
      "Smiles: 290\n",
      "Smiles: 291\n",
      "Smiles: 292\n",
      "Smiles: 293\n",
      "Smiles: 294\n",
      "Smiles: 295\n",
      "Smiles: 296\n",
      "Smiles: 297\n",
      "Smiles: 298\n",
      "Smiles: 299\n",
      "Smiles: 300\n",
      "Smiles: 301\n",
      "Smiles: 302\n",
      "Smiles: 303\n",
      "Smiles: 304\n",
      "Smiles: 305\n",
      "Smiles: 306\n",
      "Smiles: 307\n",
      "Smiles: 308\n",
      "Smiles: 309\n",
      "Smiles: 310\n",
      "Smiles: 311\n",
      "Smiles: 312\n",
      "Smiles: 313\n",
      "Smiles: 314\n",
      "Smiles: 315\n",
      "Smiles: 316\n",
      "Smiles: 317\n",
      "Smiles: 318\n",
      "Smiles: 319\n",
      "Smiles: 320\n",
      "Smiles: 321\n",
      "Smiles: 322\n",
      "Smiles: 323\n",
      "Smiles: 324\n",
      "Smiles: 325\n",
      "Smiles: 326\n",
      "Smiles: 327\n",
      "Smiles: 328\n",
      "Smiles: 329\n",
      "Smiles: 330\n",
      "Smiles: 331\n",
      "Smiles: 332\n",
      "Smiles: 333\n",
      "Smiles: 334\n",
      "Smiles: 335\n",
      "Smiles: 336\n",
      "Smiles: 337\n",
      "Smiles: 338\n",
      "Smiles: 339\n",
      "Smiles: 340\n",
      "Smiles: 341\n",
      "Smiles: 342\n",
      "Smiles: 343\n",
      "Smiles: 344\n",
      "Smiles: 345\n",
      "Smiles: 346\n",
      "Smiles: 347\n",
      "Smiles: 348\n",
      "Smiles: 349\n",
      "Smiles: 350\n",
      "Smiles: 351\n",
      "Smiles: 352\n",
      "Smiles: 353\n",
      "Smiles: 354\n",
      "Smiles: 355\n",
      "Smiles: 356\n",
      "Smiles: 357\n",
      "Smiles: 358\n",
      "Smiles: 359\n",
      "Smiles: 360\n",
      "Smiles: 361\n",
      "Smiles: 362\n",
      "Smiles: 363\n",
      "Smiles: 364\n",
      "Smiles: 365\n",
      "Smiles: 366\n",
      "Smiles: 367\n",
      "Smiles: 368\n",
      "Smiles: 369\n",
      "Smiles: 370\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")\n",
    "test(model, loss_fn, train_dataloader, device)\n",
    "\n",
    "fingerprint = fingerprint.detach().cpu().numpy().tolist()\n",
    "pred_output = pred_output.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90df9b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>pred_out</th>\n",
       "      <th>orig_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.6056618690490723, -0.2879466116428375, -0....</td>\n",
       "      <td>[-0.10683786123991013]</td>\n",
       "      <td>6.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.17731203138828278, -1.2010531425476074, 0.2...</td>\n",
       "      <td>[1.6284528970718384]</td>\n",
       "      <td>7.6332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.22237494587898254, -0.9703258872032166, 0....</td>\n",
       "      <td>[2.2101566791534424]</td>\n",
       "      <td>8.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.294385701417923, -1.02885103225708, 0.3449...</td>\n",
       "      <td>[2.6464226245880127]</td>\n",
       "      <td>8.5986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.018702805042266846, -1.2984548807144165, 1....</td>\n",
       "      <td>[2.8387887477874756]</td>\n",
       "      <td>9.0178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[-1.2633453607559204, -1.704049825668335, 0.53...</td>\n",
       "      <td>[0.01380742434412241]</td>\n",
       "      <td>6.1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>[1.20831298828125, -1.3530679941177368, 0.5141...</td>\n",
       "      <td>[-0.6642560958862305]</td>\n",
       "      <td>5.6948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>[-1.0325931310653687, -1.3280613422393799, 1.3...</td>\n",
       "      <td>[-0.42180341482162476]</td>\n",
       "      <td>5.8838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>[-0.26856863498687744, -0.6897868514060974, 0....</td>\n",
       "      <td>[-0.5758365392684937]</td>\n",
       "      <td>5.8176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>[-0.23930279910564423, -0.9270451068878174, 0....</td>\n",
       "      <td>[3.186389207839966]</td>\n",
       "      <td>9.0985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fingerprint  \\\n",
       "0    [-0.6056618690490723, -0.2879466116428375, -0....   \n",
       "1    [0.17731203138828278, -1.2010531425476074, 0.2...   \n",
       "2    [-0.22237494587898254, -0.9703258872032166, 0....   \n",
       "3    [-0.294385701417923, -1.02885103225708, 0.3449...   \n",
       "4    [0.018702805042266846, -1.2984548807144165, 1....   \n",
       "..                                                 ...   \n",
       "365  [-1.2633453607559204, -1.704049825668335, 0.53...   \n",
       "366  [1.20831298828125, -1.3530679941177368, 0.5141...   \n",
       "367  [-1.0325931310653687, -1.3280613422393799, 1.3...   \n",
       "368  [-0.26856863498687744, -0.6897868514060974, 0....   \n",
       "369  [-0.23930279910564423, -0.9270451068878174, 0....   \n",
       "\n",
       "                   pred_out  orig_out  \n",
       "0    [-0.10683786123991013]    6.1850  \n",
       "1      [1.6284528970718384]    7.6332  \n",
       "2      [2.2101566791534424]    8.1531  \n",
       "3      [2.6464226245880127]    8.5986  \n",
       "4      [2.8387887477874756]    9.0178  \n",
       "..                      ...       ...  \n",
       "365   [0.01380742434412241]    6.1951  \n",
       "366   [-0.6642560958862305]    5.6948  \n",
       "367  [-0.42180341482162476]    5.8838  \n",
       "368   [-0.5758365392684937]    5.8176  \n",
       "369     [3.186389207839966]    9.0985  \n",
       "\n",
       "[370 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'fingerprint': fingerprint, 'pred_out': pred_output, 'orig_out': original_output }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('mywork/result_data/Ei/Ei_best_fingerprint.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb383d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>pred_out</th>\n",
       "      <th>orig_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.6056618690490723, -0.2879466116428375, -0....</td>\n",
       "      <td>[-0.10683786123991013]</td>\n",
       "      <td>6.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.17731203138828278, -1.2010531425476074, 0.2...</td>\n",
       "      <td>[1.6284528970718384]</td>\n",
       "      <td>7.6332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.22237494587898254, -0.9703258872032166, 0....</td>\n",
       "      <td>[2.2101566791534424]</td>\n",
       "      <td>8.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.294385701417923, -1.02885103225708, 0.3449...</td>\n",
       "      <td>[2.6464226245880127]</td>\n",
       "      <td>8.5986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.018702805042266846, -1.2984548807144165, 1....</td>\n",
       "      <td>[2.8387887477874756]</td>\n",
       "      <td>9.0178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[-1.2633453607559204, -1.704049825668335, 0.53...</td>\n",
       "      <td>[0.01380742434412241]</td>\n",
       "      <td>6.1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>[1.20831298828125, -1.3530679941177368, 0.5141...</td>\n",
       "      <td>[-0.6642560958862305]</td>\n",
       "      <td>5.6948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>[-1.0325931310653687, -1.3280613422393799, 1.3...</td>\n",
       "      <td>[-0.42180341482162476]</td>\n",
       "      <td>5.8838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>[-0.26856863498687744, -0.6897868514060974, 0....</td>\n",
       "      <td>[-0.5758365392684937]</td>\n",
       "      <td>5.8176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>[-0.23930279910564423, -0.9270451068878174, 0....</td>\n",
       "      <td>[3.186389207839966]</td>\n",
       "      <td>9.0985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fingerprint  \\\n",
       "0    [-0.6056618690490723, -0.2879466116428375, -0....   \n",
       "1    [0.17731203138828278, -1.2010531425476074, 0.2...   \n",
       "2    [-0.22237494587898254, -0.9703258872032166, 0....   \n",
       "3    [-0.294385701417923, -1.02885103225708, 0.3449...   \n",
       "4    [0.018702805042266846, -1.2984548807144165, 1....   \n",
       "..                                                 ...   \n",
       "365  [-1.2633453607559204, -1.704049825668335, 0.53...   \n",
       "366  [1.20831298828125, -1.3530679941177368, 0.5141...   \n",
       "367  [-1.0325931310653687, -1.3280613422393799, 1.3...   \n",
       "368  [-0.26856863498687744, -0.6897868514060974, 0....   \n",
       "369  [-0.23930279910564423, -0.9270451068878174, 0....   \n",
       "\n",
       "                   pred_out  orig_out  \n",
       "0    [-0.10683786123991013]    6.1850  \n",
       "1      [1.6284528970718384]    7.6332  \n",
       "2      [2.2101566791534424]    8.1531  \n",
       "3      [2.6464226245880127]    8.5986  \n",
       "4      [2.8387887477874756]    9.0178  \n",
       "..                      ...       ...  \n",
       "365   [0.01380742434412241]    6.1951  \n",
       "366   [-0.6642560958862305]    5.6948  \n",
       "367  [-0.42180341482162476]    5.8838  \n",
       "368   [-0.5758365392684937]    5.8176  \n",
       "369     [3.186389207839966]    9.0985  \n",
       "\n",
       "[370 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mywork/result_data/Ei/Ei_best_fingerprint.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1d5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
