{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253a74ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mm22d016/miniconda3/envs/TransPolymer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "from dataset import Downstream_Dataset, DataAugmentation, LoadPretrainData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18acc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = torch.empty(368, 768)\n",
    "pred_output = torch.empty(368,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec056da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamRegression(nn.Module):\n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        super(DownstreamRegression, self).__init__()\n",
    "        self.PretrainedModel = deepcopy(PretrainedModel)\n",
    "        self.PretrainedModel.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        self.Regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, self.PretrainedModel.config.hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.PretrainedModel.config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,step):\n",
    "        outputs = self.PretrainedModel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0, :] #fingerprint\n",
    "        fingerprint[step] = logits\n",
    "        output = self.Regressor(logits)\n",
    "        return output\n",
    "    \n",
    "def test(model, loss_fn, train_dataloader,device):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print(f'Smiles: {step+1}')\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            prop = batch[\"prop\"].to(device).float()\n",
    "            outputs = model(input_ids, attention_mask,step).float()\n",
    "            pred_output[step] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bb2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = pd.read_csv('data/Eea.csv')\n",
    "original_output = train_data['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02b3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpt/pretrain.pt were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "saved_state = torch.load('ckpt/Eea/Eea_best_model.pt')\n",
    "vocab_sup = pd.read_csv('data/vocab/vocab_sup_PE_I.csv', header=None).values.flatten().tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data.iloc[:, 1] = scaler.fit_transform(train_data.iloc[:, 1].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "PretrainedModel = RobertaModel.from_pretrained('ckpt/pretrain.pt')\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=411)\n",
    "tokenizer.add_tokens(vocab_sup)\n",
    "train_dataset = Downstream_Dataset(train_data, tokenizer, 411)\n",
    "\n",
    "model = DownstreamRegression(drop_rate=0.1).to(device)\n",
    "model = model.double()\n",
    "model.load_state_dict(saved_state['model'])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6bcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Smiles: 1\n",
      "Smiles: 2\n",
      "Smiles: 3\n",
      "Smiles: 4\n",
      "Smiles: 5\n",
      "Smiles: 6\n",
      "Smiles: 7\n",
      "Smiles: 8\n",
      "Smiles: 9\n",
      "Smiles: 10\n",
      "Smiles: 11\n",
      "Smiles: 12\n",
      "Smiles: 13\n",
      "Smiles: 14\n",
      "Smiles: 15\n",
      "Smiles: 16\n",
      "Smiles: 17\n",
      "Smiles: 18\n",
      "Smiles: 19\n",
      "Smiles: 20\n",
      "Smiles: 21\n",
      "Smiles: 22\n",
      "Smiles: 23\n",
      "Smiles: 24\n",
      "Smiles: 25\n",
      "Smiles: 26\n",
      "Smiles: 27\n",
      "Smiles: 28\n",
      "Smiles: 29\n",
      "Smiles: 30\n",
      "Smiles: 31\n",
      "Smiles: 32\n",
      "Smiles: 33\n",
      "Smiles: 34\n",
      "Smiles: 35\n",
      "Smiles: 36\n",
      "Smiles: 37\n",
      "Smiles: 38\n",
      "Smiles: 39\n",
      "Smiles: 40\n",
      "Smiles: 41\n",
      "Smiles: 42\n",
      "Smiles: 43\n",
      "Smiles: 44\n",
      "Smiles: 45\n",
      "Smiles: 46\n",
      "Smiles: 47\n",
      "Smiles: 48\n",
      "Smiles: 49\n",
      "Smiles: 50\n",
      "Smiles: 51\n",
      "Smiles: 52\n",
      "Smiles: 53\n",
      "Smiles: 54\n",
      "Smiles: 55\n",
      "Smiles: 56\n",
      "Smiles: 57\n",
      "Smiles: 58\n",
      "Smiles: 59\n",
      "Smiles: 60\n",
      "Smiles: 61\n",
      "Smiles: 62\n",
      "Smiles: 63\n",
      "Smiles: 64\n",
      "Smiles: 65\n",
      "Smiles: 66\n",
      "Smiles: 67\n",
      "Smiles: 68\n",
      "Smiles: 69\n",
      "Smiles: 70\n",
      "Smiles: 71\n",
      "Smiles: 72\n",
      "Smiles: 73\n",
      "Smiles: 74\n",
      "Smiles: 75\n",
      "Smiles: 76\n",
      "Smiles: 77\n",
      "Smiles: 78\n",
      "Smiles: 79\n",
      "Smiles: 80\n",
      "Smiles: 81\n",
      "Smiles: 82\n",
      "Smiles: 83\n",
      "Smiles: 84\n",
      "Smiles: 85\n",
      "Smiles: 86\n",
      "Smiles: 87\n",
      "Smiles: 88\n",
      "Smiles: 89\n",
      "Smiles: 90\n",
      "Smiles: 91\n",
      "Smiles: 92\n",
      "Smiles: 93\n",
      "Smiles: 94\n",
      "Smiles: 95\n",
      "Smiles: 96\n",
      "Smiles: 97\n",
      "Smiles: 98\n",
      "Smiles: 99\n",
      "Smiles: 100\n",
      "Smiles: 101\n",
      "Smiles: 102\n",
      "Smiles: 103\n",
      "Smiles: 104\n",
      "Smiles: 105\n",
      "Smiles: 106\n",
      "Smiles: 107\n",
      "Smiles: 108\n",
      "Smiles: 109\n",
      "Smiles: 110\n",
      "Smiles: 111\n",
      "Smiles: 112\n",
      "Smiles: 113\n",
      "Smiles: 114\n",
      "Smiles: 115\n",
      "Smiles: 116\n",
      "Smiles: 117\n",
      "Smiles: 118\n",
      "Smiles: 119\n",
      "Smiles: 120\n",
      "Smiles: 121\n",
      "Smiles: 122\n",
      "Smiles: 123\n",
      "Smiles: 124\n",
      "Smiles: 125\n",
      "Smiles: 126\n",
      "Smiles: 127\n",
      "Smiles: 128\n",
      "Smiles: 129\n",
      "Smiles: 130\n",
      "Smiles: 131\n",
      "Smiles: 132\n",
      "Smiles: 133\n",
      "Smiles: 134\n",
      "Smiles: 135\n",
      "Smiles: 136\n",
      "Smiles: 137\n",
      "Smiles: 138\n",
      "Smiles: 139\n",
      "Smiles: 140\n",
      "Smiles: 141\n",
      "Smiles: 142\n",
      "Smiles: 143\n",
      "Smiles: 144\n",
      "Smiles: 145\n",
      "Smiles: 146\n",
      "Smiles: 147\n",
      "Smiles: 148\n",
      "Smiles: 149\n",
      "Smiles: 150\n",
      "Smiles: 151\n",
      "Smiles: 152\n",
      "Smiles: 153\n",
      "Smiles: 154\n",
      "Smiles: 155\n",
      "Smiles: 156\n",
      "Smiles: 157\n",
      "Smiles: 158\n",
      "Smiles: 159\n",
      "Smiles: 160\n",
      "Smiles: 161\n",
      "Smiles: 162\n",
      "Smiles: 163\n",
      "Smiles: 164\n",
      "Smiles: 165\n",
      "Smiles: 166\n",
      "Smiles: 167\n",
      "Smiles: 168\n",
      "Smiles: 169\n",
      "Smiles: 170\n",
      "Smiles: 171\n",
      "Smiles: 172\n",
      "Smiles: 173\n",
      "Smiles: 174\n",
      "Smiles: 175\n",
      "Smiles: 176\n",
      "Smiles: 177\n",
      "Smiles: 178\n",
      "Smiles: 179\n",
      "Smiles: 180\n",
      "Smiles: 181\n",
      "Smiles: 182\n",
      "Smiles: 183\n",
      "Smiles: 184\n",
      "Smiles: 185\n",
      "Smiles: 186\n",
      "Smiles: 187\n",
      "Smiles: 188\n",
      "Smiles: 189\n",
      "Smiles: 190\n",
      "Smiles: 191\n",
      "Smiles: 192\n",
      "Smiles: 193\n",
      "Smiles: 194\n",
      "Smiles: 195\n",
      "Smiles: 196\n",
      "Smiles: 197\n",
      "Smiles: 198\n",
      "Smiles: 199\n",
      "Smiles: 200\n",
      "Smiles: 201\n",
      "Smiles: 202\n",
      "Smiles: 203\n",
      "Smiles: 204\n",
      "Smiles: 205\n",
      "Smiles: 206\n",
      "Smiles: 207\n",
      "Smiles: 208\n",
      "Smiles: 209\n",
      "Smiles: 210\n",
      "Smiles: 211\n",
      "Smiles: 212\n",
      "Smiles: 213\n",
      "Smiles: 214\n",
      "Smiles: 215\n",
      "Smiles: 216\n",
      "Smiles: 217\n",
      "Smiles: 218\n",
      "Smiles: 219\n",
      "Smiles: 220\n",
      "Smiles: 221\n",
      "Smiles: 222\n",
      "Smiles: 223\n",
      "Smiles: 224\n",
      "Smiles: 225\n",
      "Smiles: 226\n",
      "Smiles: 227\n",
      "Smiles: 228\n",
      "Smiles: 229\n",
      "Smiles: 230\n",
      "Smiles: 231\n",
      "Smiles: 232\n",
      "Smiles: 233\n",
      "Smiles: 234\n",
      "Smiles: 235\n",
      "Smiles: 236\n",
      "Smiles: 237\n",
      "Smiles: 238\n",
      "Smiles: 239\n",
      "Smiles: 240\n",
      "Smiles: 241\n",
      "Smiles: 242\n",
      "Smiles: 243\n",
      "Smiles: 244\n",
      "Smiles: 245\n",
      "Smiles: 246\n",
      "Smiles: 247\n",
      "Smiles: 248\n",
      "Smiles: 249\n",
      "Smiles: 250\n",
      "Smiles: 251\n",
      "Smiles: 252\n",
      "Smiles: 253\n",
      "Smiles: 254\n",
      "Smiles: 255\n",
      "Smiles: 256\n",
      "Smiles: 257\n",
      "Smiles: 258\n",
      "Smiles: 259\n",
      "Smiles: 260\n",
      "Smiles: 261\n",
      "Smiles: 262\n",
      "Smiles: 263\n",
      "Smiles: 264\n",
      "Smiles: 265\n",
      "Smiles: 266\n",
      "Smiles: 267\n",
      "Smiles: 268\n",
      "Smiles: 269\n",
      "Smiles: 270\n",
      "Smiles: 271\n",
      "Smiles: 272\n",
      "Smiles: 273\n",
      "Smiles: 274\n",
      "Smiles: 275\n",
      "Smiles: 276\n",
      "Smiles: 277\n",
      "Smiles: 278\n",
      "Smiles: 279\n",
      "Smiles: 280\n",
      "Smiles: 281\n",
      "Smiles: 282\n",
      "Smiles: 283\n",
      "Smiles: 284\n",
      "Smiles: 285\n",
      "Smiles: 286\n",
      "Smiles: 287\n",
      "Smiles: 288\n",
      "Smiles: 289\n",
      "Smiles: 290\n",
      "Smiles: 291\n",
      "Smiles: 292\n",
      "Smiles: 293\n",
      "Smiles: 294\n",
      "Smiles: 295\n",
      "Smiles: 296\n",
      "Smiles: 297\n",
      "Smiles: 298\n",
      "Smiles: 299\n",
      "Smiles: 300\n",
      "Smiles: 301\n",
      "Smiles: 302\n",
      "Smiles: 303\n",
      "Smiles: 304\n",
      "Smiles: 305\n",
      "Smiles: 306\n",
      "Smiles: 307\n",
      "Smiles: 308\n",
      "Smiles: 309\n",
      "Smiles: 310\n",
      "Smiles: 311\n",
      "Smiles: 312\n",
      "Smiles: 313\n",
      "Smiles: 314\n",
      "Smiles: 315\n",
      "Smiles: 316\n",
      "Smiles: 317\n",
      "Smiles: 318\n",
      "Smiles: 319\n",
      "Smiles: 320\n",
      "Smiles: 321\n",
      "Smiles: 322\n",
      "Smiles: 323\n",
      "Smiles: 324\n",
      "Smiles: 325\n",
      "Smiles: 326\n",
      "Smiles: 327\n",
      "Smiles: 328\n",
      "Smiles: 329\n",
      "Smiles: 330\n",
      "Smiles: 331\n",
      "Smiles: 332\n",
      "Smiles: 333\n",
      "Smiles: 334\n",
      "Smiles: 335\n",
      "Smiles: 336\n",
      "Smiles: 337\n",
      "Smiles: 338\n",
      "Smiles: 339\n",
      "Smiles: 340\n",
      "Smiles: 341\n",
      "Smiles: 342\n",
      "Smiles: 343\n",
      "Smiles: 344\n",
      "Smiles: 345\n",
      "Smiles: 346\n",
      "Smiles: 347\n",
      "Smiles: 348\n",
      "Smiles: 349\n",
      "Smiles: 350\n",
      "Smiles: 351\n",
      "Smiles: 352\n",
      "Smiles: 353\n",
      "Smiles: 354\n",
      "Smiles: 355\n",
      "Smiles: 356\n",
      "Smiles: 357\n",
      "Smiles: 358\n",
      "Smiles: 359\n",
      "Smiles: 360\n",
      "Smiles: 361\n",
      "Smiles: 362\n",
      "Smiles: 363\n",
      "Smiles: 364\n",
      "Smiles: 365\n",
      "Smiles: 366\n",
      "Smiles: 367\n",
      "Smiles: 368\n"
     ]
    }
   ],
   "source": [
    "print('Hello')\n",
    "test(model, loss_fn, train_dataloader, device)\n",
    "\n",
    "fingerprint = fingerprint.detach().cpu().numpy().tolist()\n",
    "pred_output = pred_output.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90df9b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>pred_out</th>\n",
       "      <th>orig_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.038528792560100555, -0.060478050261735916, ...</td>\n",
       "      <td>[-1.6442649364471436]</td>\n",
       "      <td>0.4343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.03517939895391464, -0.2776426076889038, -1...</td>\n",
       "      <td>[-1.5285539627075195]</td>\n",
       "      <td>0.8740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.11787766963243484, -0.558820903301239, -1.6...</td>\n",
       "      <td>[-1.4760966300964355]</td>\n",
       "      <td>1.1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.0772891715168953, -1.3392486572265625, -1....</td>\n",
       "      <td>[-1.0364990234375]</td>\n",
       "      <td>1.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.1507545709609985, -1.1616188287734985, -0.6...</td>\n",
       "      <td>[-1.937893033027649]</td>\n",
       "      <td>0.4489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>[0.8615942597389221, -0.9150235652923584, 0.71...</td>\n",
       "      <td>[-0.4574372470378876]</td>\n",
       "      <td>1.5387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>[2.93359112739563, -0.7651606202125549, 0.9211...</td>\n",
       "      <td>[-0.8315487504005432]</td>\n",
       "      <td>1.3385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[-0.3269875943660736, -0.46392083168029785, 0....</td>\n",
       "      <td>[1.5946460962295532]</td>\n",
       "      <td>3.8901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>[1.1184444427490234, -0.38528648018836975, 0.6...</td>\n",
       "      <td>[0.5692479610443115]</td>\n",
       "      <td>2.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>[1.2975678443908691, -1.8936792612075806, -0.4...</td>\n",
       "      <td>[0.7107349634170532]</td>\n",
       "      <td>2.9195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fingerprint               pred_out  \\\n",
       "0    [0.038528792560100555, -0.060478050261735916, ...  [-1.6442649364471436]   \n",
       "1    [-0.03517939895391464, -0.2776426076889038, -1...  [-1.5285539627075195]   \n",
       "2    [0.11787766963243484, -0.558820903301239, -1.6...  [-1.4760966300964355]   \n",
       "3    [-0.0772891715168953, -1.3392486572265625, -1....     [-1.0364990234375]   \n",
       "4    [1.1507545709609985, -1.1616188287734985, -0.6...   [-1.937893033027649]   \n",
       "..                                                 ...                    ...   \n",
       "363  [0.8615942597389221, -0.9150235652923584, 0.71...  [-0.4574372470378876]   \n",
       "364  [2.93359112739563, -0.7651606202125549, 0.9211...  [-0.8315487504005432]   \n",
       "365  [-0.3269875943660736, -0.46392083168029785, 0....   [1.5946460962295532]   \n",
       "366  [1.1184444427490234, -0.38528648018836975, 0.6...   [0.5692479610443115]   \n",
       "367  [1.2975678443908691, -1.8936792612075806, -0.4...   [0.7107349634170532]   \n",
       "\n",
       "     orig_out  \n",
       "0      0.4343  \n",
       "1      0.8740  \n",
       "2      1.1415  \n",
       "3      1.5240  \n",
       "4      0.4489  \n",
       "..        ...  \n",
       "363    1.5387  \n",
       "364    1.3385  \n",
       "365    3.8901  \n",
       "366    2.7568  \n",
       "367    2.9195  \n",
       "\n",
       "[368 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'fingerprint': fingerprint, 'pred_out': pred_output, 'orig_out': original_output }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb383d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>pred_out</th>\n",
       "      <th>orig_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.038528792560100555, -0.060478050261735916, ...</td>\n",
       "      <td>[-1.6442649364471436]</td>\n",
       "      <td>0.4343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.03517939895391464, -0.2776426076889038, -1...</td>\n",
       "      <td>[-1.5285539627075195]</td>\n",
       "      <td>0.8740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.11787766963243484, -0.558820903301239, -1.6...</td>\n",
       "      <td>[-1.4760966300964355]</td>\n",
       "      <td>1.1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.0772891715168953, -1.3392486572265625, -1....</td>\n",
       "      <td>[-1.0364990234375]</td>\n",
       "      <td>1.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.1507545709609985, -1.1616188287734985, -0.6...</td>\n",
       "      <td>[-1.937893033027649]</td>\n",
       "      <td>0.4489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>[0.8615942597389221, -0.9150235652923584, 0.71...</td>\n",
       "      <td>[-0.4574372470378876]</td>\n",
       "      <td>1.5387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>[2.93359112739563, -0.7651606202125549, 0.9211...</td>\n",
       "      <td>[-0.8315487504005432]</td>\n",
       "      <td>1.3385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[-0.3269875943660736, -0.46392083168029785, 0....</td>\n",
       "      <td>[1.5946460962295532]</td>\n",
       "      <td>3.8901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>[1.1184444427490234, -0.38528648018836975, 0.6...</td>\n",
       "      <td>[0.5692479610443115]</td>\n",
       "      <td>2.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>[1.2975678443908691, -1.8936792612075806, -0.4...</td>\n",
       "      <td>[0.7107349634170532]</td>\n",
       "      <td>2.9195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fingerprint               pred_out  \\\n",
       "0    [0.038528792560100555, -0.060478050261735916, ...  [-1.6442649364471436]   \n",
       "1    [-0.03517939895391464, -0.2776426076889038, -1...  [-1.5285539627075195]   \n",
       "2    [0.11787766963243484, -0.558820903301239, -1.6...  [-1.4760966300964355]   \n",
       "3    [-0.0772891715168953, -1.3392486572265625, -1....     [-1.0364990234375]   \n",
       "4    [1.1507545709609985, -1.1616188287734985, -0.6...   [-1.937893033027649]   \n",
       "..                                                 ...                    ...   \n",
       "363  [0.8615942597389221, -0.9150235652923584, 0.71...  [-0.4574372470378876]   \n",
       "364  [2.93359112739563, -0.7651606202125549, 0.9211...  [-0.8315487504005432]   \n",
       "365  [-0.3269875943660736, -0.46392083168029785, 0....   [1.5946460962295532]   \n",
       "366  [1.1184444427490234, -0.38528648018836975, 0.6...   [0.5692479610443115]   \n",
       "367  [1.2975678443908691, -1.8936792612075806, -0.4...   [0.7107349634170532]   \n",
       "\n",
       "     orig_out  \n",
       "0      0.4343  \n",
       "1      0.8740  \n",
       "2      1.1415  \n",
       "3      1.5240  \n",
       "4      0.4489  \n",
       "..        ...  \n",
       "363    1.5387  \n",
       "364    1.3385  \n",
       "365    3.8901  \n",
       "366    2.7568  \n",
       "367    2.9195  \n",
       "\n",
       "[368 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1d5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
